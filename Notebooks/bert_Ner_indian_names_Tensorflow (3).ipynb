{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "bert Ner indian names Tensorflow",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "a594ddd4bbd04fe5bfaebf075601de43": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_5c043bc7cc8943528e38584ce4952ab7",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_54da51376714422aa0e4272b50524eed",
              "IPY_MODEL_8a5845bbafee4c8c8bb06d69d2d060ab"
            ]
          }
        },
        "5c043bc7cc8943528e38584ce4952ab7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "54da51376714422aa0e4272b50524eed": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_1bd60691c2ab4aedaaabe203440eecdb",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 433,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 433,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_9a546f8bca4b497ca25ffb9b15b83fa2"
          }
        },
        "8a5845bbafee4c8c8bb06d69d2d060ab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_08567e53754f47eebc7e3a2ead1c2207",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 433/433 [00:00&lt;00:00, 2.86kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_0bbaeb54bf5b42a8866803576cd3c974"
          }
        },
        "1bd60691c2ab4aedaaabe203440eecdb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "9a546f8bca4b497ca25ffb9b15b83fa2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "08567e53754f47eebc7e3a2ead1c2207": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "0bbaeb54bf5b42a8866803576cd3c974": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "c5f9258b9e22459fa1098fdf59486b78": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_717ad637862a4e43ad7a9206a56be19d",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_5a0cb095c4ff407a84f2d82ee48fac91",
              "IPY_MODEL_583ed522c3454a42bdd6bd9e0e902e96"
            ]
          }
        },
        "717ad637862a4e43ad7a9206a56be19d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "5a0cb095c4ff407a84f2d82ee48fac91": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_197f61c79711455bb1f2938a2ed88140",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 440473133,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 440473133,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_7fdc998ecc3846eeacc649f4b508148e"
          }
        },
        "583ed522c3454a42bdd6bd9e0e902e96": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_1c26eae779494c92a4db415f883c6903",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 440M/440M [01:01&lt;00:00, 7.12MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_12f4a98ede3648d99b1a0db59b94e7b8"
          }
        },
        "197f61c79711455bb1f2938a2ed88140": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "7fdc998ecc3846eeacc649f4b508148e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "1c26eae779494c92a4db415f883c6903": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "12f4a98ede3648d99b1a0db59b94e7b8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "tgzNC2CecTxx",
        "outputId": "444ab4ad-04db-44b1-daff-ba4767e1fd5c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 870
        }
      },
      "source": [
        "!pip install transformers\n",
        "!pip install seqeval"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/19/22/aff234f4a841f8999e68a7a94bdd4b60b4cebcfeca5d67d61cd08c9179de/transformers-3.3.1-py3-none-any.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 4.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Collecting tokenizers==0.8.1.rc2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/80/83/8b9fccb9e48eeb575ee19179e2bdde0ee9a1904f97de5f02d19016b8804f/tokenizers-0.8.1rc2-cp36-cp36m-manylinux1_x86_64.whl (3.0MB)\n",
            "\u001b[K     |████████████████████████████████| 3.0MB 13.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n",
            "Collecting sentencepiece!=0.1.92\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 46.3MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 49.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.6.20)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.16.0)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893257 sha256=098a2c2008e97e5db1dce539a2a9244b2ed160790752eb7deba165e1810867ae\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: tokenizers, sentencepiece, sacremoses, transformers\n",
            "Successfully installed sacremoses-0.0.43 sentencepiece-0.1.91 tokenizers-0.8.1rc2 transformers-3.3.1\n",
            "Collecting seqeval\n",
            "  Downloading https://files.pythonhosted.org/packages/93/e5/b7705156a77f742cfe4fc6f22d0c71591edb2d243328dff2f8fc0f933ab6/seqeval-0.0.19.tar.gz\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.6/dist-packages (from seqeval) (1.18.5)\n",
            "Requirement already satisfied: Keras>=2.2.4 in /usr/local/lib/python3.6/dist-packages (from seqeval) (2.4.3)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.4->seqeval) (3.13)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.4->seqeval) (2.10.0)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.4->seqeval) (1.4.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from h5py->Keras>=2.2.4->seqeval) (1.15.0)\n",
            "Building wheels for collected packages: seqeval\n",
            "  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for seqeval: filename=seqeval-0.0.19-cp36-none-any.whl size=9919 sha256=934644b4533bae2f14d91c662ec5fa1332fabcde3de67c049850a812c73e1522\n",
            "  Stored in directory: /root/.cache/pip/wheels/8d/1f/bf/1198beceed805a2099060975f6281d1b01046dd279e19c97be\n",
            "Successfully built seqeval\n",
            "Installing collected packages: seqeval\n",
            "Successfully installed seqeval-0.0.19\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8WVhkdIee9lC"
      },
      "source": [
        "####We will be using hugging face stat-of-the are nlp mdoels with wrapper of tensorflow on it "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IjtsprVaejMh",
        "outputId": "fddf6d0f-0163-4a60-ce53-304820ac2e86",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "! unzip /content/NLP_Assignment.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  /content/NLP_Assignment.zip\n",
            "   creating: NLP Assignment/\n",
            "  inflating: NLP Assignment/train.csv  \n",
            "  inflating: NLP Assignment/test.csv  \n",
            "   creating: NLP Assignment/train_data/\n",
            "  inflating: NLP Assignment/train_data/11875_2017_Judgement_13-Nov-2017.txt  \n",
            "  inflating: NLP Assignment/train_data/CRLREVP902005.txt  \n",
            "  inflating: NLP Assignment/train_data/WA1342017.txt  \n",
            "  inflating: NLP Assignment/train_data/WPC41342010.txt  \n",
            "  inflating: NLP Assignment/train_data/WA1742017.txt  \n",
            "  inflating: NLP Assignment/train_data/10210_2010_Judgement_06-Dec-2017.txt  \n",
            "  inflating: NLP Assignment/train_data/10689_2008_Judgement_17-May-2018.txt  \n",
            "  inflating: NLP Assignment/train_data/WPC13972016.txt  \n",
            "  inflating: NLP Assignment/train_data/CRP3022016.txt  \n",
            "  inflating: NLP Assignment/train_data/CrlPet8092014.txt  \n",
            "  inflating: NLP Assignment/train_data/WPC45672011.txt  \n",
            "  inflating: NLP Assignment/train_data/RevnPet42016.txt  \n",
            "  inflating: NLP Assignment/train_data/WPC44562017.txt  \n",
            "  inflating: NLP Assignment/train_data/WPC70482016.txt  \n",
            "  inflating: NLP Assignment/train_data/WPC55902016.txt  \n",
            "  inflating: NLP Assignment/train_data/WPCAZ2152016.txt  \n",
            "  inflating: NLP Assignment/train_data/RSA2122017.txt  \n",
            "  inflating: NLP Assignment/train_data/11451_2011_Judgement_29-Aug-2018.txt  \n",
            "  inflating: NLP Assignment/train_data/WPC19632013.txt  \n",
            "  inflating: NLP Assignment/train_data/PIL192016.txt  \n",
            "  inflating: NLP Assignment/train_data/CRP1932015.txt  \n",
            "  inflating: NLP Assignment/train_data/WPC35982017.txt  \n",
            "  inflating: NLP Assignment/train_data/10306_2017_Judgement_14-Nov-2017.txt  \n",
            "  inflating: NLP Assignment/train_data/WPC9012012.txt  \n",
            "  inflating: NLP Assignment/train_data/1120_2013_Judgement_04-Oct-2017.txt  \n",
            "  inflating: NLP Assignment/train_data/11297_2017_Judgement_09-Feb-2018.txt  \n",
            "  inflating: NLP Assignment/train_data/10__Judgement_11-May-2018.txt  \n",
            "  inflating: NLP Assignment/train_data/CRPIO112017.txt  \n",
            "  inflating: NLP Assignment/train_data/WPC7132013.txt  \n",
            "  inflating: NLP Assignment/train_data/CRP882016.txt  \n",
            "  inflating: NLP Assignment/train_data/CrlA1822005.txt  \n",
            "  inflating: NLP Assignment/train_data/RSA1632007.txt  \n",
            "  inflating: NLP Assignment/train_data/11424_2019_5_1501_16350_Judgement_28-Aug-2019.txt  \n",
            "  inflating: NLP Assignment/train_data/WA2012017.txt  \n",
            "  inflating: NLP Assignment/train_data/11703_2016_Judgement_04-Oct-2018.txt  \n",
            "  inflating: NLP Assignment/train_data/ReviewPet152016.txt  \n",
            "  inflating: NLP Assignment/train_data/PIL512016.txt  \n",
            "  inflating: NLP Assignment/train_data/10655_2018_Judgement_01-Aug-2018.txt  \n",
            "  inflating: NLP Assignment/train_data/11046_2017_Judgement_02-Jul-2018.txt  \n",
            "  inflating: NLP Assignment/train_data/10000_2008_Judgement_09-May-2018.txt  \n",
            "  inflating: NLP Assignment/train_data/WA2952014.txt  \n",
            "  inflating: NLP Assignment/train_data/11311_2009_Judgement_28-Feb-2019.txt  \n",
            "  inflating: NLP Assignment/train_data/RSA1692004.txt  \n",
            "  inflating: NLP Assignment/train_data/WPC55872014.txt  \n",
            "  inflating: NLP Assignment/train_data/WPC56362010.txt  \n",
            "  inflating: NLP Assignment/train_data/WPC30982014.txt  \n",
            "  inflating: NLP Assignment/train_data/WPC61312012.txt  \n",
            "  inflating: NLP Assignment/train_data/CRPIO232015.txt  \n",
            "  inflating: NLP Assignment/train_data/WPC64972011.txt  \n",
            "  inflating: NLP Assignment/train_data/WPC36222015.txt  \n",
            "  inflating: NLP Assignment/train_data/WPC74682016.txt  \n",
            "  inflating: NLP Assignment/train_data/WPC73712016.txt  \n",
            "  inflating: NLP Assignment/train_data/CrlAJ1232016.txt  \n",
            "  inflating: NLP Assignment/train_data/WPC2222009.txt  \n",
            "  inflating: NLP Assignment/train_data/WPC30392016.txt  \n",
            "  inflating: NLP Assignment/train_data/WA1802017.txt  \n",
            "  inflating: NLP Assignment/train_data/CrlRevP52017.txt  \n",
            "  inflating: NLP Assignment/train_data/WPC45692017.txt  \n",
            "  inflating: NLP Assignment/train_data/1013_2018_Judgement_11-Mar-2019.txt  \n",
            "  inflating: NLP Assignment/train_data/WPC51172012.txt  \n",
            "  inflating: NLP Assignment/train_data/PIL1182015.txt  \n",
            "  inflating: NLP Assignment/train_data/CrlPet632016.txt  \n",
            "  inflating: NLP Assignment/train_data/CRPIO762017.txt  \n",
            "  inflating: NLP Assignment/train_data/CrlAJ912016.txt  \n",
            "  inflating: NLP Assignment/train_data/WA452014.txt  \n",
            "  inflating: NLP Assignment/train_data/11774_2008_Judgement_31-Jan-2019.txt  \n",
            "  inflating: NLP Assignment/train_data/WPC43862014.txt  \n",
            "  inflating: NLP Assignment/train_data/WPC35612016.txt  \n",
            "  inflating: NLP Assignment/train_data/WPC4722014.txt  \n",
            "  inflating: NLP Assignment/train_data/10136_2008_Judgement_01-Feb-2019.txt  \n",
            "  inflating: NLP Assignment/train_data/MACAPP2122010.txt  \n",
            "  inflating: NLP Assignment/train_data/WA1972017.txt  \n",
            "  inflating: NLP Assignment/train_data/CRP4132016.txt  \n",
            "  inflating: NLP Assignment/train_data/WPC22832017.txt  \n",
            "  inflating: NLP Assignment/train_data/MACAPP722013.txt  \n",
            "  inflating: NLP Assignment/train_data/WPC65402010.txt  \n",
            "  inflating: NLP Assignment/train_data/CRP632017.txt  \n",
            "  inflating: NLP Assignment/train_data/CrlA1832008.txt  \n",
            "  inflating: NLP Assignment/train_data/CrlPet5862014.txt  \n",
            "  inflating: NLP Assignment/train_data/CRP4332016.txt  \n",
            "  inflating: NLP Assignment/train_data/WA1822016.txt  \n",
            "  inflating: NLP Assignment/train_data/WPC64812015.txt  \n",
            "  inflating: NLP Assignment/train_data/CrlAJ172016.txt  \n",
            "  inflating: NLP Assignment/train_data/CRP1242014.txt  \n",
            "  inflating: NLP Assignment/train_data/WPC35442010.txt  \n",
            "  inflating: NLP Assignment/train_data/CRLPET2632014.txt  \n",
            "  inflating: NLP Assignment/train_data/CRPIO1292015.txt  \n",
            "  inflating: NLP Assignment/train_data/CrlAJ1062015.txt  \n",
            "  inflating: NLP Assignment/train_data/CrlA1432008.txt  \n",
            "  inflating: NLP Assignment/train_data/11572_2017_Judgement_19-Jul-2017.txt  \n",
            "  inflating: NLP Assignment/train_data/10631_2008_Judgement_15-Feb-2019.txt  \n",
            "  inflating: NLP Assignment/train_data/CRP802017.txt  \n",
            "  inflating: NLP Assignment/train_data/WA2292015.txt  \n",
            "  inflating: NLP Assignment/train_data/WPC57062012.txt  \n",
            "  inflating: NLP Assignment/train_data/IACivil19242017.txt  \n",
            "  inflating: NLP Assignment/train_data/WPC9042016.txt  \n",
            "  inflating: NLP Assignment/train_data/CrlA312008.txt  \n",
            "  inflating: NLP Assignment/train_data/CrlAJ612015.txt  \n",
            "  inflating: NLP Assignment/train_data/WPC21232017.txt  \n",
            "  inflating: NLP Assignment/train_data/10034_2016_Judgement_09-Feb-2018.txt  \n",
            "  inflating: NLP Assignment/train_data/WPC48632008.txt  \n",
            "  inflating: NLP Assignment/train_data/10523_2008_Judgement_07-Jan-2019.txt  \n",
            "  inflating: NLP Assignment/train_data/WPC26522010.txt  \n",
            "  inflating: NLP Assignment/train_data/WPC23572011.txt  \n",
            "  inflating: NLP Assignment/train_data/WA3592014.txt  \n",
            "  inflating: NLP Assignment/train_data/10022_2008_Judgement_03-Dec-2018.txt  \n",
            "  inflating: NLP Assignment/train_data/CrlPet8122014.txt  \n",
            "  inflating: NLP Assignment/train_data/CRP1312017.txt  \n",
            "  inflating: NLP Assignment/train_data/CrlPet5982013.txt  \n",
            "  inflating: NLP Assignment/train_data/WPC23232008.txt  \n",
            "  inflating: NLP Assignment/train_data/CrlA1752015.txt  \n",
            "  inflating: NLP Assignment/train_data/RSA1662017.txt  \n",
            "  inflating: NLP Assignment/train_data/11723_2006_Judgement_13-Sep-2018.txt  \n",
            "  inflating: NLP Assignment/train_data/WPC55482012.txt  \n",
            "  inflating: NLP Assignment/train_data/WPC29552011.txt  \n",
            "  inflating: NLP Assignment/train_data/WPC58632010.txt  \n",
            "  inflating: NLP Assignment/train_data/BR28072017WP(C)25872015.txt  \n",
            "  inflating: NLP Assignment/train_data/BKM13122012WA4262011.txt  \n",
            "  inflating: NLP Assignment/train_data/SCP19082013WP(C)187352013.txt  \n",
            "  inflating: NLP Assignment/train_data/PM23082004CRP1862003.txt  \n",
            "  inflating: NLP Assignment/train_data/SKS21112016CRLMC28032010.txt  \n",
            "  inflating: NLP Assignment/train_data/BRS18092013WP(C)171552006.txt  \n",
            "  inflating: NLP Assignment/train_data/DD25072014CRLA1361991.txt  \n",
            "  inflating: NLP Assignment/train_data/BM31082016WP(C)32012016.txt  \n",
            "  inflating: NLP Assignment/train_data/SNP08042015WP(C)218282010.txt  \n",
            "  inflating: NLP Assignment/train_data/DPC10052017WP(C)80042016.txt  \n",
            "  inflating: NLP Assignment/train_data/WPC58442015.txt  \n",
            "  inflating: NLP Assignment/train_data/SKS21092016CRLREV6452016.txt  \n",
            "  inflating: NLP Assignment/train_data/SCP22112013WP(C)227072013.txt  \n",
            "  inflating: NLP Assignment/train_data/SCP14082013WP(C)185642013.txt  \n",
            "  inflating: NLP Assignment/train_data/AR04092014WA2852014.txt  \n",
            "  inflating: NLP Assignment/train_data/AKG27012014WP(C)7662014.txt  \n",
            "  inflating: NLP Assignment/train_data/SCP22082013WP(C)177172013.txt  \n",
            "  inflating: NLP Assignment/train_data/VGG02082011WP(C)82422011.txt  \n",
            "  inflating: NLP Assignment/train_data/SCP25092013WP(C)224472013.txt  \n",
            "  inflating: NLP Assignment/train_data/DD01112016SA1671998.txt  \n",
            "  inflating: NLP Assignment/train_data/RFA282009.txt  \n",
            "  inflating: NLP Assignment/train_data/MMD20092010WP(C)130792008.txt  \n",
            "  inflating: NLP Assignment/train_data/AKR27032018SA311991.txt  \n",
            "  inflating: NLP Assignment/train_data/WPC10422013.txt  \n",
            "  inflating: NLP Assignment/train_data/DD04102016RSA1762015.txt  \n",
            "  inflating: NLP Assignment/train_data/SCP16092013WP(C)206782013.txt  \n",
            "  inflating: NLP Assignment/train_data/AKR29032017SA371990.txt  \n",
            "  inflating: NLP Assignment/train_data/BRS29042016WP(C)87692003.txt  \n",
            "  inflating: NLP Assignment/train_data/CrlA572009.txt  \n",
            "  inflating: NLP Assignment/train_data/BRS28072015WP(C)83502012.txt  \n",
            "  inflating: NLP Assignment/train_data/BMP22042014WP(C)16372006.txt  \n",
            "  inflating: NLP Assignment/train_data/DPC21062016WP(C)32452016.txt  \n",
            "  inflating: NLP Assignment/train_data/MMD23022013CRP522005.txt  \n",
            "  inflating: NLP Assignment/train_data/BKN01052015RFA2432007.txt  \n",
            "  inflating: NLP Assignment/train_data/BR30012017OJC140341999.txt  \n",
            "  inflating: NLP Assignment/train_data/MMD04092012WP(C)150492008.txt  \n",
            "  inflating: NLP Assignment/train_data/RBD05102015MATA952014.txt  \n",
            "  inflating: NLP Assignment/train_data/AKG11122013WP(C)260822013.txt  \n",
            "  inflating: NLP Assignment/train_data/AKR20022017WP(C)109672016.txt  \n",
            "  inflating: NLP Assignment/train_data/SCP20112013WP(C)241312013.txt  \n",
            "  inflating: NLP Assignment/train_data/VGG31072012WP(C)24442011.txt  \n",
            "  inflating: NLP Assignment/train_data/AKR19022018SA2421996.txt  \n",
            "  inflating: NLP Assignment/train_data/VGG28032012WP(C)53312011.txt  \n",
            "  inflating: NLP Assignment/train_data/DD05022015GA61995.txt  \n",
            "  inflating: NLP Assignment/train_data/ASN22072011MACA1042010.txt  \n",
            "  inflating: NLP Assignment/train_data/DD21112016FAO1742012.txt  \n",
            "  inflating: NLP Assignment/train_data/PM25032014JCRLA312004.txt  \n",
            "  inflating: NLP Assignment/train_data/SKS06112017CRLMC17412005.txt  \n",
            "  inflating: NLP Assignment/train_data/DD08082014CRLA3321990.txt  \n",
            "  inflating: NLP Assignment/train_data/AKG17012014WP(C)260372013.txt  \n",
            "  inflating: NLP Assignment/train_data/SCP13092013WP(C)214092013.txt  \n",
            "  inflating: NLP Assignment/train_data/MMD18122009WP(C)43032009.txt  \n",
            "  inflating: NLP Assignment/train_data/WPC3392009.txt  \n",
            "  inflating: NLP Assignment/train_data/RBD27112015CRA3441990.txt  \n",
            "  inflating: NLP Assignment/train_data/SP22012014WP(C)167072008.txt  \n",
            "  inflating: NLP Assignment/train_data/DD12112014GA331998.txt  \n",
            "  inflating: NLP Assignment/train_data/BRS03032016WP(C)151022005.txt  \n",
            "  inflating: NLP Assignment/train_data/BMP23112012WP(C)63862012.txt  \n",
            "  inflating: NLP Assignment/train_data/SKS24032017RPFAM752015.txt  \n",
            "  inflating: NLP Assignment/train_data/JPD10042018CRLREV4162017.txt  \n",
            "  inflating: NLP Assignment/train_data/DD02012017WP(C)96762011.txt  \n",
            "  inflating: NLP Assignment/train_data/IM15072014OJC141841999.txt  \n",
            "  inflating: NLP Assignment/train_data/SKS09052017CRLMC14422005.txt  \n",
            "  inflating: NLP Assignment/train_data/SNP16042018WP(C)61062018.txt  \n",
            "  inflating: NLP Assignment/train_data/CRD09102013CRLMC1342012.txt  \n",
            "  inflating: NLP Assignment/train_data/BRS12072013WP(C)8632013.txt  \n",
            "  inflating: NLP Assignment/train_data/AKR06052015WP(C)156182014.txt  \n",
            "  inflating: NLP Assignment/train_data/AKR17042017SA2191998.txt  \n",
            "  inflating: NLP Assignment/train_data/SP09102017CRLA2242003.txt  \n",
            "  inflating: NLP Assignment/train_data/SKS01092015BLAPL20022015.txt  \n",
            "  inflating: NLP Assignment/train_data/SKS03032015JCRLA402008.txt  \n",
            "  inflating: NLP Assignment/train_data/SKS10072017ABLAPL47782017.txt  \n",
            "  inflating: NLP Assignment/train_data/BRS07012014WP(C)140782007.txt  \n",
            "  inflating: NLP Assignment/train_data/SCP25092013WP(C)224412013.txt  \n",
            "  inflating: NLP Assignment/train_data/BRS05082014WP(C)193622010.txt  \n",
            "  inflating: NLP Assignment/train_data/SKS10102017CRLA322009.txt  \n",
            "  inflating: NLP Assignment/train_data/SCP03122013WP(C)258392013.txt  \n",
            "  inflating: NLP Assignment/train_data/SKS26092017CRLMC16952004.txt  \n",
            "  inflating: NLP Assignment/train_data/SNP25062015WP(C)92312003.txt  \n",
            "  inflating: NLP Assignment/train_data/BKP06052014WP(C)33222012.txt  \n",
            "  inflating: NLP Assignment/train_data/SKS27112017CRLMC14472005.txt  \n",
            "  inflating: NLP Assignment/train_data/BKP29022012JCRLA422003.txt  \n",
            "  inflating: NLP Assignment/train_data/RBD18082014MA2492000.txt  \n",
            "  inflating: NLP Assignment/train_data/AKG27012014WP(C)79232013.txt  \n",
            "  inflating: NLP Assignment/train_data/SKM09102012WP(C)51862012.txt  \n",
            "  inflating: NLP Assignment/train_data/BPD21122011WP(C)209952011.txt  \n",
            "  inflating: NLP Assignment/train_data/SKS21092015BLAPL39392015.txt  \n",
            "  inflating: NLP Assignment/train_data/CRD15052012JCRLA862003.txt  \n",
            "  inflating: NLP Assignment/train_data/AKR11072017CMP18442016.txt  \n",
            "  inflating: NLP Assignment/train_data/DD25092014FA331992.txt  \n",
            "  inflating: NLP Assignment/train_data/11038_2009_11_1502_17052_Judgement_26-Sep-2019.txt  \n",
            "  inflating: NLP Assignment/train_data/BKP08092011FA771985.txt  \n",
            "  inflating: NLP Assignment/train_data/AKR16072016WP(C)123572008.txt  \n",
            "  inflating: NLP Assignment/train_data/BRS03032015WP(C)8712014.txt  \n",
            "  inflating: NLP Assignment/train_data/CRD17042012JCRLA742003.txt  \n",
            "  inflating: NLP Assignment/train_data/SCP30082013WP(C)199462013.txt  \n",
            "  inflating: NLP Assignment/train_data/ASN28092011WP(C)124702011.txt  \n",
            "  inflating: NLP Assignment/train_data/SCP20092013WP(C)216012013.txt  \n",
            "  inflating: NLP Assignment/train_data/DD27032015FAO7132014.txt  \n",
            "  inflating: NLP Assignment/train_data/AKR07072015WP(C)83272015.txt  \n",
            "  inflating: NLP Assignment/train_data/WPC28582014.txt  \n",
            "  inflating: NLP Assignment/train_data/BMP05112012WP(C)179562012.txt  \n",
            "  inflating: NLP Assignment/train_data/11530_2007_Judgement_19-Jul-2017.txt  \n",
            "  inflating: NLP Assignment/train_data/AKR25022014WP(C)36582014.txt  \n",
            "  inflating: NLP Assignment/train_data/CRD21112012CRLMC22232009.txt  \n",
            "  inflating: NLP Assignment/train_data/SNP20032018WP(C)12012018.txt  \n",
            "  inflating: NLP Assignment/train_data/SCP10012014CRLMC17942005.txt  \n",
            "  inflating: NLP Assignment/train_data/AKG27012014WP(C)26832013.txt  \n",
            "  inflating: NLP Assignment/train_data/PM30012009CRA311989.txt  \n",
            "  inflating: NLP Assignment/train_data/AKR24072015WP(C)116602015.txt  \n",
            "  inflating: NLP Assignment/train_data/SCP10032014BLAPL306012013.txt  \n",
            "  inflating: NLP Assignment/train_data/RBD17072013RFA1402009.txt  \n",
            "  inflating: NLP Assignment/train_data/SKM17042013WP(C)44912013.txt  \n",
            "  inflating: NLP Assignment/train_data/AKR17082017SA2941990.txt  \n",
            "  inflating: NLP Assignment/train_data/DD03102016RSA32010.txt  \n",
            "  inflating: NLP Assignment/train_data/PM19032010JCRA2942000.txt  \n",
            "  inflating: NLP Assignment/train_data/AKR19032018SA1832001.txt  \n",
            "  inflating: NLP Assignment/train_data/WPC41282011.txt  \n",
            "  inflating: NLP Assignment/train_data/SCP19082013WP(C)186342013.txt  \n",
            "  inflating: NLP Assignment/train_data/MMD31072012RSA2012004.txt  \n",
            "  inflating: NLP Assignment/train_data/BR16112014WP(C)112552005.txt  \n",
            "  inflating: NLP Assignment/train_data/SCP24072013WP(C)68392012.txt  \n",
            "  inflating: NLP Assignment/train_data/AKR01122017SA1111986.txt  \n",
            "  inflating: NLP Assignment/train_data/WPC22132016.txt  \n",
            "  inflating: NLP Assignment/train_data/AKG27012014WP(C)5882014.txt  \n",
            "  inflating: NLP Assignment/train_data/SKM12112012CONTC21722011.txt  \n",
            "  inflating: NLP Assignment/train_data/SKM17072012WP(C)61352012.txt  \n",
            "  inflating: NLP Assignment/train_data/10345_2018_Judgement_09-Apr-2018.txt  \n",
            "  inflating: NLP Assignment/train_data/AKR21112014WP(C)114582014.txt  \n",
            "  inflating: NLP Assignment/train_data/RBD24072014JCRLA412004.txt  \n",
            "  inflating: NLP Assignment/train_data/SNP31032016WP(C)190102011.txt  \n",
            "  inflating: NLP Assignment/train_data/LMP20092012WP(C)13322012.txt  \n",
            "  inflating: NLP Assignment/train_data/VP12012015JCRLA482006.txt  \n",
            "  inflating: NLP Assignment/train_data/SCP19122013WP(C)278302013.txt  \n",
            "  inflating: NLP Assignment/train_data/SCP29082013WP(C)200902013.txt  \n",
            "  inflating: NLP Assignment/train_data/PM06012010JCRA2681999.txt  \n",
            "  inflating: NLP Assignment/train_data/AKR30032016WP(C)124112008.txt  \n",
            "  inflating: NLP Assignment/train_data/ARBP322016.txt  \n",
            "  inflating: NLP Assignment/train_data/SCP30082013WP(C)107112013.txt  \n",
            "  inflating: NLP Assignment/train_data/DD20052016SA1641995.txt  \n",
            "  inflating: NLP Assignment/train_data/DPC21062016WP(C)61182016.txt  \n",
            "  inflating: NLP Assignment/train_data/BMP26022014RVWPET72013.txt  \n",
            "  inflating: NLP Assignment/train_data/AKR30032018SA1062000.txt  \n",
            "  inflating: NLP Assignment/train_data/SCP13082013WP(C)126702009.txt  \n",
            "  inflating: NLP Assignment/train_data/BKP22022012JCRLA392002.txt  \n",
            "  inflating: NLP Assignment/train_data/IM29112014WPCRL2232014.txt  \n",
            "  inflating: NLP Assignment/train_data/SKS20012017CRLREV6812016.txt  \n",
            "  inflating: NLP Assignment/train_data/KRM10072015FAO862012.txt  \n",
            "  inflating: NLP Assignment/train_data/IM24122014WP(C)252112013.txt  \n",
            "  inflating: NLP Assignment/train_data/SP04052017CRA1311992.txt  \n",
            "  inflating: NLP Assignment/train_data/DPC21062016WP(C)135472015.txt  \n",
            "  inflating: NLP Assignment/train_data/BM18122015WP(C)253582014.txt  \n",
            "  inflating: NLP Assignment/train_data/SKS24072017CRLMC23242004.txt  \n",
            "  inflating: NLP Assignment/train_data/BKN18062015OJC56792002.txt  \n",
            "  inflating: NLP Assignment/train_data/AKR05102016RSA2342012.txt  \n",
            "  inflating: NLP Assignment/train_data/SCP03122013WP(C)240612013.txt  \n",
            "  inflating: NLP Assignment/train_data/BKP12122011JCRLA552002.txt  \n",
            "  inflating: NLP Assignment/train_data/SCP25092013WP(C)224492013.txt  \n",
            "  inflating: NLP Assignment/train_data/WA1782017.txt  \n",
            "  inflating: NLP Assignment/train_data/SKS20062016JCRLA1202004.txt  \n",
            "  inflating: NLP Assignment/train_data/DD06082014CRLA3241991.txt  \n",
            "  inflating: NLP Assignment/train_data/DD09012015RFA1332005.txt  \n",
            "  inflating: NLP Assignment/train_data/CRD18082010JCRA1332000.txt  \n",
            "  inflating: NLP Assignment/train_data/DD23042014FA1051986.txt  \n",
            "  inflating: NLP Assignment/train_data/11319_2008_Judgement_12-Oct-2018.txt  \n",
            "  inflating: NLP Assignment/train_data/SKM03092012WA1662011.txt  \n",
            "  inflating: NLP Assignment/train_data/CrlA1832014.txt  \n",
            "  inflating: NLP Assignment/train_data/SKS07092017CRLA552012.txt  \n",
            "  inflating: NLP Assignment/train_data/SKS24012018CRLA5442016.txt  \n",
            "  inflating: NLP Assignment/train_data/BRS19082014WA362013.txt  \n",
            "  inflating: NLP Assignment/train_data/SKS10032017CRREV6812000.txt  \n",
            "  inflating: NLP Assignment/train_data/PM12032008OJC71921996.txt  \n",
            "  inflating: NLP Assignment/train_data/BKP17012012JCRA552002.txt  \n",
            "  inflating: NLP Assignment/train_data/WA1112014.txt  \n",
            "  inflating: NLP Assignment/train_data/SKS05022018CRLMC2192012.txt  \n",
            "  inflating: NLP Assignment/train_data/PM09032006OJC143042001.txt  \n",
            "  inflating: NLP Assignment/train_data/AKR09122016OJC20671995.txt  \n",
            "  inflating: NLP Assignment/train_data/BMP14122012WP(C)17172012.txt  \n",
            "  inflating: NLP Assignment/train_data/BKM10122012WP(C)90912010.txt  \n",
            "  inflating: NLP Assignment/train_data/DPC14102017JCRLA52016.txt  \n",
            "  inflating: NLP Assignment/train_data/DD10122014FA3451989.txt  \n",
            "  inflating: NLP Assignment/train_data/10691_2006_Judgement_04-Oct-2017.txt  \n",
            "  inflating: NLP Assignment/train_data/MMD29062010WP(C)7562010.txt  \n",
            "  inflating: NLP Assignment/train_data/BRS11112014WP(C)141222014.txt  \n",
            "  inflating: NLP Assignment/train_data/MMD18122009WP(C)44572009.txt  \n",
            "  inflating: NLP Assignment/train_data/AKR20112017SA2191990.txt  \n",
            "  inflating: NLP Assignment/train_data/CNGPN24072013WP(C)160162013.txt  \n",
            "  inflating: NLP Assignment/train_data/AKR26032014WP(C)60702014.txt  \n",
            "  inflating: NLP Assignment/train_data/AKR12022018SA771997.txt  \n",
            "  inflating: NLP Assignment/train_data/VGG07082012WP(C)159622010.txt  \n",
            "  inflating: NLP Assignment/train_data/AKR27112017RSA3692009.txt  \n",
            "  inflating: NLP Assignment/train_data/LMP20092012WP(C)220042011.txt  \n",
            "  inflating: NLP Assignment/train_data/DD22092015RSA732006.txt  \n",
            "  inflating: NLP Assignment/train_data/10461_2016_Judgement_12-Mar-2019.txt  \n",
            "  inflating: NLP Assignment/train_data/KRM18052016OJC85561995.txt  \n",
            "  inflating: NLP Assignment/train_data/MMD29012010CRLMC8162008.txt  \n",
            "  inflating: NLP Assignment/train_data/VGG16052012WA1032010.txt  \n",
            "  inflating: NLP Assignment/train_data/AKR20032018SA2741993.txt  \n",
            "  inflating: NLP Assignment/train_data/MMD03052011WP(C)121862010.txt  \n",
            "  inflating: NLP Assignment/train_data/WPC69642016.txt  \n",
            "  inflating: NLP Assignment/train_data/SCP13082013MACA1742012.txt  \n",
            "  inflating: NLP Assignment/train_data/SCP20092013WP(C)215972013.txt  \n",
            "  inflating: NLP Assignment/train_data/BR09102017CMP2062017.txt  \n",
            "  inflating: NLP Assignment/train_data/AKR15042015WP(C)12432003.txt  \n",
            "  inflating: NLP Assignment/train_data/SKS02012018CRLMC9112006.txt  \n",
            "  inflating: NLP Assignment/train_data/DD04122015RSA4642009.txt  \n",
            "  inflating: NLP Assignment/train_data/BRS18082015WP(C)89422015.txt  \n",
            "  inflating: NLP Assignment/train_data/MMD27112012WP(C)115502012.txt  \n",
            "  inflating: NLP Assignment/train_data/CrlPet1352014.txt  \n",
            "  inflating: NLP Assignment/train_data/SCP23092013WP(C)221242013.txt  \n",
            "  inflating: NLP Assignment/train_data/PM30102008CRA122001.txt  \n",
            "  inflating: NLP Assignment/train_data/SKS31072017BLAPL71072016.txt  \n",
            "  inflating: NLP Assignment/train_data/BMP23112012WP(C)335462011.txt  \n",
            "  inflating: NLP Assignment/train_data/DD17062015SA2542002.txt  \n",
            "  inflating: NLP Assignment/train_data/SCP24092013MACA852010.txt  \n",
            "  inflating: NLP Assignment/train_data/DD28082019CRA1981993.txt  \n",
            "  inflating: NLP Assignment/train_data/RBD17092014MA6661996.txt  \n",
            "  inflating: NLP Assignment/train_data/SCP03092013WP(C)123052009.txt  \n",
            "  inflating: NLP Assignment/train_data/SKS28032016BLAPL8542015.txt  \n",
            "  inflating: NLP Assignment/train_data/IACivil21372017.txt  \n",
            "  inflating: NLP Assignment/train_data/AKR25072016WP(C)175882006.txt  \n",
            "  inflating: NLP Assignment/train_data/AKR23042014WP(C)168152012.txt  \n",
            "  inflating: NLP Assignment/train_data/AKR26042016WP(C)79012007.txt  \n",
            "  inflating: NLP Assignment/train_data/BR05122016OJC94232001.txt  \n",
            "  inflating: NLP Assignment/train_data/BMP23112012WP(C)335452011.txt  \n",
            "  inflating: NLP Assignment/train_data/SNP28112017OJC143491998.txt  \n",
            "  inflating: NLP Assignment/train_data/SCP08082013MACA5992012.txt  \n",
            "  inflating: NLP Assignment/train_data/BR14092016OJC36571995.txt  \n",
            "  inflating: NLP Assignment/train_data/AKR11052016WP(C)50222013.txt  \n",
            "  inflating: NLP Assignment/train_data/IM08042016WP(C)121192014.txt  \n",
            "  inflating: NLP Assignment/train_data/BRS18122013WP(C)93302010.txt  \n",
            "  inflating: NLP Assignment/train_data/SKS22092016CRREV4912000.txt  \n",
            "  inflating: NLP Assignment/train_data/11650_2016_Judgement_10-Jan-2018.txt  \n",
            "  inflating: NLP Assignment/train_data/ASN11042014FA2501994.txt  \n",
            "  inflating: NLP Assignment/train_data/SKS21112016CRLREV7762016.txt  \n",
            "  inflating: NLP Assignment/train_data/BR01122017WP(C)3662006.txt  \n",
            "  inflating: NLP Assignment/train_data/SKS23042018CRLMC9562009.txt  \n",
            "  inflating: NLP Assignment/train_data/CRD18082010JCRA442001.txt  \n",
            "  inflating: NLP Assignment/train_data/AKR26042016OJC120102000.txt  \n",
            "  inflating: NLP Assignment/train_data/AKR22112017SA891990.txt  \n",
            "  inflating: NLP Assignment/train_data/SP24042013WP(C)68492009.txt  \n",
            "  inflating: NLP Assignment/train_data/DD10052017RSA3432016.txt  \n",
            "  inflating: NLP Assignment/train_data/DD16112015RSA952005.txt  \n",
            "  inflating: NLP Assignment/train_data/SCP03122013CRLMC30612013.txt  \n",
            "  inflating: NLP Assignment/train_data/BRS26082014WA1182012.txt  \n",
            "  inflating: NLP Assignment/train_data/SCP04092013WP(C)206602013.txt  \n",
            "  inflating: NLP Assignment/train_data/SP21122016CRLA5512010.txt  \n",
            "  inflating: NLP Assignment/train_data/CRD28072010CRA341997.txt  \n",
            "  inflating: NLP Assignment/train_data/AKG24032014WP(C)85142011.txt  \n",
            "  inflating: NLP Assignment/train_data/BR17112016OJC101932001.txt  \n",
            "  inflating: NLP Assignment/train_data/BR07022018CMP19432016.txt  \n",
            "  inflating: NLP Assignment/train_data/AKR31102016CMP13252016.txt  \n",
            "  inflating: NLP Assignment/train_data/WPC59732014.txt  \n",
            "  inflating: NLP Assignment/train_data/IM15072014CRLMC30782012.txt  \n",
            "  inflating: NLP Assignment/train_data/MMD16012013WP(C)151702010.txt  \n",
            "  inflating: NLP Assignment/train_data/SNP08112017WP(C)173372016.txt  \n",
            "  inflating: NLP Assignment/train_data/SNP31102017OJC93081995.txt  \n",
            "  inflating: NLP Assignment/train_data/CRPIO922017.txt  \n",
            "  inflating: NLP Assignment/train_data/SP06012017CRLA5692013.txt  \n",
            "  inflating: NLP Assignment/train_data/SCP16092013WP(C)215012013.txt  \n",
            "  inflating: NLP Assignment/train_data/SKS31102016BLAPL35902016.txt  \n",
            "  inflating: NLP Assignment/train_data/SKS18042016BLAPL1422016.txt  \n",
            "  inflating: NLP Assignment/train_data/PM23072008MA631998.txt  \n",
            "  inflating: NLP Assignment/train_data/AKG03022014WP(C)234592012.txt  \n",
            "  inflating: NLP Assignment/train_data/CRP2142014.txt  \n",
            "  inflating: NLP Assignment/train_data/CRLPET8902014.txt  \n",
            "  inflating: NLP Assignment/train_data/CRLA1202008.txt  \n",
            "  inflating: NLP Assignment/train_data/SCP06092013FAO3022003.txt  \n",
            "  inflating: NLP Assignment/train_data/SP17112016CRA1621992.txt  \n",
            "  inflating: NLP Assignment/train_data/MMD22112012JCRA532002.txt  \n",
            "  inflating: NLP Assignment/train_data/DPC21062016WP(C)101602015.txt  \n",
            "  inflating: NLP Assignment/train_data/MMD12092011RSA4232010.txt  \n",
            "  inflating: NLP Assignment/train_data/WPC34732010.txt  \n",
            "  inflating: NLP Assignment/train_data/BRS19072013CRLMC18942004.txt  \n",
            "  inflating: NLP Assignment/train_data/MMD09112010WP(C)7892005.txt  \n",
            "  inflating: NLP Assignment/train_data/AKR19062017CMP13232016.txt  \n",
            "  inflating: NLP Assignment/train_data/DD04052015RSA1412008.txt  \n",
            "  inflating: NLP Assignment/train_data/SCP30082013WP(C)202232013.txt  \n",
            "  inflating: NLP Assignment/train_data/CrlPet322007.txt  \n",
            "  inflating: NLP Assignment/train_data/MMD15112012CRLMC10452008.txt  \n",
            "  inflating: NLP Assignment/train_data/RBD03032014FAO3272011.txt  \n",
            "  inflating: NLP Assignment/train_data/10348_2013_Judgement_07-Sep-2017.txt  \n",
            "  inflating: NLP Assignment/train_data/10043_2017_9_1502_17294_Judgement_04-Oct-2019.txt  \n",
            "  inflating: NLP Assignment/train_data/WA3652016.txt  \n",
            "  inflating: NLP Assignment/train_data/1075_2010_Judgement_16-Aug-2018.txt  \n",
            "  inflating: NLP Assignment/train_data/WPC29832010.txt  \n",
            "  inflating: NLP Assignment/train_data/10962_2014_Judgement_22-Jan-2019.txt  \n",
            "  inflating: NLP Assignment/train_data/CrlPet1272016.txt  \n",
            "  inflating: NLP Assignment/train_data/WA812017.txt  \n",
            "  inflating: NLP Assignment/train_data/CrlPet1502015.txt  \n",
            "  inflating: NLP Assignment/train_data/CrlPet9332016.txt  \n",
            "  inflating: NLP Assignment/train_data/CrlA1852009.txt  \n",
            "  inflating: NLP Assignment/train_data/RSA402007.txt  \n",
            "  inflating: NLP Assignment/train_data/CRLREVP5292005.txt  \n",
            "  inflating: NLP Assignment/train_data/CrlAJ1132014.txt  \n",
            "  inflating: NLP Assignment/train_data/WA3742015.txt  \n",
            "  inflating: NLP Assignment/train_data/CrlPet4152016.txt  \n",
            "  inflating: NLP Assignment/train_data/11178_2007_Judgement_10-Aug-2018.txt  \n",
            "  inflating: NLP Assignment/train_data/CRP2152015.txt  \n",
            "  inflating: NLP Assignment/train_data/CrlRevP1262015.txt  \n",
            "  inflating: NLP Assignment/train_data/CRP3852016.txt  \n",
            "  inflating: NLP Assignment/train_data/10332_2016_Judgement_07-Feb-2018.txt  \n",
            "  inflating: NLP Assignment/train_data/INTESTCAS62007.txt  \n",
            "  inflating: NLP Assignment/train_data/11463_2017_Judgement_05-Dec-2017.txt  \n",
            "  inflating: NLP Assignment/train_data/CrlPet2132016.txt  \n",
            "  inflating: NLP Assignment/train_data/10144_2016_Judgement_01-Aug-2018.txt  \n",
            "  inflating: NLP Assignment/train_data/WPC26672013.txt  \n",
            "  inflating: NLP Assignment/train_data/CRLAJ1162013.txt  \n",
            "  inflating: NLP Assignment/train_data/11524_2014_16_1502_18087_Judgement_08-Nov-2019.txt  \n",
            "  inflating: NLP Assignment/train_data/CrlPet2052015.txt  \n",
            "  inflating: NLP Assignment/train_data/11363_2017_Judgement_10-Oct-2018.txt  \n",
            "  inflating: NLP Assignment/train_data/11867_2016_Judgement_26-Feb-2019.txt  \n",
            "  inflating: NLP Assignment/train_data/11765_2016_Judgement_27-Nov-2018.txt  \n",
            "  inflating: NLP Assignment/train_data/RFA332008.txt  \n",
            "  inflating: NLP Assignment/train_data/WPC34442016.txt  \n",
            "  inflating: NLP Assignment/train_data/10804_2013_9_1501_16776_Judgement_16-Sep-2019.txt  \n",
            "  inflating: NLP Assignment/train_data/CrlPet8102014.txt  \n",
            "  inflating: NLP Assignment/train_data/10982_2015_Judgement_25-Jan-2017.txt  \n",
            "  inflating: NLP Assignment/train_data/11677_2016_Judgement_13-Nov-2018.txt  \n",
            "  inflating: NLP Assignment/train_data/WPC63562011.txt  \n",
            "  inflating: NLP Assignment/train_data/CRP3322016.txt  \n",
            "  inflating: NLP Assignment/train_data/WPC14872010.txt  \n",
            "  inflating: NLP Assignment/train_data/10459_2016_Judgement_27-Mar-2018.txt  \n",
            "  inflating: NLP Assignment/train_data/RSA392007.txt  \n",
            "  inflating: NLP Assignment/train_data/WPC62622010.txt  \n",
            "  inflating: NLP Assignment/train_data/11800_2011_Judgement_07-Feb-2019.txt  \n",
            "  inflating: NLP Assignment/train_data/CrlPet5622016.txt  \n",
            "  inflating: NLP Assignment/train_data/CrlA242008.txt  \n",
            "  inflating: NLP Assignment/train_data/10208_2009_Judgement_18-Feb-2019.txt  \n",
            "  inflating: NLP Assignment/train_data/11622_2018_Judgement_29-Oct-2018.txt  \n",
            "  inflating: NLP Assignment/train_data/WPC53772010.txt  \n",
            "  inflating: NLP Assignment/train_data/CRLREVP1842005.txt  \n",
            "  inflating: NLP Assignment/train_data/CRLPET1132014.txt  \n",
            "  inflating: NLP Assignment/train_data/RSA1682004.txt  \n",
            "  inflating: NLP Assignment/train_data/WPC51082008.txt  \n",
            "  inflating: NLP Assignment/train_data/WPC48052012.txt  \n",
            "  inflating: NLP Assignment/train_data/CrlPet4962010.txt  \n",
            "  inflating: NLP Assignment/train_data/WPC40122011.txt  \n",
            "  inflating: NLP Assignment/train_data/WPC14102013.txt  \n",
            "  inflating: NLP Assignment/train_data/WPC53862010.txt  \n",
            "  inflating: NLP Assignment/train_data/WPC31772013.txt  \n",
            "  inflating: NLP Assignment/train_data/WPC65112011.txt  \n",
            "  inflating: NLP Assignment/train_data/11095_2018_Judgement_29-Oct-2018.txt  \n",
            "  inflating: NLP Assignment/train_data/WPC21802017.txt  \n",
            "  inflating: NLP Assignment/train_data/11512_2009_3_1502_13358_Judgement_29-Mar-2019.txt  \n",
            "  inflating: NLP Assignment/train_data/10157_2011_Judgement_15-Nov-2018.txt  \n",
            "  inflating: NLP Assignment/train_data/10091_2003_Judgement_13-Apr-2018.txt  \n",
            "  inflating: NLP Assignment/train_data/WPC3882017.txt  \n",
            "  inflating: NLP Assignment/train_data/10992_2012_Judgement_22-Feb-2019.txt  \n",
            "  inflating: NLP Assignment/train_data/1070_2018_Judgement_16-Apr-2019.txt  \n",
            "  inflating: NLP Assignment/train_data/WPC57682013.txt  \n",
            "  inflating: NLP Assignment/train_data/CrlPet9872014.txt  \n",
            "  inflating: NLP Assignment/train_data/11710_2017_4_1504_15807_Judgement_08-Aug-2019.txt  \n",
            "  inflating: NLP Assignment/train_data/RSA942006.txt  \n",
            "  inflating: NLP Assignment/train_data/CrlA472009.txt  \n",
            "  inflating: NLP Assignment/train_data/CrlAJ812015.txt  \n",
            "  inflating: NLP Assignment/train_data/10674_2019_8_1503_15475_Judgement_30-Jul-2019.txt  \n",
            "  inflating: NLP Assignment/train_data/CrlPet8042014.txt  \n",
            "  inflating: NLP Assignment/train_data/WPC65442011.txt  \n",
            "  inflating: NLP Assignment/train_data/1116_2011_Judgement_19-Sep-2018.txt  \n",
            "  inflating: NLP Assignment/train_data/MACAPP1682013.txt  \n",
            "  inflating: NLP Assignment/train_data/RSA952017.txt  \n",
            "  inflating: NLP Assignment/train_data/CrlPet282015.txt  \n",
            "  inflating: NLP Assignment/train_data/WPC37092012.txt  \n",
            "  inflating: NLP Assignment/train_data/11330_2016_Judgement_08-Aug-2017.txt  \n",
            "  inflating: NLP Assignment/train_data/10609_2009_13_1502_16840_Judgement_17-Sep-2019.txt  \n",
            "  inflating: NLP Assignment/train_data/10__Judgement_20-Feb-2019.txt  \n",
            "  inflating: NLP Assignment/train_data/WPC40342017.txt  \n",
            "  inflating: NLP Assignment/train_data/10396_2015_Judgement_15-Feb-2019.txt  \n",
            "  inflating: NLP Assignment/train_data/WPC26242012.txt  \n",
            "  inflating: NLP Assignment/train_data/WPC14172015.txt  \n",
            "  inflating: NLP Assignment/train_data/CrlAJ1242016.txt  \n",
            "  inflating: NLP Assignment/train_data/WPC4072017.txt  \n",
            "  inflating: NLP Assignment/train_data/CRLPET8472016.txt  \n",
            "  inflating: NLP Assignment/train_data/CrlAJ902015.txt  \n",
            "  inflating: NLP Assignment/train_data/CrlPet7812015.txt  \n",
            "  inflating: NLP Assignment/train_data/ArbA52013.txt  \n",
            "  inflating: NLP Assignment/train_data/RSA712017.txt  \n",
            "  inflating: NLP Assignment/train_data/10415_2018_9_1502_16069_Judgement_19-Aug-2019.txt  \n",
            "  inflating: NLP Assignment/train_data/ContCasC4062016.txt  \n",
            "  inflating: NLP Assignment/train_data/WPC26422011.txt  \n",
            "  inflating: NLP Assignment/train_data/RSA322004.txt  \n",
            "  inflating: NLP Assignment/train_data/WPC40942014.txt  \n",
            "  inflating: NLP Assignment/train_data/CRPIO1622017.txt  \n",
            "  inflating: NLP Assignment/train_data/11573_2007_Judgement_11-Dec-2018.txt  \n",
            "  inflating: NLP Assignment/train_data/WPC47292011.txt  \n",
            "  inflating: NLP Assignment/train_data/TestCas42005.txt  \n",
            "  inflating: NLP Assignment/train_data/10036_2018_Judgement_28-Mar-2018.txt  \n",
            "  inflating: NLP Assignment/train_data/RSA302004.txt  \n",
            "  inflating: NLP Assignment/train_data/CrlA152009.txt  \n",
            "  inflating: NLP Assignment/train_data/WPC26192013.txt  \n",
            "  inflating: NLP Assignment/train_data/WPC21802012.txt  \n",
            "  inflating: NLP Assignment/train_data/CRP3712016.txt  \n",
            "  inflating: NLP Assignment/train_data/CrlPet7082013.txt  \n",
            "  inflating: NLP Assignment/train_data/RSA1752017.txt  \n",
            "  inflating: NLP Assignment/train_data/10558_2018_Judgement_11-Sep-2018.txt  \n",
            "  inflating: NLP Assignment/train_data/CrlAJ412016.txt  \n",
            "  inflating: NLP Assignment/train_data/MACApp1282016.txt  \n",
            "  inflating: NLP Assignment/train_data/WPC43922016.txt  \n",
            "  inflating: NLP Assignment/train_data/WA22015.txt  \n",
            "  inflating: NLP Assignment/train_data/WPC25002017.txt  \n",
            "  inflating: NLP Assignment/train_data/WA2782016.txt  \n",
            "  inflating: NLP Assignment/train_data/CRP3372016.txt  \n",
            "  inflating: NLP Assignment/train_data/11456_2012_9_1501_16894_Judgement_20-Sep-2019.txt  \n",
            "  inflating: NLP Assignment/train_data/11391_2017_5_3_15008_Judgement_10-Jul-2019.txt  \n",
            "  inflating: NLP Assignment/train_data/WPC51512011.txt  \n",
            "  inflating: NLP Assignment/train_data/MFA1342004.txt  \n",
            "  inflating: NLP Assignment/train_data/11232_2014_Judgement_25-Sep-2018.txt  \n",
            "  inflating: NLP Assignment/train_data/11520_2005_Judgement_20-Jul-2017.txt  \n",
            "  inflating: NLP Assignment/train_data/WPC40582017.txt  \n",
            "  inflating: NLP Assignment/train_data/CrA1712008.txt  \n",
            "  inflating: NLP Assignment/train_data/RSA1052005.txt  \n",
            "  inflating: NLP Assignment/train_data/CRLAJ1222014.txt  \n",
            "  inflating: NLP Assignment/train_data/WPC27822010.txt  \n",
            "  inflating: NLP Assignment/train_data/MFA282009.txt  \n",
            "  inflating: NLP Assignment/train_data/11339_2017_Judgement_03-Oct-2018.txt  \n",
            "  inflating: NLP Assignment/train_data/10299_2015_3_1501_15661_Judgement_06-Aug-2019.txt  \n",
            "  inflating: NLP Assignment/train_data/WPC54002010.txt  \n",
            "  inflating: NLP Assignment/train_data/WPC60692016.txt  \n",
            "  inflating: NLP Assignment/train_data/WPC23862010.txt  \n",
            "  inflating: NLP Assignment/train_data/MatApp292015.txt  \n",
            "  inflating: NLP Assignment/train_data/WPC6652011.txt  \n",
            "  inflating: NLP Assignment/train_data/10322_2013_Judgement_17-Aug-2016.txt  \n",
            "  inflating: NLP Assignment/train_data/WPC57962010.txt  \n",
            "  inflating: NLP Assignment/train_data/11655_2008_Judgement_22-Apr-2019.txt  \n",
            "  inflating: NLP Assignment/train_data/RSA62003.txt  \n",
            "  inflating: NLP Assignment/train_data/1052_2018_12_1502_17954_Judgement_06-Nov-2019.txt  \n",
            "  inflating: NLP Assignment/train_data/WPC14502011.txt  \n",
            "  inflating: NLP Assignment/train_data/WPC18342008.txt  \n",
            "  inflating: NLP Assignment/train_data/WPC60652016.txt  \n",
            "  inflating: NLP Assignment/train_data/WPC11262010.txt  \n",
            "  inflating: NLP Assignment/train_data/WA1872017.txt  \n",
            "  inflating: NLP Assignment/train_data/CRLAJ952015.txt  \n",
            "  inflating: NLP Assignment/train_data/CRP1962017.txt  \n",
            "  inflating: NLP Assignment/train_data/RSA602008.txt  \n",
            "  inflating: NLP Assignment/train_data/CrlA1892008.txt  \n",
            "  inflating: NLP Assignment/train_data/CRP892017.txt  \n",
            "  inflating: NLP Assignment/train_data/11649_2017_Judgement_17-Jul-2017.txt  \n",
            "  inflating: NLP Assignment/train_data/WPC53752010.txt  \n",
            "  inflating: NLP Assignment/train_data/CrlA1042008.txt  \n",
            "  inflating: NLP Assignment/train_data/CRLREVP2952005.txt  \n",
            "  inflating: NLP Assignment/train_data/CrlA1732009.txt  \n",
            "  inflating: NLP Assignment/train_data/RSA1062004.txt  \n",
            "  inflating: NLP Assignment/train_data/VJS02042009ITA882007.txt  \n",
            "  inflating: NLP Assignment/train_data/HK17032010LAA2682010.txt  \n",
            "  inflating: NLP Assignment/train_data/SKN13032013CRLA10972012.txt  \n",
            "  inflating: NLP Assignment/train_data/RVE31012012CW79312010.txt  \n",
            "  inflating: NLP Assignment/train_data/ISM20092017BA11142017.txt  \n",
            "  inflating: NLP Assignment/train_data/IKK03052013FAOOS1402011.txt  \n",
            "  inflating: NLP Assignment/train_data/RKG07032019CRLMM37202018.txt  \n",
            "  inflating: NLP Assignment/train_data/PNJ05072010CW195412004.txt  \n",
            "  inflating: NLP Assignment/train_data/VJM06052013CW29002013.txt  \n",
            "  inflating: NLP Assignment/train_data/VJM20022013CW44831996.txt  \n",
            "  inflating: NLP Assignment/train_data/SPG26022013CRLA3682001.txt  \n",
            "  inflating: NLP Assignment/train_data/PKB13042012RCR1742011.txt  \n",
            "  inflating: NLP Assignment/train_data/ISM25032019CW73622016.txt  \n",
            "  inflating: NLP Assignment/train_data/RSE17012013S972010.txt  \n",
            "  inflating: NLP Assignment/train_data/VJM14052013CW3121995.txt  \n",
            "  inflating: NLP Assignment/train_data/RSE17072008S2482004.txt  \n",
            "  inflating: NLP Assignment/train_data/SUG13032013CRLR3142007.txt  \n",
            "  inflating: NLP Assignment/train_data/MAN06092016OMP4312015.txt  \n",
            "  inflating: NLP Assignment/train_data/VJM10042018REVIEWPET1472018.txt  \n",
            "  inflating: NLP Assignment/train_data/GM04122009CP502003.txt  \n",
            "  inflating: NLP Assignment/train_data/SAS16032017CW77032016.txt  \n",
            "  inflating: NLP Assignment/train_data/SRB13072007CRLR7492006.txt  \n",
            "  inflating: NLP Assignment/train_data/GPM01102012MACA1512005.txt  \n",
            "  inflating: NLP Assignment/train_data/MMH06032014CW71822010.txt  \n",
            "  inflating: NLP Assignment/train_data/RSE29042010CW149062004.txt  \n",
            "  inflating: NLP Assignment/train_data/GM20092007S10102005.txt  \n",
            "  inflating: NLP Assignment/train_data/SUG02042013CRLR4802008.txt  \n",
            "  inflating: NLP Assignment/train_data/BDA29052007CW36952007.txt  \n",
            "  inflating: NLP Assignment/train_data/PNJ06042015LPA1172014.txt  \n",
            "  inflating: NLP Assignment/train_data/ABH27012011CRLMM19072008.txt  \n",
            "  inflating: NLP Assignment/train_data/AMA15112016CRLA6612014.txt  \n",
            "  inflating: NLP Assignment/train_data/SKT13012012CRLMM1462012.txt  \n",
            "  inflating: NLP Assignment/train_data/VIB31102014CW15042011.txt  \n",
            "  inflating: NLP Assignment/train_data/AK01082008BA8272008.txt  \n",
            "  inflating: NLP Assignment/train_data/MAN27012010S14241988.txt  \n",
            "  inflating: NLP Assignment/train_data/VKJ27032014CRLA15212013.txt  \n",
            "  inflating: NLP Assignment/train_data/SKT10122014CW87362014.txt  \n",
            "  inflating: NLP Assignment/train_data/SUG04102018MACA2022018.txt  \n",
            "  inflating: NLP Assignment/train_data/SND03112009S7002008.txt  \n",
            "  inflating: NLP Assignment/train_data/SRB10032014CEAC112014.txt  \n",
            "  inflating: NLP Assignment/train_data/SKT01122015CRLMM48802015.txt  \n",
            "  inflating: NLP Assignment/train_data/SUG11032019CRLMM13182019.txt  \n",
            "  inflating: NLP Assignment/train_data/VKR01072019CW71632017.txt  \n",
            "  inflating: NLP Assignment/train_data/SKN23022010CW59702007.txt  \n",
            "  inflating: NLP Assignment/train_data/MS28032008LPA552008.txt  \n",
            "  inflating: NLP Assignment/train_data/VJM09012013CW52191998.txt  \n",
            "  inflating: NLP Assignment/train_data/PNJ20092011CW24752011.txt  \n",
            "  inflating: NLP Assignment/train_data/VJM04022013CW35171997.txt  \n",
            "  inflating: NLP Assignment/train_data/ASK18032015CRLA1172014.txt  \n",
            "  inflating: NLP Assignment/train_data/RKG09102017MACA10302015.txt  \n",
            "  inflating: NLP Assignment/train_data/RSE03052010CW63811998.txt  \n",
            "  inflating: NLP Assignment/train_data/RSE14062011CW85632008.txt  \n",
            "  inflating: NLP Assignment/train_data/VJM28022012RFA4652010.txt  \n",
            "  inflating: NLP Assignment/train_data/VKR20022015AAP72015.txt  \n",
            "  inflating: NLP Assignment/train_data/RSE27092011CW69922011.txt  \n",
            "  inflating: NLP Assignment/train_data/VKJ21102013CW19822013.txt  \n",
            "  inflating: NLP Assignment/train_data/MMH11072016CW75322012.txt  \n",
            "  inflating: NLP Assignment/train_data/NAC03122018AA3552018.txt  \n",
            "  inflating: NLP Assignment/train_data/SUG08092016FAO4082015.txt  \n",
            "  inflating: NLP Assignment/train_data/VB09112011CRLA1191999.txt  \n",
            "  inflating: NLP Assignment/train_data/SID09022010ITA1192010.txt  \n",
            "  inflating: NLP Assignment/train_data/PNJ09092013CRLA2522004.txt  \n",
            "  inflating: NLP Assignment/train_data/SAS15102019CR1092019.txt  \n",
            "  inflating: NLP Assignment/train_data/VKJ22102013CW46612011.txt  \n",
            "  inflating: NLP Assignment/train_data/PNJ14052013CW6932012.txt  \n",
            "  inflating: NLP Assignment/train_data/VSA26082015CMM5792015.txt  \n",
            "  inflating: NLP Assignment/train_data/BDA11122008ITA13542008.txt  \n",
            "  inflating: NLP Assignment/train_data/SRB27012009MATC50742008.txt  \n",
            "  inflating: NLP Assignment/train_data/MMH18082008CRLMM392007.txt  \n",
            "  inflating: NLP Assignment/train_data/SAS05122018CRLR8132015.txt  \n",
            "  inflating: NLP Assignment/train_data/CSH18052017ITA10582011.txt  \n",
            "  inflating: NLP Assignment/train_data/SRB01022010S6392008.txt  \n",
            "  inflating: NLP Assignment/train_data/GSS22022013CW11772013.txt  \n",
            "  inflating: NLP Assignment/train_data/RSE17122015CW39032011.txt  \n",
            "  inflating: NLP Assignment/train_data/SND17092009AAP82009.txt  \n",
            "  inflating: NLP Assignment/train_data/KG25022009CW74922003.txt  \n",
            "  inflating: NLP Assignment/train_data/GPM02122011MACA122011.txt  \n",
            "  inflating: NLP Assignment/train_data/PNJ07102013CW42872002.txt  \n",
            "  inflating: NLP Assignment/train_data/AKS02052008CW17512006.txt  \n",
            "  inflating: NLP Assignment/train_data/HKO06012016S16062015.txt  \n",
            "  inflating: NLP Assignment/train_data/PNJ06082009CRLA112001.txt  \n",
            "  inflating: NLP Assignment/train_data/PNJ06082007FAO1921990.txt  \n",
            "  inflating: NLP Assignment/train_data/HK18122014CW76242013.txt  \n",
            "  inflating: NLP Assignment/train_data/SRB28012008CW27382006.txt  \n",
            "  inflating: NLP Assignment/train_data/RSE07072016CW46482014.txt  \n",
            "  inflating: NLP Assignment/train_data/GSS01122017CW63512014.txt  \n",
            "  inflating: NLP Assignment/train_data/KG06042009FAO581991.txt  \n",
            "  inflating: NLP Assignment/train_data/MLM27042012CCP1062012.txt  \n",
            "  inflating: NLP Assignment/train_data/KG24032008FAO1122000.txt  \n",
            "  inflating: NLP Assignment/train_data/SRB31072017CW43932017.txt  \n",
            "  inflating: NLP Assignment/train_data/SNA05032010CW79032008.txt  \n",
            "  inflating: NLP Assignment/train_data/GPM26022015MACA9212013.txt  \n",
            "  inflating: NLP Assignment/train_data/RK30012008LPA1151999.txt  \n",
            "  inflating: NLP Assignment/train_data/SPG03042018CRLR1412018.txt  \n",
            "  inflating: NLP Assignment/train_data/VKS28052013FAO3752007.txt  \n",
            "  inflating: NLP Assignment/train_data/PMS22032018RFA432017.txt  \n",
            "  inflating: NLP Assignment/train_data/ABH14012011CRLA7002008.txt  \n",
            "  inflating: NLP Assignment/train_data/VBG15052008FAO861998.txt  \n",
            "  inflating: NLP Assignment/train_data/AKP16102014RFA4842012.txt  \n",
            "  inflating: NLP Assignment/train_data/VIB14082014CW20332013.txt  \n",
            "  inflating: NLP Assignment/train_data/VKS09052016FAO1041987.txt  \n",
            "  inflating: NLP Assignment/train_data/MMH07092016CW57512015.txt  \n",
            "  inflating: NLP Assignment/train_data/SPG06072015CRLA15872011.txt  \n",
            "  inflating: NLP Assignment/train_data/SUG12022018CW13022018.txt  \n",
            "  inflating: NLP Assignment/train_data/YKH18112019RFA9652019.txt  \n",
            "  inflating: NLP Assignment/train_data/VIB21052014ITA2322014.txt  \n",
            "  inflating: NLP Assignment/train_data/SND04082009OMP2822009.txt  \n",
            "  inflating: NLP Assignment/train_data/SKN25092012CRLA5132012.txt  \n",
            "  inflating: NLP Assignment/train_data/ASK09092015CRLR6452013.txt  \n",
            "  inflating: NLP Assignment/train_data/MAN19032015BA14672014.txt  \n",
            "  inflating: NLP Assignment/train_data/KG08032011RFA3612004.txt  \n",
            "  inflating: NLP Assignment/train_data/IKK14032011RSA452011.txt  \n",
            "  inflating: NLP Assignment/train_data/SUG23012019CRLMM3452019.txt  \n",
            "  inflating: NLP Assignment/train_data/GPM10042015MACA4742011.txt  \n",
            "  inflating: NLP Assignment/train_data/RKG03122018CRLA8232004.txt  \n",
            "  inflating: NLP Assignment/train_data/PKB03012012CR2002010.txt  \n",
            "  inflating: NLP Assignment/train_data/SKT18122014CW82312014.txt  \n",
            "  inflating: NLP Assignment/train_data/VJM15032013FAOOS4582009.txt  \n",
            "  inflating: NLP Assignment/train_data/VKJ28022012CW11502012.txt  \n",
            "  inflating: NLP Assignment/train_data/KG04092013CRLA4322010.txt  \n",
            "  inflating: NLP Assignment/train_data/GPM03092012MACA2132007.txt  \n",
            "  inflating: NLP Assignment/train_data/VJM26032014FAO2222013.txt  \n",
            "  inflating: NLP Assignment/train_data/SKT28092015CRLMM39162015.txt  \n",
            "  inflating: NLP Assignment/train_data/CHS20032019RFA2412019.txt  \n",
            "  inflating: NLP Assignment/train_data/SKT04112019CRLMM56162019.txt  \n",
            "  inflating: NLP Assignment/train_data/JAN21052013LPA2822013.txt  \n",
            "  inflating: NLP Assignment/train_data/10330_2017_Judgement_24-Aug-2017.txt  \n",
            "  inflating: NLP Assignment/train_data/MS05022008LPA62008.txt  \n",
            "  inflating: NLP Assignment/train_data/RSE09092009OMP5142009.txt  \n",
            "  inflating: NLP Assignment/train_data/GM04042008CW27742008.txt  \n",
            "  inflating: NLP Assignment/train_data/VKJ12092012S23292011.txt  \n",
            "  inflating: NLP Assignment/train_data/PNJ14072015LPA2692015.txt  \n",
            "  inflating: NLP Assignment/train_data/PRA05122012CRLMM3872012.txt  \n",
            "  inflating: NLP Assignment/train_data/MKO04112019CRLMM43202019.txt  \n",
            "  inflating: NLP Assignment/train_data/COR1_SAS31102018BA6142018.txt  \n",
            "  inflating: NLP Assignment/train_data/VKR07092016CW77142016.txt  \n",
            "  inflating: NLP Assignment/train_data/RAS01072013OMP5612007.txt  \n",
            "  inflating: NLP Assignment/train_data/VJM01122011RFA1392010.txt  \n",
            "  inflating: NLP Assignment/train_data/HKO19012017RFA7052016.txt  \n",
            "  inflating: NLP Assignment/train_data/SDS26052017MATFC1782016.txt  \n",
            "  inflating: NLP Assignment/train_data/SUG07112013CRLMM50632006.txt  \n",
            "  inflating: NLP Assignment/train_data/SPG23022017CRLR1822012.txt  \n",
            "  inflating: NLP Assignment/train_data/GPM25042011CRLA3112011.txt  \n",
            "  inflating: NLP Assignment/train_data/REP27112018CRLR6842002.txt  \n",
            "  inflating: NLP Assignment/train_data/RSE20042010CW35691996.txt  \n",
            "  inflating: NLP Assignment/train_data/RSE21082012LPA9432011.txt  \n",
            "  inflating: NLP Assignment/train_data/GPM13012015MACA10502011.txt  \n",
            "  inflating: NLP Assignment/train_data/BDA23022016CW85652015.txt  \n",
            "  inflating: NLP Assignment/train_data/SUG25042019CRLMM21852019.txt  \n",
            "  inflating: NLP Assignment/train_data/HKO01042016CW230532005.txt  \n",
            "  inflating: NLP Assignment/train_data/GSS19122011S17632005.txt  \n",
            "  inflating: NLP Assignment/train_data/SKK20082010FAOOS12010.txt  \n",
            "  inflating: NLP Assignment/train_data/SMD16092009IA97682009.txt  \n",
            "  inflating: NLP Assignment/train_data/SRB28022013CW3642013.txt  \n",
            "  inflating: NLP Assignment/train_data/HK23022012CW76352010.txt  \n",
            "  inflating: NLP Assignment/train_data/GSS06102016PR832013.txt  \n",
            "  inflating: NLP Assignment/train_data/PMS06032019S3492018.txt  \n",
            "  inflating: NLP Assignment/train_data/RVE07062013BA9262013.txt  \n",
            "  inflating: NLP Assignment/train_data/PNJ05102015LPA3592015.txt  \n",
            "  inflating: NLP Assignment/train_data/VKJ24052013CW30732013.txt  \n",
            "  inflating: NLP Assignment/train_data/IKK09052011RSA2432007.txt  \n",
            "  inflating: NLP Assignment/train_data/SMD09022009CRLMM18082008.txt  \n",
            "  inflating: NLP Assignment/train_data/VIB27022019CW110472017.txt  \n",
            "  inflating: NLP Assignment/train_data/VJM03092014CR1292013.txt  \n",
            "  inflating: NLP Assignment/train_data/AK14122007EX732005.txt  \n",
            "  inflating: NLP Assignment/train_data/AMA26042017LPA1342017.txt  \n",
            "  inflating: NLP Assignment/train_data/SPG17102017S9002002.txt  \n",
            "  inflating: NLP Assignment/train_data/MMH01092008FAOOS3032005.txt  \n",
            "  inflating: NLP Assignment/train_data/HKO23012017RFA852017.txt  \n",
            "  inflating: NLP Assignment/train_data/SMD13032008CRLMM52112006.txt  \n",
            "  inflating: NLP Assignment/train_data/PNJ09102015LPA6732015.txt  \n",
            "  inflating: NLP Assignment/train_data/SUG26082013CRLMM74872006.txt  \n",
            "  inflating: NLP Assignment/train_data/RKG07042015CW30112010.txt  \n",
            "  inflating: NLP Assignment/train_data/MLM18052012CRLMM36492010.txt  \n",
            "  inflating: NLP Assignment/train_data/CHS23082017CW34902010.txt  \n",
            "  inflating: NLP Assignment/train_data/PNJ02112010CW64232010.txt  \n",
            "  inflating: NLP Assignment/train_data/HK29092011CW72392011.txt  \n",
            "  inflating: NLP Assignment/train_data/SMD30082018CRLA6722015.txt  \n",
            "  inflating: NLP Assignment/train_data/VKR28042016S28292015.txt  \n",
            "  inflating: NLP Assignment/train_data/ABH24022011CRLMM33152010.txt  \n",
            "  inflating: NLP Assignment/train_data/SKN01022012CEAC22012.txt  \n",
            "  inflating: NLP Assignment/train_data/PNJ04032010CRLA1712010.txt  \n",
            "  inflating: NLP Assignment/train_data/PNJ11092007CRLR4612001.txt  \n",
            "  inflating: NLP Assignment/train_data/SUG10072019CRLMM32512019.txt  \n",
            "  inflating: NLP Assignment/train_data/SAS19042018CRLMM14982018.txt  \n",
            "  inflating: NLP Assignment/train_data/GSS16012018CW112015.txt  \n",
            "  inflating: NLP Assignment/train_data/PMS07112019CMM14912019.txt  \n",
            "  inflating: NLP Assignment/train_data/RSE14052009S12921992.txt  \n",
            "  inflating: NLP Assignment/train_data/SAS12052016CW25262015.txt  \n",
            "  inflating: NLP Assignment/train_data/MMH13092010ITA13552010.txt  \n",
            "  inflating: NLP Assignment/train_data/MUG21072014CRLA4821998.txt  \n",
            "  inflating: NLP Assignment/train_data/SRB28082008S5722005.txt  \n",
            "  inflating: NLP Assignment/train_data/PIL182016.txt  \n",
            "  inflating: NLP Assignment/train_data/PNJ22042013CW57922012.txt  \n",
            "  inflating: NLP Assignment/train_data/VKR27112013CW48992011.txt  \n",
            "  inflating: NLP Assignment/train_data/RB14082019CW87722019.txt  \n",
            "  inflating: NLP Assignment/train_data/SID05032015CRLA1702013.txt  \n",
            "  inflating: NLP Assignment/train_data/RAS06112008ITA102007.txt  \n",
            "  inflating: NLP Assignment/train_data/KG05082010CW41522007.txt  \n",
            "  inflating: NLP Assignment/train_data/SRB06012015CW87762014.txt  \n",
            "  inflating: NLP Assignment/train_data/PST12022016CRLMM45012013.txt  \n",
            "  inflating: NLP Assignment/train_data/VS31012013CW26152011.txt  \n",
            "  inflating: NLP Assignment/train_data/PST17032016CRLMM822016.txt  \n",
            "  inflating: NLP Assignment/train_data/VKS17042009CW166642006.txt  \n",
            "  inflating: NLP Assignment/train_data/SKN16012013CRLA252011.txt  \n",
            "  inflating: NLP Assignment/train_data/VJM06082013CW49792013.txt  \n",
            "  inflating: NLP Assignment/train_data/VJS26022007CW180542004.txt  \n",
            "  inflating: NLP Assignment/train_data/VKJ19012010CRLA182009.txt  \n",
            "  inflating: NLP Assignment/train_data/VJM17122018RFA9852018.txt  \n",
            "  inflating: NLP Assignment/train_data/JAN22082019S532017.txt  \n",
            "  inflating: NLP Assignment/train_data/VKJ10122009CRLMM30952009.txt  \n",
            "  inflating: NLP Assignment/train_data/SNA27102009CW102892009.txt  \n",
            "  inflating: NLP Assignment/train_data/GPM17072012MACA5232011.txt  \n",
            "  inflating: NLP Assignment/train_data/ABH22122009CRLA291995.txt  \n",
            "  inflating: NLP Assignment/train_data/AS21082009S3922005.txt  \n",
            "  inflating: NLP Assignment/train_data/AK23012008CW10522006.txt  \n",
            "  inflating: NLP Assignment/train_data/PST22082016CRLMM21402016.txt  \n",
            "  inflating: NLP Assignment/train_data/VKJ06082013CW78562012.txt  \n",
            "  inflating: NLP Assignment/train_data/GPM19122011MACA12011.txt  \n",
            "  inflating: NLP Assignment/train_data/VB26052014CRLR342013.txt  \n",
            "  inflating: NLP Assignment/train_data/SAS10082018CRLMM40252018.txt  \n",
            "  inflating: NLP Assignment/train_data/NVR02122013CW78182008.txt  \n",
            "  inflating: NLP Assignment/train_data/MAN05032014RCR3592013.txt  \n",
            "  inflating: NLP Assignment/train_data/JRM27082019CW34222014.txt  \n",
            "  inflating: NLP Assignment/train_data/RSE24052011CW21632011.txt  \n",
            "  inflating: NLP Assignment/train_data/RK23012009FAO5122007.txt  \n",
            "  inflating: NLP Assignment/train_data/SDS06112017BA19662017.txt  \n",
            "  inflating: NLP Assignment/train_data/PNJ20122010CW11572010.txt  \n",
            "  inflating: NLP Assignment/train_data/AK01072008CW7181987.txt  \n",
            "  inflating: NLP Assignment/train_data/RSE12112010CW2552010.txt  \n",
            "  inflating: NLP Assignment/train_data/SKT20022014MACA5662008.txt  \n",
            "  inflating: NLP Assignment/train_data/PKB17092013CRLA672009.txt  \n",
            "  inflating: NLP Assignment/train_data/SUG07122015CRLMM33362014.txt  \n",
            "  inflating: NLP Assignment/train_data/VB04102013CRLMP4382012.txt  \n",
            "  inflating: NLP Assignment/train_data/PST19012018CRLA6952017.txt  \n",
            "  inflating: NLP Assignment/train_data/SKN07082009CW9932008.txt  \n",
            "  inflating: NLP Assignment/train_data/MCG07032011FAO4302008.txt  \n",
            "  inflating: NLP Assignment/train_data/BS23092019BA16382019.txt  \n",
            "  inflating: NLP Assignment/train_data/PMS19072019PR1042014.txt  \n",
            "  inflating: NLP Assignment/train_data/SRB06082007CRLR8562006.txt  \n",
            "  inflating: NLP Assignment/train_data/SUG02052018CW46402018.txt  \n",
            "  inflating: NLP Assignment/train_data/PNJ19102015LPA4352015.txt  \n",
            "  inflating: NLP Assignment/train_data/PNJ24122014CRLA14162014.txt  \n",
            "  inflating: NLP Assignment/train_data/GPM17122012MACA11992012.txt  \n",
            "  inflating: NLP Assignment/train_data/SRB15022010S242007.txt  \n",
            "  inflating: NLP Assignment/train_data/PNJ22042015CW30462015.txt  \n",
            "  inflating: NLP Assignment/train_data/SPG22072015CRLA11072011.txt  \n",
            "  inflating: NLP Assignment/train_data/SUG18032011PR352003.txt  \n",
            "  inflating: NLP Assignment/train_data/PNJ07012011CW3041994.txt  \n",
            "  inflating: NLP Assignment/train_data/VKJ18112010CRLA6702006.txt  \n",
            "  inflating: NLP Assignment/train_data/RSE15062011CW43262011.txt  \n",
            "  inflating: NLP Assignment/train_data/HK22072010CRLMM15072010.txt  \n",
            "  inflating: NLP Assignment/train_data/VKR08032017FAOOS3852014.txt  \n",
            "  inflating: NLP Assignment/train_data/MUG28052015CMM5452015.txt  \n",
            "  inflating: NLP Assignment/train_data/VJM27022013CW10261997.txt  \n",
            "  inflating: NLP Assignment/train_data/GPM22032012MACA7002010.txt  \n",
            "  inflating: NLP Assignment/train_data/GPM27082012MACA4562009.txt  \n",
            "  inflating: NLP Assignment/train_data/MAN29072011S1272008.txt  \n",
            "  inflating: NLP Assignment/train_data/SKN11052018CW18132018.txt  \n",
            "  inflating: NLP Assignment/train_data/RSE01072014FAOOS4162013.txt  \n",
            "  inflating: NLP Assignment/train_data/SKN03092009CW31722008.txt  \n",
            "  inflating: NLP Assignment/train_data/RKG11122017MACA3402013.txt  \n",
            "  inflating: NLP Assignment/train_data/JRM17082018SC8972018.txt  \n",
            "  inflating: NLP Assignment/train_data/SAS07052019CRLR5432019.txt  \n",
            "  inflating: NLP Assignment/train_data/RSE17122015CW64142013.txt  \n",
            "  inflating: NLP Assignment/train_data/RK01052014RFAOS302013.txt  \n",
            "  inflating: NLP Assignment/train_data/JRM23012012CW64701999.txt  \n",
            "  inflating: NLP Assignment/train_data/SKM16102014CRLMM40432014.txt  \n",
            "  inflating: NLP Assignment/train_data/VJM02022018RFA702018.txt  \n",
            "  inflating: NLP Assignment/train_data/PNJ12092012RFAOS832012.txt  \n",
            "  inflating: NLP Assignment/train_data/MAN15122014S19972012.txt  \n",
            "  inflating: NLP Assignment/train_data/SNA26082009CW47142004.txt  \n",
            "  inflating: NLP Assignment/train_data/SUG21072009CW43901998.txt  \n",
            "  inflating: NLP Assignment/train_data/GM09112010CW24672001.txt  \n",
            "  inflating: NLP Assignment/train_data/VKJ19032014CRLA11402013.txt  \n",
            "  inflating: NLP Assignment/train_data/AK20082009CW66352007.txt  \n",
            "  inflating: NLP Assignment/train_data/MMH28092010ITA14792010.txt  \n",
            "  inflating: NLP Assignment/train_data/SUG10012018CW114932017.txt  \n",
            "  inflating: NLP Assignment/train_data/GPM15112011MACA7822010.txt  \n",
            "  inflating: NLP Assignment/train_data/SKN29072009CW105052009.txt  \n",
            "  inflating: NLP Assignment/train_data/JAN20112013IA115252012.txt  \n",
            "  inflating: NLP Assignment/train_data/RK29112010RFA181997.txt  \n",
            "  inflating: NLP Assignment/train_data/DES04012016CW23582013.txt  \n",
            "  inflating: NLP Assignment/train_data/SKT11092015CRLMM37442015.txt  \n",
            "  inflating: NLP Assignment/train_data/PNJ26082014CRLA5632014.txt  \n",
            "  inflating: NLP Assignment/train_data/SKT19112019CRLMM58542019.txt  \n",
            "  inflating: NLP Assignment/train_data/BDA06112012CRLW15522010.txt  \n",
            "  inflating: NLP Assignment/train_data/SUG29032017MACA3122009.txt  \n",
            "  inflating: NLP Assignment/train_data/MMH06092010LPA17352006.txt  \n",
            "  inflating: NLP Assignment/train_data/11883_2014_5_1501_14974_Judgement_09-Jul-2019.txt  \n",
            "  inflating: NLP Assignment/train_data/CrlAJ772015.txt  \n",
            "  inflating: NLP Assignment/train_data/10102_2017_Judgement_26-Mar-2019.txt  \n",
            "  inflating: NLP Assignment/train_data/11530_2019_4_1501_14597_Judgement_04-Jun-2019.txt  \n",
            "  inflating: NLP Assignment/train_data/11295_2013_Judgement_24-Apr-2018.txt  \n",
            "  inflating: NLP Assignment/train_data/11810_2019_7_1502_15506_Judgement_30-Jul-2019.txt  \n",
            "  inflating: NLP Assignment/train_data/11269_2005_Judgement_05-Sep-2017.txt  \n",
            "  inflating: NLP Assignment/train_data/WPC22292015.txt  \n",
            "  inflating: NLP Assignment/train_data/WPC13252017.txt  \n",
            "  inflating: NLP Assignment/train_data/MFA2162010.txt  \n",
            "  inflating: NLP Assignment/train_data/117_2005_Judgement_05-Sep-2017.txt  \n",
            "  inflating: NLP Assignment/train_data/WPC37972017.txt  \n",
            "  inflating: NLP Assignment/train_data/WPC35512017.txt  \n",
            "  inflating: NLP Assignment/train_data/WPC45042017.txt  \n",
            "  inflating: NLP Assignment/train_data/11457_2012_Judgement_12-Feb-2018.txt  \n",
            "  inflating: NLP Assignment/train_data/WA2042017.txt  \n",
            "  inflating: NLP Assignment/train_data/CRLA992008.txt  \n",
            "  inflating: NLP Assignment/train_data/WPC43612017.txt  \n",
            "  inflating: NLP Assignment/train_data/10189_2007_Judgement_07-Jan-2019.txt  \n",
            "  inflating: NLP Assignment/train_data/10096_2016_Judgement_14-Aug-2017.txt  \n",
            "  inflating: NLP Assignment/train_data/WA1622017.txt  \n",
            "  inflating: NLP Assignment/train_data/WPC52872010.txt  \n",
            "  inflating: NLP Assignment/train_data/CRP1562017.txt  \n",
            "  inflating: NLP Assignment/train_data/CRP4302016.txt  \n",
            "  inflating: NLP Assignment/train_data/WPC21412017.txt  \n",
            "  inflating: NLP Assignment/train_data/WPC45762012.txt  \n",
            "  inflating: NLP Assignment/train_data/CrlAJ492014.txt  \n",
            "  inflating: NLP Assignment/train_data/1033_2017_Judgement_01-May-2019.txt  \n",
            "  inflating: NLP Assignment/train_data/WPC60132015.txt  \n",
            "  inflating: NLP Assignment/train_data/WPC1392017.txt  \n",
            "  inflating: NLP Assignment/train_data/WPC29652012.txt  \n",
            "  inflating: NLP Assignment/train_data/CrlA1882009.txt  \n",
            "  inflating: NLP Assignment/train_data/11032_2008_8_1501_16819_Judgement_19-Sep-2019.txt  \n",
            "  inflating: NLP Assignment/train_data/1081_2014_Judgement_16-May-2018.txt  \n",
            "  inflating: NLP Assignment/train_data/CrlA502008.txt  \n",
            "  inflating: NLP Assignment/train_data/MatApp272015.txt  \n",
            "  inflating: NLP Assignment/train_data/IACivil5822016.txt  \n",
            "  inflating: NLP Assignment/train_data/WPC4112017.txt  \n",
            "  inflating: NLP Assignment/train_data/10__Judgement_12-Feb-2018.txt  \n",
            "  inflating: NLP Assignment/train_data/WPC48072014.txt  \n",
            "  inflating: NLP Assignment/train_data/CrlAJ732015.txt  \n",
            "  inflating: NLP Assignment/train_data/WPC12162009.txt  \n",
            "  inflating: NLP Assignment/train_data/RSA1732013.txt  \n",
            "  inflating: NLP Assignment/train_data/10254_2009_12_1501_16652_Judgement_11-Sep-2019.txt  \n",
            "  inflating: NLP Assignment/train_data/WPC63912016.txt  \n",
            "  inflating: NLP Assignment/train_data/10934_2012_Judgement_19-Sep-2018.txt  \n",
            "  inflating: NLP Assignment/train_data/WPCrl12017.txt  \n",
            "  inflating: NLP Assignment/train_data/CrlAJ1262014.txt  \n",
            "  inflating: NLP Assignment/train_data/WPC35862017.txt  \n",
            "  inflating: NLP Assignment/train_data/FAO282010.txt  \n",
            "  inflating: NLP Assignment/train_data/11317_2019_5_1_16555_Judgement_05-Sep-2019.txt  \n",
            "  inflating: NLP Assignment/train_data/11827_2009_Judgement_25-Nov-2016.txt  \n",
            "  inflating: NLP Assignment/train_data/CrlA2752013.txt  \n",
            "  inflating: NLP Assignment/train_data/WA1962016.txt  \n",
            "  inflating: NLP Assignment/train_data/WPC28982016.txt  \n",
            "  inflating: NLP Assignment/train_data/CRP2252011.txt  \n",
            "  inflating: NLP Assignment/train_data/10194_2019_Judgement_10-May-2019.txt  \n",
            "  inflating: NLP Assignment/train_data/CRP892016.txt  \n",
            "  inflating: NLP Assignment/train_data/11795_2006_Judgement_19-Jun-2017.txt  \n",
            "  inflating: NLP Assignment/train_data/10392_2009_Judgement_29-Mar-2019.txt  \n",
            "  inflating: NLP Assignment/train_data/WA1822017.txt  \n",
            "  inflating: NLP Assignment/train_data/CoApp22016.txt  \n",
            "  inflating: NLP Assignment/train_data/11254_2018_Judgement_06-Dec-2018.txt  \n",
            "  inflating: NLP Assignment/train_data/WA1702015.txt  \n",
            "  inflating: NLP Assignment/train_data/WPC5742009.txt  \n",
            "  inflating: NLP Assignment/train_data/MACAPP122011.txt  \n",
            "  inflating: NLP Assignment/train_data/CrlPet9552016.txt  \n",
            "  inflating: NLP Assignment/train_data/WPC40032010.txt  \n",
            "  inflating: NLP Assignment/train_data/WPC18492015.txt  \n",
            "  inflating: NLP Assignment/train_data/PIL292017.txt  \n",
            "  inflating: NLP Assignment/train_data/WPC58572010.txt  \n",
            "  inflating: NLP Assignment/train_data/WAAP72013.txt  \n",
            "  inflating: NLP Assignment/train_data/CRP2322015.txt  \n",
            "  inflating: NLP Assignment/train_data/WPC50942011.txt  \n",
            "  inflating: NLP Assignment/train_data/WA2072017.txt  \n",
            "  inflating: NLP Assignment/train_data/10676_2018_Judgement_07-Mar-2019.txt  \n",
            "  inflating: NLP Assignment/train_data/11532_2017_Judgement_07-Feb-2019.txt  \n",
            "  inflating: NLP Assignment/train_data/ArbP92013.txt  \n",
            "  inflating: NLP Assignment/train_data/CRP942017.txt  \n",
            "  inflating: NLP Assignment/train_data/MatApp152016.txt  \n",
            "  inflating: NLP Assignment/train_data/11522_2017_Judgement_09-Aug-2018.txt  \n",
            "  inflating: NLP Assignment/train_data/ReviewPet102017.txt  \n",
            "  inflating: NLP Assignment/train_data/WPC30202017.txt  \n",
            "  inflating: NLP Assignment/train_data/11556_2008_Judgement_31-Jan-2019.txt  \n",
            "  inflating: NLP Assignment/train_data/10822_2017_Judgement_18-Aug-2017.txt  \n",
            "  inflating: NLP Assignment/train_data/MatApp202011.txt  \n",
            "  inflating: NLP Assignment/train_data/11566_2019_Judgement_16-Apr-2019.txt  \n",
            "  inflating: NLP Assignment/train_data/10605_2017_Judgement_17-Jul-2017.txt  \n",
            "  inflating: NLP Assignment/train_data/CRP3982016.txt  \n",
            "  inflating: NLP Assignment/train_data/WPC9772010.txt  \n",
            "  inflating: NLP Assignment/train_data/WPC26142012.txt  \n",
            "  inflating: NLP Assignment/train_data/11016_2008_Judgement_28-Jan-2019.txt  \n",
            "  inflating: NLP Assignment/train_data/RSA692003.txt  \n",
            "  inflating: NLP Assignment/train_data/11542_2018_Judgement_05-Sep-2018.txt  \n",
            "  inflating: NLP Assignment/train_data/10785_2017_Judgement_30-Apr-2019.txt  \n",
            "  inflating: NLP Assignment/train_data/WPC24092015.txt  \n",
            "  inflating: NLP Assignment/train_data/WPC10322010.txt  \n",
            "  inflating: NLP Assignment/train_data/CrlPet7572014.txt  \n",
            "  inflating: NLP Assignment/train_data/WPC57342014.txt  \n",
            "  inflating: NLP Assignment/train_data/WPC20202008.txt  \n",
            "  inflating: NLP Assignment/train_data/10694_2018_12_1501_14356_Judgement_03-May-2019.txt  \n",
            "  inflating: NLP Assignment/train_data/11587_2008_Judgement_07-Jan-2019.txt  \n",
            "  inflating: NLP Assignment/train_data/11645_2017_Judgement_22-Sep-2017.txt  \n",
            "  inflating: NLP Assignment/train_data/11511_2018_Judgement_13-Dec-2018.txt  \n",
            "  inflating: NLP Assignment/train_data/CrlPet5242016.txt  \n",
            "  inflating: NLP Assignment/train_data/CrlAJ662016.txt  \n",
            "  inflating: NLP Assignment/train_data/10074_2009_Judgement_08-Feb-2018.txt  \n",
            "  inflating: NLP Assignment/train_data/CrlAJ792015.txt  \n",
            "  inflating: NLP Assignment/train_data/10148_2018_Judgement_26-Nov-2018.txt  \n",
            "  inflating: NLP Assignment/train_data/ElPet32014.txt  \n",
            "  inflating: NLP Assignment/train_data/WA1752017.txt  \n",
            "  inflating: NLP Assignment/train_data/WPC41462006.txt  \n",
            "  inflating: NLP Assignment/train_data/WPC66022010.txt  \n",
            "  inflating: NLP Assignment/train_data/CrlRevP672013.txt  \n",
            "  inflating: NLP Assignment/train_data/WPC25922010.txt  \n",
            "  inflating: NLP Assignment/train_data/11443_2018_11_6_15359_Judgement_25-Jul-2019.txt  \n",
            "  inflating: NLP Assignment/train_data/11307_2008_Judgement_07-Jan-2019.txt  \n",
            "  inflating: NLP Assignment/train_data/10364_2017_Judgement_31-Jul-2017.txt  \n",
            "  inflating: NLP Assignment/train_data/11160_2008_Judgement_07-Mar-2019.txt  \n",
            "  inflating: NLP Assignment/train_data/11020_2014_6_1501_15918_Judgement_08-Aug-2019.txt  \n",
            "  inflating: NLP Assignment/train_data/11362_2007_Judgement_04-Oct-2017.txt  \n",
            "  inflating: NLP Assignment/train_data/WPC7632017.txt  \n",
            "  inflating: NLP Assignment/train_data/CRLA682008.txt  \n",
            "  inflating: NLP Assignment/train_data/CrlA512008.txt  \n",
            "  inflating: NLP Assignment/train_data/WPC61842012.txt  \n",
            "  inflating: NLP Assignment/train_data/MACAPP812011.txt  \n",
            "  inflating: NLP Assignment/train_data/WPC9612015.txt  \n",
            "  inflating: NLP Assignment/train_data/1092_2006_Judgement_06-Sep-2018.txt  \n",
            "  inflating: NLP Assignment/train_data/WPC49932011.txt  \n",
            "  inflating: NLP Assignment/train_data/11436_2015_Judgement_25-Jul-2017.txt  \n",
            "  inflating: NLP Assignment/train_data/WPC33142013.txt  \n",
            "  inflating: NLP Assignment/train_data/CRLPET1592016.txt  \n",
            "  inflating: NLP Assignment/train_data/WPC62912014.txt  \n",
            "  inflating: NLP Assignment/train_data/10439_2007_Judgement_19-Feb-2018.txt  \n",
            "  inflating: NLP Assignment/train_data/ArbA222013.txt  \n",
            "  inflating: NLP Assignment/train_data/WPC16322014.txt  \n",
            "  inflating: NLP Assignment/train_data/11276_2016_11_1501_15651_Judgement_05-Aug-2019.txt  \n",
            "  inflating: NLP Assignment/train_data/WPC49932008.txt  \n",
            "  inflating: NLP Assignment/train_data/CrlA202006.txt  \n",
            "  inflating: NLP Assignment/train_data/PIL372016.txt  \n",
            "  inflating: NLP Assignment/train_data/11043_2017_Judgement_14-Jan-2019.txt  \n",
            "  inflating: NLP Assignment/train_data/10678_2010_Judgement_28-Aug-2018.txt  \n",
            "  inflating: NLP Assignment/train_data/10757_2008_Judgement_07-Jan-2019.txt  \n",
            "  inflating: NLP Assignment/train_data/PIL272015.txt  \n",
            "  inflating: NLP Assignment/train_data/11840_2008_Judgement_24-Sep-2018.txt  \n",
            "  inflating: NLP Assignment/train_data/10595_2016_Judgement_15-Mar-2019.txt  \n",
            "  inflating: NLP Assignment/train_data/11210_2017_Judgement_17-Aug-2017.txt  \n",
            "  inflating: NLP Assignment/train_data/11290_2005_Judgement_25-Sep-2018.txt  \n",
            "  inflating: NLP Assignment/train_data/CrlAJ472012.txt  \n",
            "  inflating: NLP Assignment/train_data/MatApp302013.txt  \n",
            "  inflating: NLP Assignment/train_data/10243_2018_Judgement_01-Oct-2018.txt  \n",
            "  inflating: NLP Assignment/train_data/WPC44152010.txt  \n",
            "  inflating: NLP Assignment/train_data/WPC24022016.txt  \n",
            "  inflating: NLP Assignment/train_data/WPC33792014.txt  \n",
            "  inflating: NLP Assignment/train_data/WPC40162012.txt  \n",
            "  inflating: NLP Assignment/train_data/10099_2008_Judgement_12-Feb-2019.txt  \n",
            "  inflating: NLP Assignment/train_data/WPC6112017.txt  \n",
            "  inflating: NLP Assignment/train_data/10532_2013_3_1501_17728_Judgement_22-Oct-2019.txt  \n",
            "  inflating: NLP Assignment/train_data/10738_2019_14_1501_17459_Judgement_15-Oct-2019.txt  \n",
            "  inflating: NLP Assignment/train_data/WPC50402010.txt  \n",
            "  inflating: NLP Assignment/train_data/WPC54952012.txt  \n",
            "  inflating: NLP Assignment/train_data/PIL22017.txt  \n",
            "  inflating: NLP Assignment/train_data/WA932015.txt  \n",
            "  inflating: NLP Assignment/train_data/CrlPet7432016.txt  \n",
            "  inflating: NLP Assignment/train_data/CrlPet5322016.txt  \n",
            "  inflating: NLP Assignment/train_data/WA2482015.txt  \n",
            "  inflating: NLP Assignment/train_data/WPC39772011.txt  \n",
            "  inflating: NLP Assignment/train_data/CrlPet7882016.txt  \n",
            "  inflating: NLP Assignment/train_data/WPC22582017.txt  \n",
            "  inflating: NLP Assignment/train_data/11574_2017_Judgement_16-May-2018.txt  \n",
            "  inflating: NLP Assignment/train_data/CrlAJ722016.txt  \n",
            "  inflating: NLP Assignment/train_data/10297_2019_12_1504_16240_Judgement_21-Aug-2019.txt  \n",
            "  inflating: NLP Assignment/train_data/WPC54092012.txt  \n",
            "  inflating: NLP Assignment/train_data/CrlAJ742011.txt  \n",
            "  inflating: NLP Assignment/train_data/CrlAJ1172015.txt  \n",
            "  inflating: NLP Assignment/train_data/WPC23092017.txt  \n",
            "  inflating: NLP Assignment/train_data/11233_2009_13_1501_17132_Judgement_27-Sep-2019.txt  \n",
            "  inflating: NLP Assignment/train_data/WPC62042012.txt  \n",
            "  inflating: NLP Assignment/train_data/WA1882017.txt  \n",
            "  inflating: NLP Assignment/train_data/10255_2008_Judgement_30-Oct-2018.txt  \n",
            "  inflating: NLP Assignment/train_data/1033_1999_Judgement_18-May-2018.txt  \n",
            "  inflating: NLP Assignment/train_data/10151_2015_Judgement_13-Feb-2019.txt  \n",
            "   creating: NLP Assignment/test_data/\n",
            "  inflating: NLP Assignment/test_data/CrlA2212011.txt  \n",
            "  inflating: NLP Assignment/test_data/MFA82006.txt  \n",
            "  inflating: NLP Assignment/test_data/MFA52001.txt  \n",
            "  inflating: NLP Assignment/test_data/RSA1622015.txt  \n",
            "  inflating: NLP Assignment/test_data/RSA592005.txt  \n",
            "  inflating: NLP Assignment/test_data/WPC18602008.txt  \n",
            "  inflating: NLP Assignment/test_data/RSA312005.txt  \n",
            "  inflating: NLP Assignment/test_data/MFA162005.txt  \n",
            "  inflating: NLP Assignment/test_data/WPC55402013.txt  \n",
            "  inflating: NLP Assignment/test_data/MC12732014.txt  \n",
            "  inflating: NLP Assignment/test_data/WPC12762008.txt  \n",
            "  inflating: NLP Assignment/test_data/WPC56322011.txt  \n",
            "  inflating: NLP Assignment/test_data/WPC29702008.txt  \n",
            "  inflating: NLP Assignment/test_data/WPC24852012.txt  \n",
            "  inflating: NLP Assignment/test_data/MFA12003.txt  \n",
            "  inflating: NLP Assignment/test_data/WPC29752015.txt  \n",
            "  inflating: NLP Assignment/test_data/WPCAP2732015.txt  \n",
            "  inflating: NLP Assignment/test_data/WPC19442015.txt  \n",
            "  inflating: NLP Assignment/test_data/WPC48972015.txt  \n",
            "  inflating: NLP Assignment/test_data/CrlAJ882012.txt  \n",
            "  inflating: NLP Assignment/test_data/WPC40712013.txt  \n",
            "  inflating: NLP Assignment/test_data/WPC45182015.txt  \n",
            "  inflating: NLP Assignment/test_data/WPC29152015.txt  \n",
            "  inflating: NLP Assignment/test_data/RSA12009.txt  \n",
            "  inflating: NLP Assignment/test_data/RSA12007.txt  \n",
            "  inflating: NLP Assignment/test_data/WPC53652010.txt  \n",
            "  inflating: NLP Assignment/test_data/MFA782004.txt  \n",
            "  inflating: NLP Assignment/test_data/WPC2382010.txt  \n",
            "  inflating: NLP Assignment/test_data/RFA992006.txt  \n",
            "  inflating: NLP Assignment/test_data/WPC14042009.txt  \n",
            "  inflating: NLP Assignment/test_data/WPC47942006.txt  \n",
            "  inflating: NLP Assignment/test_data/CrlA812011.txt  \n",
            "  inflating: NLP Assignment/test_data/RSA432007.txt  \n",
            "  inflating: NLP Assignment/test_data/RFA212009.txt  \n",
            "  inflating: NLP Assignment/test_data/MFA592004.txt  \n",
            "  inflating: NLP Assignment/test_data/RSA1632009.txt  \n",
            "  inflating: NLP Assignment/test_data/WPC38032008.txt  \n",
            "  inflating: NLP Assignment/test_data/WPC49832015.txt  \n",
            "  inflating: NLP Assignment/test_data/MFA652006.txt  \n",
            "  inflating: NLP Assignment/test_data/RSA542005.txt  \n",
            "  inflating: NLP Assignment/test_data/MMD20052009WP(C)44232009.txt  \n",
            "  inflating: NLP Assignment/test_data/SKS09052017CRLLP152016.txt  \n",
            "  inflating: NLP Assignment/test_data/BKN02092014RFA1582004.txt  \n",
            "  inflating: NLP Assignment/test_data/SKS23102017CRLMC10872005.txt  \n",
            "  inflating: NLP Assignment/test_data/AKG17122013WP(C)134162012.txt  \n",
            "  inflating: NLP Assignment/test_data/AKR24022016OJC102722000.txt  \n",
            "  inflating: NLP Assignment/test_data/SP28062013WP(C)36372002.txt  \n",
            "  inflating: NLP Assignment/test_data/SNP25042017WP(C)40362017.txt  \n",
            "  inflating: NLP Assignment/test_data/AKR07042017SA1031989.txt  \n",
            "  inflating: NLP Assignment/test_data/IM27072010WA42002.txt  \n",
            "  inflating: NLP Assignment/test_data/AKR28072017SA1451989.txt  \n",
            "  inflating: NLP Assignment/test_data/AKR04012017CMP9812014.txt  \n",
            "  inflating: NLP Assignment/test_data/AKR01072016WP(C)163902005.txt  \n",
            "  inflating: NLP Assignment/test_data/JPD25012018CRLMA2412016.txt  \n",
            "  inflating: NLP Assignment/test_data/AKR22102013WP(C)22542002.txt  \n",
            "  inflating: NLP Assignment/test_data/BKP22122010CRA1831998.txt  \n",
            "  inflating: NLP Assignment/test_data/SNP12102017WP(C)80912009.txt  \n",
            "  inflating: NLP Assignment/test_data/DPC02052016WP(C)8102016.txt  \n",
            "  inflating: NLP Assignment/test_data/KRM30082016WP(C)150242004.txt  \n",
            "  inflating: NLP Assignment/test_data/BRS18052016WP(C)26562016.txt  \n",
            "  inflating: NLP Assignment/test_data/SNP10102017WP(C)130492005.txt  \n",
            "  inflating: NLP Assignment/test_data/AKR01102015WP(C)115652008.txt  \n",
            "  inflating: NLP Assignment/test_data/AKR16072016OJC21241998.txt  \n",
            "  inflating: NLP Assignment/test_data/SCP13092013WP(C)214012013.txt  \n",
            "  inflating: NLP Assignment/test_data/CRD21112012CRLMC24752006.txt  \n",
            "  inflating: NLP Assignment/test_data/BKM10042012WP(C)205032011.txt  \n",
            "  inflating: NLP Assignment/test_data/VGG30032012WA2602011.txt  \n",
            "  inflating: NLP Assignment/test_data/AKR21122016CMP9302016.txt  \n",
            "  inflating: NLP Assignment/test_data/SKS06022018CRLA242012.txt  \n",
            "  inflating: NLP Assignment/test_data/SCP13122013WP(C)269332013.txt  \n",
            "  inflating: NLP Assignment/test_data/RBD03112014RSA1052013.txt  \n",
            "  inflating: NLP Assignment/test_data/BR09082017OJC14841996.txt  \n",
            "  inflating: NLP Assignment/test_data/AKR14052014WP(C)155432013.txt  \n",
            "  inflating: NLP Assignment/test_data/BRS08102015WP(C)195992010.txt  \n",
            "  inflating: NLP Assignment/test_data/SNP29012016WP(C)312442011.txt  \n",
            "  inflating: NLP Assignment/test_data/BKP03092010CRLA3672004.txt  \n",
            "  inflating: NLP Assignment/test_data/SKS09022016CRLA4742010.txt  \n",
            "  inflating: NLP Assignment/test_data/DD25012017WP(C)67982011.txt  \n",
            "  inflating: NLP Assignment/test_data/IM18062014WP(C)146292004.txt  \n",
            "  inflating: NLP Assignment/test_data/PM28012010JCRA12001.txt  \n",
            "  inflating: NLP Assignment/test_data/SCP12122013WP(C)270012013.txt  \n",
            "  inflating: NLP Assignment/test_data/SNP21072016WP(C)287742011.txt  \n",
            "  inflating: NLP Assignment/test_data/PM13032007JCRA831998.txt  \n",
            "  inflating: NLP Assignment/test_data/SCP19082013WP(C)187282013.txt  \n",
            "  inflating: NLP Assignment/test_data/SP26082013WP(C)167222013.txt  \n",
            "  inflating: NLP Assignment/test_data/BR30062017WP(C)119822008.txt  \n",
            "  inflating: NLP Assignment/test_data/DPC12012018WP(C)6802014.txt  \n",
            "  inflating: NLP Assignment/test_data/SCP07022018WP(C)30252013.txt  \n",
            "  inflating: NLP Assignment/test_data/SKS30082016TRPCRL422007.txt  \n",
            "  inflating: NLP Assignment/test_data/DD03112014FA1141992.txt  \n",
            "  inflating: NLP Assignment/test_data/CRD17042012JCRLA82004.txt  \n",
            "  inflating: NLP Assignment/test_data/KRM15102015FAO2172014.txt  \n",
            "  inflating: NLP Assignment/test_data/CRD09032011JCRA332001.txt  \n",
            "  inflating: NLP Assignment/test_data/SCP18092013WP(C)218512013.txt  \n",
            "  inflating: NLP Assignment/test_data/AKR29092014WP(C)106472014.txt  \n",
            "  inflating: NLP Assignment/test_data/BRS12052015OJC142672001.txt  \n",
            "  inflating: NLP Assignment/test_data/SKS08072016BLAPL32452016.txt  \n",
            "  inflating: NLP Assignment/test_data/SCP08082013WP(C)182912013.txt  \n",
            "  inflating: NLP Assignment/test_data/IM03072014CRLMC37242009.txt  \n",
            "  inflating: NLP Assignment/test_data/AKR21092015WP(C)17302008.txt  \n",
            "  inflating: NLP Assignment/test_data/SCP17122013WP(C)273802013.txt  \n",
            "  inflating: NLP Assignment/test_data/SKS24072014WPCRL2662014.txt  \n",
            "  inflating: NLP Assignment/test_data/KRM04032016FAO4747.txt  \n",
            "  inflating: NLP Assignment/test_data/DPC13102017WP(C)162822015.txt  \n",
            "  inflating: NLP Assignment/test_data/SCP24092013WP(C)218732013.txt  \n",
            "  inflating: NLP Assignment/test_data/PM11102006JCRA541997.txt  \n",
            "  inflating: NLP Assignment/test_data/SKS27012018CRLA802004.txt  \n",
            "  inflating: NLP Assignment/test_data/SCP16082013WP(C)186272013.txt  \n",
            "  inflating: NLP Assignment/test_data/BRS08102015WP(C)259172011.txt  \n",
            "  inflating: NLP Assignment/test_data/MMD31012014FAO4332009.txt  \n",
            "  inflating: NLP Assignment/test_data/SCP22112013WP(C)254862013.txt  \n",
            "  inflating: NLP Assignment/test_data/BRS16042015WP(C)98642013.txt  \n",
            "  inflating: NLP Assignment/test_data/SCP13092013WP(C)214042013.txt  \n",
            "  inflating: NLP Assignment/test_data/MMD07022013RVWPET1662011.txt  \n",
            "  inflating: NLP Assignment/test_data/BRS01102013WP(C)149732012.txt  \n",
            "  inflating: NLP Assignment/test_data/SKS21122016CRLREV7552016.txt  \n",
            "  inflating: NLP Assignment/test_data/SCP23092013WP(C)105372012.txt  \n",
            "  inflating: NLP Assignment/test_data/DD04082015FAO1482009.txt  \n",
            "  inflating: NLP Assignment/test_data/SCP17122013WP(C)273842013.txt  \n",
            "  inflating: NLP Assignment/test_data/SNP17032015WP(C)2902009.txt  \n",
            "  inflating: NLP Assignment/test_data/SCP25072013WP(C)168302013.txt  \n",
            "  inflating: NLP Assignment/test_data/AKR12072017CMP7092016.txt  \n",
            "  inflating: NLP Assignment/test_data/DD25012017FAO5202016.txt  \n",
            "  inflating: NLP Assignment/test_data/AKR23042018SA441996.txt  \n",
            "  inflating: NLP Assignment/test_data/SCP02122013WP(C)259802013.txt  \n",
            "  inflating: NLP Assignment/test_data/SNP25022016OJC65181998.txt  \n",
            "  inflating: NLP Assignment/test_data/SP30032016OJC21211999.txt  \n",
            "  inflating: NLP Assignment/test_data/AKR15092016RSA3022010.txt  \n",
            "  inflating: NLP Assignment/test_data/SKS15012018CRLA6942016.txt  \n",
            "  inflating: NLP Assignment/test_data/BMP31072013WP(C)143062013.txt  \n",
            "  inflating: NLP Assignment/test_data/PM25072005OJC135872000.txt  \n",
            "  inflating: NLP Assignment/test_data/DD14122015SA1011988.txt  \n",
            "  inflating: NLP Assignment/test_data/BR03112016WP(C)38252004.txt  \n",
            "  inflating: NLP Assignment/test_data/ASN29072011MACA10572005.txt  \n",
            "  inflating: NLP Assignment/test_data/AKR10102017SA811997.txt  \n",
            "  inflating: NLP Assignment/test_data/SCP20112013WP(C)241322013.txt  \n",
            "  inflating: NLP Assignment/test_data/SCP12092013WP(C)21072013.txt  \n",
            "  inflating: NLP Assignment/test_data/BMP23112012WP(C)335482011.txt  \n",
            "  inflating: NLP Assignment/test_data/CRD19062012JCRLA382003.txt  \n",
            "  inflating: NLP Assignment/test_data/SP18112016JCRLA72005.txt  \n",
            "  inflating: NLP Assignment/test_data/BR16032017WP(C)35262015.txt  \n",
            "  inflating: NLP Assignment/test_data/SCP22112013WP(C)254942013.txt  \n",
            "  inflating: NLP Assignment/test_data/SCP22112013WP(C)254582013.txt  \n",
            "  inflating: NLP Assignment/test_data/MMD31012014FAO302008.txt  \n",
            "  inflating: NLP Assignment/test_data/DD25032015SA3302000.txt  \n",
            "  inflating: NLP Assignment/test_data/AKR26102016CMP13382016.txt  \n",
            "  inflating: NLP Assignment/test_data/SCP20092013WP(C)220772013.txt  \n",
            "  inflating: NLP Assignment/test_data/AKR28102016SA1661988.txt  \n",
            "  inflating: NLP Assignment/test_data/DD03122015RSA1122012.txt  \n",
            "  inflating: NLP Assignment/test_data/SCP27082013WP(C)199612013.txt  \n",
            "  inflating: NLP Assignment/test_data/AKR23102017RSA3262006.txt  \n",
            "  inflating: NLP Assignment/test_data/AKR29032017SA141997.txt  \n",
            "  inflating: NLP Assignment/test_data/SCP19122013WP(C)278062013.txt  \n",
            "  inflating: NLP Assignment/test_data/BR04092014WP(C)294432011.txt  \n",
            "  inflating: NLP Assignment/test_data/SCP30082013WP(C)202052013.txt  \n",
            "  inflating: NLP Assignment/test_data/SKS06042015JCRLA312006.txt  \n",
            "  inflating: NLP Assignment/test_data/SKS10112017BLAPL53892017.txt  \n",
            "  inflating: NLP Assignment/test_data/RBD24092013TRP(C)672011.txt  \n",
            "  inflating: NLP Assignment/test_data/CRD24012014JCRLA42003.txt  \n",
            "  inflating: NLP Assignment/test_data/BR18082014FAO1862014.txt  \n",
            "  inflating: NLP Assignment/test_data/MMD25072013WP(C)43972011.txt  \n",
            "  inflating: NLP Assignment/test_data/JPD25012018CRLREV2402016.txt  \n",
            "  inflating: NLP Assignment/test_data/SKS02012018CRLMC25362006.txt  \n",
            "  inflating: NLP Assignment/test_data/BRS02082013CRLMC4132005.txt  \n",
            "  inflating: NLP Assignment/test_data/MMD29062009WP(C)51192002.txt  \n",
            "  inflating: NLP Assignment/test_data/SCP20112013WP(C)239982013.txt  \n",
            "  inflating: NLP Assignment/test_data/CRD27072010WP(C)151482006.txt  \n",
            "  inflating: NLP Assignment/test_data/BMP03042013BLAPL309582012.txt  \n",
            "  inflating: NLP Assignment/test_data/IM15072014CONTC19662010.txt  \n",
            "  inflating: NLP Assignment/test_data/BR07102016OJC48571997.txt  \n",
            "  inflating: NLP Assignment/test_data/WPC5952009.txt  \n",
            "  inflating: NLP Assignment/test_data/AKR19032014WP(C)54562014.txt  \n",
            "  inflating: NLP Assignment/test_data/MMD20022009WP(C)58642008.txt  \n",
            "  inflating: NLP Assignment/test_data/DPC30012018WP(C)199332016.txt  \n",
            "  inflating: NLP Assignment/test_data/AKR20122017SA1791993.txt  \n",
            "  inflating: NLP Assignment/test_data/SCP08082013WP(C)182882013.txt  \n",
            "  inflating: NLP Assignment/test_data/WPC13932010.txt  \n",
            "  inflating: NLP Assignment/test_data/AKG13122013WP(C)259852013.txt  \n",
            "  inflating: NLP Assignment/test_data/SNP05052015WP(C)34292003.txt  \n",
            "  inflating: NLP Assignment/test_data/DD30062015FAO822011.txt  \n",
            "  inflating: NLP Assignment/test_data/SNP30082017WP(C)187972009.txt  \n",
            "  inflating: NLP Assignment/test_data/SCP21082013WP(C)193462013.txt  \n",
            "  inflating: NLP Assignment/test_data/SNP17082017WP(C)36802008.txt  \n",
            "  inflating: NLP Assignment/test_data/MMD29112011WP(C)197152008.txt  \n",
            "  inflating: NLP Assignment/test_data/SKS22022018CRLREV4542008.txt  \n",
            "  inflating: NLP Assignment/test_data/DD04052015RSA1422008.txt  \n",
            "  inflating: NLP Assignment/test_data/AKR18012018RSA1452008.txt  \n",
            "  inflating: NLP Assignment/test_data/SKS22122015JCRLA862004.txt  \n",
            "  inflating: NLP Assignment/test_data/SKM29032012WP(C)295732011.txt  \n",
            "  inflating: NLP Assignment/test_data/MMD29012014WP(C)67552012.txt  \n",
            "  inflating: NLP Assignment/test_data/AKR31072015WP(C)109522009.txt  \n",
            "  inflating: NLP Assignment/test_data/PM23122005OJC48512002.txt  \n",
            "  inflating: NLP Assignment/test_data/DD27032015FAO2032014.txt  \n",
            "  inflating: NLP Assignment/test_data/SNP28012016WP(C)8572003.txt  \n",
            "  inflating: NLP Assignment/test_data/SNP01122015WP(C)180722012.txt  \n",
            "  inflating: NLP Assignment/test_data/MMD31072013WP(C)4612010.txt  \n",
            "  inflating: NLP Assignment/test_data/BKP25042013WP(C)247812012.txt  \n",
            "  inflating: NLP Assignment/test_data/SCP03092013WP(C)4692010.txt  \n",
            "  inflating: NLP Assignment/test_data/BR14072017OJC28402001.txt  \n",
            "  inflating: NLP Assignment/test_data/DD04072016RSA362004.txt  \n",
            "  inflating: NLP Assignment/test_data/AKR07122015WP(C)14162006.txt  \n",
            "  inflating: NLP Assignment/test_data/AKR03032017WP(C)95002012.txt  \n",
            "  inflating: NLP Assignment/test_data/SKS02112017CRLA6072010.txt  \n",
            "  inflating: NLP Assignment/test_data/DPC07112017WP(C)36682017.txt  \n",
            "  inflating: NLP Assignment/test_data/IM18042012ARBA252007.txt  \n",
            "  inflating: NLP Assignment/test_data/SKS26032018CRLMC22592006.txt  \n",
            "  inflating: NLP Assignment/test_data/DD17062015RSA4792006.txt  \n",
            "  inflating: NLP Assignment/test_data/KRM15092017MATA952015.txt  \n",
            "  inflating: NLP Assignment/test_data/PM16042010JCRA242001.txt  \n",
            "  inflating: NLP Assignment/test_data/VGG25042012WP(C)62872012.txt  \n",
            "  inflating: NLP Assignment/test_data/WPC22642008.txt  \n",
            "  inflating: NLP Assignment/test_data/WPC41592014.txt  \n",
            "  inflating: NLP Assignment/test_data/WPC36552010.txt  \n",
            "  inflating: NLP Assignment/test_data/RFA52006.txt  \n",
            "  inflating: NLP Assignment/test_data/WPC33812015.txt  \n",
            "  inflating: NLP Assignment/test_data/RSA102005.txt  \n",
            "  inflating: NLP Assignment/test_data/WPC74572013.txt  \n",
            "  inflating: NLP Assignment/test_data/WPC18552008.txt  \n",
            "  inflating: NLP Assignment/test_data/WPC29592008.txt  \n",
            "  inflating: NLP Assignment/test_data/WPC66782013.txt  \n",
            "  inflating: NLP Assignment/test_data/RSA172006.txt  \n",
            "  inflating: NLP Assignment/test_data/WA2952013.txt  \n",
            "  inflating: NLP Assignment/test_data/MFA1272004.txt  \n",
            "  inflating: NLP Assignment/test_data/WPC53882008.txt  \n",
            "  inflating: NLP Assignment/test_data/WPC20732008.txt  \n",
            "  inflating: NLP Assignment/test_data/WPC10222008.txt  \n",
            "  inflating: NLP Assignment/test_data/CrlAJ552012.txt  \n",
            "  inflating: NLP Assignment/test_data/WPC48532013.txt  \n",
            "  inflating: NLP Assignment/test_data/WPC26732014.txt  \n",
            "  inflating: NLP Assignment/test_data/RFA382006.txt  \n",
            "  inflating: NLP Assignment/test_data/WPC40662012.txt  \n",
            "  inflating: NLP Assignment/Assignmnent.docx  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7aC2B5Pmh8JW"
      },
      "source": [
        "## Data prepration \n",
        "\n",
        "1.   CPU bound , I/O bounds programme fastening up the data prepation process\n",
        "\n",
        "*   >threadpools for fast accessing the files store them in list of list \n",
        "*   >processpool for fast computation for the function\n",
        "\n",
        "2. Format of data\n",
        "* >metadata : sentence_id , doc_id , word , tag\n",
        "\n",
        "3. Strategy \n",
        "\n",
        "\n",
        "1. > using threadpool programmes we will be accessing and reading each files and store them in hashmap data structure \n",
        "2. > using processpool programmes we will be accessing hashmap to process each each key value pairs and store them in final_csv for training the model\n",
        "3. > cleanning of data is required over special chars to search in paragraph for getting the annotation done\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fpdpMthIhI7s",
        "outputId": "47a7ed1c-b015-464c-c588-1d66a3af82cb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "import pandas as pd\n",
        "import concurrent.futures\n",
        "import string\n",
        "import numpy as np\n",
        "import time \n",
        "import nltk.data \n",
        "import os\n",
        "import re\n",
        "from collections import defaultdict\n",
        "from nltk.tokenize import regexp_tokenize \n",
        "import torch.nn.functional as F\n",
        "import os\n",
        "\n",
        "import torch\n",
        "from torch.optim import Adam\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "from pytorch_pretrained_bert import BertTokenizer, BertConfig\n",
        "from transformers import BertForTokenClassification, AdamW\n",
        "from seqeval.metrics import f1_score\n",
        "from seqeval.metrics import classification_report,accuracy_score,f1_score\n",
        "\n",
        "from tqdm import tqdm, trange\n",
        "\n",
        "nltk.download(\"punkt\")\n",
        "# Loading PunktSentenceTokenizer using English pickle file \n",
        "sent_tokenizer = nltk.data.load('tokenizers/punkt/PY3/english.pickle') "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "57Uve1_IDsV4",
        "outputId": "363c7ce5-5b2b-4aba-a35e-329137f3cd77",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 598
        }
      },
      "source": [
        "!pip install pytorch-pretrained-bert"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pytorch-pretrained-bert\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d7/e0/c08d5553b89973d9a240605b9c12404bcf8227590de62bae27acbcfe076b/pytorch_pretrained_bert-0.6.2-py3-none-any.whl (123kB)\n",
            "\r\u001b[K     |██▋                             | 10kB 24.9MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 20kB 3.5MB/s eta 0:00:01\r\u001b[K     |████████                        | 30kB 4.5MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 40kB 4.9MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 51kB 4.1MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 61kB 4.5MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 71kB 4.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 81kB 5.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 92kB 5.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 102kB 5.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 112kB 5.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 122kB 5.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 133kB 5.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (1.18.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (2.23.0)\n",
            "Collecting boto3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e7/4f/9078e9914f3a6d07cff4d0aed269e33c45d5830042737eb03d9f14520e1d/boto3-1.15.13.tar.gz (97kB)\n",
            "\u001b[K     |████████████████████████████████| 102kB 8.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (1.6.0+cu101)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (4.41.1)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (2019.12.20)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert) (2020.6.20)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert) (1.24.3)\n",
            "Collecting botocore<1.19.0,>=1.18.13\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/82/92/accb59480b10009ac3b07d01b5fa05f8b8dac9f633c9d117c2ca89c40fd6/botocore-1.18.13-py2.py3-none-any.whl (6.7MB)\n",
            "\u001b[K     |████████████████████████████████| 6.7MB 15.7MB/s \n",
            "\u001b[?25hCollecting jmespath<1.0.0,>=0.7.1\n",
            "  Downloading https://files.pythonhosted.org/packages/07/cb/5f001272b6faeb23c1c9e0acc04d48eaaf5c862c17709d20e3469c6e0139/jmespath-0.10.0-py2.py3-none-any.whl\n",
            "Collecting s3transfer<0.4.0,>=0.3.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/69/79/e6afb3d8b0b4e96cefbdc690f741d7dd24547ff1f94240c997a26fa908d3/s3transfer-0.3.3-py2.py3-none-any.whl (69kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 9.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch>=0.4.1->pytorch-pretrained-bert) (0.16.0)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.19.0,>=1.18.13->boto3->pytorch-pretrained-bert) (2.8.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.19.0,>=1.18.13->boto3->pytorch-pretrained-bert) (1.15.0)\n",
            "Building wheels for collected packages: boto3\n",
            "  Building wheel for boto3 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for boto3: filename=boto3-1.15.13-py2.py3-none-any.whl size=127861 sha256=a2c26a33b5d3a7ed009acba6469ae97890fc6680e5f8f88f861c0a07b9acc4ef\n",
            "  Stored in directory: /root/.cache/pip/wheels/35/5a/eb/fe69289e84a5dddca3cceac736e265215b1eee4adc65c48c65\n",
            "Successfully built boto3\n",
            "Installing collected packages: jmespath, botocore, s3transfer, boto3, pytorch-pretrained-bert\n",
            "Successfully installed boto3-1.15.13 botocore-1.18.13 jmespath-0.10.0 pytorch-pretrained-bert-0.6.2 s3transfer-0.3.3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yHzqHZkmiLWx",
        "outputId": "221c5c2a-ae90-4fc2-a0a4-3a9b3992fe7a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "#reading of files storing it as hashmap\n",
        "\n",
        "def read_files(filename):\n",
        "  store_data={}\n",
        "  with open(filename) as f:\n",
        "    data=f.read().splitlines()\n",
        "    store_data[filename]=data\n",
        "    return store_data\n",
        "start=time.time()\n",
        "\n",
        "filenames_train=[\"/content/NLP Assignment/train_data/\"+i for i in os.listdir(\"/content/NLP Assignment/train_data\")]\n",
        "filenames_test=[\"/content/NLP Assignment/test_data/\"+i for i in os.listdir(\"/content/NLP Assignment/test_data\")]\n",
        "pool = concurrent.futures.ThreadPoolExecutor(max_workers=10)\n",
        "\n",
        "train_data=pool.map(read_files,filenames_train)\n",
        "train_data=list(train_data)\n",
        "\n",
        "test_data=pool.map(read_files,filenames_test)\n",
        "test_data=list(test_data)\n",
        "\n",
        "end=time.time()\n",
        "print(\"prepation function execution :\", abs(end-start),\"secs\")\n",
        "print(\"length of train_data files: \",len(train_data))\n",
        "print(\"length of test_data files: \",len(test_data))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "prepation function execution : 0.3264503479003906 secs\n",
            "length of train_data files:  1030\n",
            "length of test_data files:  231\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XbUS49p2h_gm"
      },
      "source": [
        "#### Sentence tokenizer has issue making in our data due space and punctuations non-uniformity some judges names are not able catch so we will be using whole text and then move on with the same strategy we had earlier\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "icskehofy9If",
        "outputId": "91d15cbe-c769-410c-c20d-7443f40866e7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "# process pool executor taking same time as 4 for loop so avoid that concurrent for time being \n",
        "# small cleaning function and lot more things can be done too \n",
        "def cleaning_txt(data,data_split=None):\n",
        "  data_pairs=defaultdict(list)\n",
        "  start=time.time()\n",
        "  for filenames in data:\n",
        "    for key , values in filenames.items():\n",
        "      for lines in values:\n",
        "        for line in sent_tokenizer.tokenize(lines):\n",
        "          line=' '.join(line.split())\n",
        "          data_pairs[key].append(line)\n",
        "  end=time.time()\n",
        "  print(data_split)\n",
        "  print(\"prepation function execution :\", abs(end-start),\"secs\")\n",
        "  return data_pairs\n",
        "\n",
        "data_train_pairs=cleaning_txt(train_data,\"train\")\n",
        "data_test_pairs=cleaning_txt(test_data,\"test\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train\n",
            "prepation function execution : 5.64803409576416 secs\n",
            "test\n",
            "prepation function execution : 1.244990348815918 secs\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hE-ufErTqIld"
      },
      "source": [
        "#### Final stage of Data preparation part is started where we will making our desired format to train it on neural nets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lVhRobYvrCAi"
      },
      "source": [
        "### Strategy for final part\n",
        "#### we will taking up train.csv and it's label data \n",
        "#### another hashmap we will be needing to perform this function \n",
        "#### names vs file.txt (to handle linear collision multikey-index dictionary)\n",
        "#### will search in each key file sentence if we found that we will replace that string with empty value and tokenize the whole modified string. \n",
        "#### so that end output will be like the way we want.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QhBLwbWFJkqZ"
      },
      "source": [
        "#### Data engg stage 1 forming final dictionaries for train and test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F9vrG2QN5meI"
      },
      "source": [
        "def key_file_pair(filename):\n",
        "  df=pd.read_csv(filename)\n",
        "  csv_key=defaultdict(list)\n",
        "  for file_name,label in zip(df[\"File_Name\"],df[\"Name\"]):\n",
        "    labels=label.split(\",\")\n",
        "    csv_key['/content/NLP Assignment/train_data/'+file_name]=labels\n",
        "  return csv_key"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_SiorC_KD6wx"
      },
      "source": [
        "csv_key_train=key_file_pair(\"/content/NLP Assignment/train.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QF5_cd1jrnIh"
      },
      "source": [
        "def data_prepation_stage1(csv_key,data_pairs,data=None,csv_test_key=None):\n",
        "  if data==\"train\":\n",
        "    final_dictionary=defaultdict(list)\n",
        "    for key , names in csv_key.items():\n",
        "      if key in data_pairs:\n",
        "        for name in names:\n",
        "          contents=data_pairs[key]\n",
        "          for content in contents:\n",
        "            if name in content:\n",
        "              final_dictionary[key].append((name,\"Person\"))\n",
        "              content=content.replace(name,\"\")\n",
        "              word_token=regexp_tokenize(content, \"[\\w']+\")\n",
        "              bigrams = list(nltk.ngrams(word_token,2))\n",
        "              for grams in bigrams:\n",
        "                final_dictionary[key].append((grams,\"O\")) \n",
        "            else:\n",
        "              word_token=regexp_tokenize(content, \"[\\w']+\")\n",
        "              bigrams = list(nltk.ngrams(word_token,2))\n",
        "              for grams in bigrams:\n",
        "                final_dictionary[key].append((grams,\"O\"))\n",
        "  else:\n",
        "      final_dictionary=defaultdict(list)\n",
        "      for key in csv_test_key:\n",
        "        if key in data_pairs:\n",
        "          contents=data_pairs[key]\n",
        "          for content in contents:\n",
        "            word_token=regexp_tokenize(content, \"[\\w']+\")\n",
        "            bigrams = list(nltk.ngrams(word_token,2))\n",
        "            for grams in bigrams:\n",
        "              final_dictionary[key].append(grams) \n",
        "\n",
        "  return final_dictionary"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CCSeY5AFtcsc",
        "outputId": "11e292b2-c7df-450c-b924-e15ed995d381",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "final_dictionary_train=data_prepation_stage1(csv_key_train,data_train_pairs,\"train\")\n",
        "csv_test=pd.read_csv(\"/content/NLP Assignment/test.csv\")\n",
        "csv_test_key_values=\"/content/NLP Assignment/test_data/\"+csv_test[\"File_Name\"]\n",
        "final_dictionary_test=data_prepation_stage1(csv_key_train,data_test_pairs,\"test\",csv_test_key_values)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: DeprecationWarning: generator 'ngrams' raised StopIteration\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:13: DeprecationWarning: generator 'ngrams' raised StopIteration\n",
            "  del sys.path[0]\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:28: DeprecationWarning: generator 'ngrams' raised StopIteration\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QUp-kmMg5_az",
        "outputId": "5805f1e3-a788-4af9-f9ce-5b673b07826e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "final_dictionary_train['/content/NLP Assignment/train_data/10__Judgement_11-May-2018.txt']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(('IN', 'THE'), 'O'),\n",
              " (('THE', 'SUPREME'), 'O'),\n",
              " (('SUPREME', 'COURT'), 'O'),\n",
              " (('COURT', 'OF'), 'O'),\n",
              " (('OF', 'INDIA'), 'O'),\n",
              " (('CIVIL', 'APPELLATE'), 'O'),\n",
              " (('APPELLATE', 'JURISDICTION'), 'O'),\n",
              " (('CIVIL', 'APPEAL'), 'O'),\n",
              " (('APPEAL', 'No'), 'O'),\n",
              " (('No', '3159'), 'O'),\n",
              " (('3159', 'OF'), 'O'),\n",
              " (('OF', '2004'), 'O'),\n",
              " (('COMMISSIONER', 'OF'), 'O'),\n",
              " (('OF', 'CENTRAL'), 'O'),\n",
              " (('CENTRAL', 'EXCISE'), 'O'),\n",
              " (('INDORE', 'APPELLANT'), 'O'),\n",
              " (('APPELLANT', 'S'), 'O'),\n",
              " (('M', 'S'), 'O'),\n",
              " (('S', 'GRASIM'), 'O'),\n",
              " (('GRASIM', 'INDUSTRIES'), 'O'),\n",
              " (('INDUSTRIES', 'LTD'), 'O'),\n",
              " (('THROUGH', 'ITS'), 'O'),\n",
              " (('ITS', 'SECRETARY'), 'O'),\n",
              " (('SECRETARY', 'RESPONDENT'), 'O'),\n",
              " (('RESPONDENT', 'S'), 'O'),\n",
              " (('C', 'A'), 'O'),\n",
              " (('Nos', '3455'), 'O'),\n",
              " (('3455', '2004'), 'O'),\n",
              " (('2004', '7272'), 'O'),\n",
              " (('7272', '2005'), 'O'),\n",
              " (('2005', '2982'), 'O'),\n",
              " (('2982', '2985'), 'O'),\n",
              " (('2985', '2005'), 'O'),\n",
              " (('2005', '2986'), 'O'),\n",
              " (('2986', '2005'), 'O'),\n",
              " (('7143', '2005'), 'O'),\n",
              " (('2005', '2261'), 'O'),\n",
              " (('2261', '2006'), 'O'),\n",
              " (('2006', '2246'), 'O'),\n",
              " (('2246', '2247'), 'O'),\n",
              " (('2247', '2008'), 'O'),\n",
              " (('2008', '2934'), 'O'),\n",
              " (('2934', '2935'), 'O'),\n",
              " (('2935', '2008'), 'O'),\n",
              " (('2008', '3528'), 'O'),\n",
              " (('3528', '2008'), 'O'),\n",
              " (('4820', '2008'), 'O'),\n",
              " (('2008', '6695'), 'O'),\n",
              " (('6695', '2008'), 'O'),\n",
              " (('2008', '2534'), 'O'),\n",
              " (('2534', '2009'), 'O'),\n",
              " (('2009', '253'), 'O'),\n",
              " (('253', '2010'), 'O'),\n",
              " (('2010', '8541'), 'O'),\n",
              " (('8541', '2009'), 'O'),\n",
              " (('2009', '445'), 'O'),\n",
              " (('445', '2010'), 'O'),\n",
              " (('1382', '2010'), 'O'),\n",
              " (('2010', '2003'), 'O'),\n",
              " (('2003', '2004'), 'O'),\n",
              " (('2004', '2010'), 'O'),\n",
              " (('2010', '2430'), 'O'),\n",
              " (('2430', '2010'), 'O'),\n",
              " (('2010', '2363'), 'O'),\n",
              " (('2363', '2010'), 'O'),\n",
              " (('2010', '7174'), 'O'),\n",
              " (('7174', '7175'), 'O'),\n",
              " (('7175', '2010'), 'O'),\n",
              " (('4696', '2011'), 'O'),\n",
              " (('2011', '6984'), 'O'),\n",
              " (('6984', '2011'), 'O'),\n",
              " (('2011', '2705'), 'O'),\n",
              " (('2705', '2012'), 'O'),\n",
              " (('RANJAN', 'GOGOI'), 'O'),\n",
              " (('GOGOI', 'J'), 'O'),\n",
              " (('First', 'the'), 'O'),\n",
              " (('the', 'facts'), 'O'),\n",
              " (('The', 'respondent'), 'O'),\n",
              " (('respondent', 'Assessees'), 'O'),\n",
              " (('Assessees', 'are'), 'O'),\n",
              " (('are', 'manufacturers'), 'O'),\n",
              " (('manufacturers', 'of'), 'O'),\n",
              " (('of', 'dissolved'), 'O'),\n",
              " (('dissolved', 'and'), 'O'),\n",
              " (('compressed', 'industrial'), 'O'),\n",
              " (('industrial', 'gases'), 'O'),\n",
              " (('gases', 'liquid'), 'O'),\n",
              " (('liquid', 'chlorine'), 'O'),\n",
              " (('chlorine', 'and'), 'O'),\n",
              " (('and', 'other'), 'O'),\n",
              " (('other', 'allied'), 'O'),\n",
              " (('allied', 'products'), 'O'),\n",
              " (('Cotton', 'yarn'), 'O'),\n",
              " (('yarn', 'and'), 'O'),\n",
              " (('and', 'Post'), 'O'),\n",
              " (('Post', 'Mix'), 'O'),\n",
              " (('Mix', 'Concentrate'), 'O'),\n",
              " (('Concentrate', 'manufactured'), 'O'),\n",
              " (('manufactured', 'by'), 'O'),\n",
              " (('by', 'two'), 'O'),\n",
              " (('two', 'other'), 'O'),\n",
              " (('individual', 'assessees'), 'O'),\n",
              " (('assessees', 'are'), 'O'),\n",
              " (('are', 'also'), 'O'),\n",
              " (('also', 'in'), 'O'),\n",
              " (('in', 'issue'), 'O'),\n",
              " (('These', 'articles'), 'O'),\n",
              " (('articles', 'are'), 'O'),\n",
              " (('are', 'supplied'), 'O'),\n",
              " (('supplied', 'to'), 'O'),\n",
              " (('to', 'the'), 'O'),\n",
              " (('customers', 'in'), 'O'),\n",
              " (('in', 'tonners'), 'O'),\n",
              " (('tonners', 'cylinders'), 'O'),\n",
              " (('cylinders', 'carboys'), 'O'),\n",
              " (('carboys', 'paper'), 'O'),\n",
              " (('paper', 'cones'), 'O'),\n",
              " (('cones', 'and'), 'O'),\n",
              " (('and', 'HDPE'), 'O'),\n",
              " (('HDPE', 'bags'), 'O'),\n",
              " (('BIBs', 'pipeline'), 'O'),\n",
              " (('pipeline', 'and'), 'O'),\n",
              " (('and', 'canisters'), 'O'),\n",
              " (('canisters', 'which'), 'O'),\n",
              " (('which', 'may'), 'O'),\n",
              " (('may', 'be'), 'O'),\n",
              " (('be', 'more'), 'O'),\n",
              " (('more', 'conveniently'), 'O'),\n",
              " (('conveniently', 'referred'), 'O'),\n",
              " (('referred', 'to'), 'O'),\n",
              " (('as', 'containers'), 'O'),\n",
              " (('In', 'some'), 'O'),\n",
              " (('some', 'cases'), 'O'),\n",
              " (('cases', 'the'), 'O'),\n",
              " (('the', 'containers'), 'O'),\n",
              " (('containers', 'are'), 'O'),\n",
              " (('are', 'provided'), 'O'),\n",
              " (('provided', 'by'), 'O'),\n",
              " (('by', 'the'), 'O'),\n",
              " (('Assessees', 'to'), 'O'),\n",
              " (('to', 'the'), 'O'),\n",
              " (('the', 'customers'), 'O'),\n",
              " (('customers', 'on'), 'O'),\n",
              " (('on', 'rent'), 'O'),\n",
              " (('rent', 'whereas'), 'O'),\n",
              " (('whereas', 'in'), 'O'),\n",
              " (('in', 'others'), 'O'),\n",
              " (('others', 'the'), 'O'),\n",
              " (('the', 'customers'), 'O'),\n",
              " (('bring', 'their'), 'O'),\n",
              " (('their', 'own'), 'O'),\n",
              " (('own', 'containers'), 'O'),\n",
              " (('For', 'making'), 'O'),\n",
              " (('making', 'available'), 'O'),\n",
              " (('available', 'or'), 'O'),\n",
              " (('or', 'for'), 'O'),\n",
              " (('for', 'filling'), 'O'),\n",
              " (('filling', 'up'), 'O'),\n",
              " (('up', 'the'), 'O'),\n",
              " (('containers', 'provided'), 'O'),\n",
              " (('provided', 'by'), 'O'),\n",
              " (('by', 'the'), 'O'),\n",
              " (('the', 'customers'), 'O'),\n",
              " (('customers', 'the'), 'O'),\n",
              " (('the', 'Assessees'), 'O'),\n",
              " (('Assessees', 'charge'), 'O'),\n",
              " (('charge', 'the'), 'O'),\n",
              " (('customers', 'certain'), 'O'),\n",
              " (('certain', 'amounts'), 'O'),\n",
              " (('amounts', 'under'), 'O'),\n",
              " (('under', 'different'), 'O'),\n",
              " (('different', 'heads'), 'O'),\n",
              " (('heads', 'viz'), 'O'),\n",
              " (('packing', 'charges'), 'O'),\n",
              " (('wear', 'and'), 'O'),\n",
              " (('and', 'tear'), 'O'),\n",
              " (('tear', 'charges'), 'O'),\n",
              " (('charges', 'facility'), 'O'),\n",
              " (('facility', 'charges'), 'O'),\n",
              " (('charges', 'service'), 'O'),\n",
              " (('service', 'charges'), 'O'),\n",
              " (('charges', 'delivery'), 'O'),\n",
              " (('delivery', 'and'), 'O'),\n",
              " (('collection', 'charges'), 'O'),\n",
              " (('charges', 'rental'), 'O'),\n",
              " (('rental', 'charges'), 'O'),\n",
              " (('charges', 'repair'), 'O'),\n",
              " (('repair', 'and'), 'O'),\n",
              " (('and', 'testing'), 'O'),\n",
              " (('testing', 'charges'), 'O'),\n",
              " (('Assessees', 'treat'), 'O'),\n",
              " (('treat', 'the'), 'O'),\n",
              " (('the', 'said'), 'O'),\n",
              " (('said', 'amounts'), 'O'),\n",
              " (('amounts', 'as'), 'O'),\n",
              " (('as', 'their'), 'O'),\n",
              " (('their', 'income'), 'O'),\n",
              " (('income', 'from'), 'O'),\n",
              " (('from', 'ancillary'), 'O'),\n",
              " (('ancillary', 'or'), 'O'),\n",
              " (('or', 'allied'), 'O'),\n",
              " (('The', 'issue'), 'O'),\n",
              " (('issue', 'arising'), 'O'),\n",
              " (('arising', 'is'), 'O'),\n",
              " (('is', 'whether'), 'O'),\n",
              " (('whether', 'the'), 'O'),\n",
              " (('the', 'aforesaid'), 'O'),\n",
              " (('aforesaid', 'charges'), 'O'),\n",
              " (('charges', 'realised'), 'O'),\n",
              " (('realised', 'by'), 'O'),\n",
              " (('by', 'the'), 'O'),\n",
              " (('Assessees', 'are'), 'O'),\n",
              " (('are', 'liable'), 'O'),\n",
              " (('liable', 'to'), 'O'),\n",
              " (('to', 'be'), 'O'),\n",
              " (('be', 'taken'), 'O'),\n",
              " (('taken', 'into'), 'O'),\n",
              " (('into', 'account'), 'O'),\n",
              " (('account', 'for'), 'O'),\n",
              " (('for', 'determination'), 'O'),\n",
              " (('determination', 'of'), 'O'),\n",
              " (('of', 'value'), 'O'),\n",
              " (('for', 'the'), 'O'),\n",
              " (('the', 'purpose'), 'O'),\n",
              " (('purpose', 'of'), 'O'),\n",
              " (('of', 'levy'), 'O'),\n",
              " (('levy', 'of'), 'O'),\n",
              " (('of', 'duty'), 'O'),\n",
              " (('duty', 'in'), 'O'),\n",
              " (('in', 'terms'), 'O'),\n",
              " (('terms', 'of'), 'O'),\n",
              " (('of', 'Section'), 'O'),\n",
              " (('Section', '4'), 'O'),\n",
              " (('4', 'of'), 'O'),\n",
              " (('of', 'the'), 'O'),\n",
              " (('the', 'Central'), 'O'),\n",
              " (('Central', 'Excise'), 'O'),\n",
              " (('Act', '1944'), 'O'),\n",
              " (('1944', 'hereinafter'), 'O'),\n",
              " (('hereinafter', 'referred'), 'O'),\n",
              " (('referred', 'to'), 'O'),\n",
              " (('to', 'as'), 'O'),\n",
              " (('as', 'the'), 'O'),\n",
              " (('the', 'Act'), 'O'),\n",
              " (('Act', 'as'), 'O'),\n",
              " (('as', 'amended'), 'O'),\n",
              " (('amended', 'with'), 'O'),\n",
              " (('with', 'effect'), 'O'),\n",
              " (('from', '1st'), 'O'),\n",
              " (('1st', 'July'), 'O'),\n",
              " (('July', '2000'), 'O'),\n",
              " (('Perceiving', 'a'), 'O'),\n",
              " (('a', 'conflict'), 'O'),\n",
              " (('conflict', 'between'), 'O'),\n",
              " (('between', 'the'), 'O'),\n",
              " (('the', 'two'), 'O'),\n",
              " (('two', 'decisions'), 'O'),\n",
              " (('decisions', 'of'), 'O'),\n",
              " (('of', 'this'), 'O'),\n",
              " (('this', 'court'), 'O'),\n",
              " (('court', 'in'), 'O'),\n",
              " (('Union', 'of'), 'O'),\n",
              " (('of', 'India'), 'O'),\n",
              " (('India', 'and'), 'O'),\n",
              " (('and', 'Ors'), 'O'),\n",
              " (('v', 'Bombay'), 'O'),\n",
              " (('Bombay', 'Tyre'), 'O'),\n",
              " (('Tyre', 'International'), 'O'),\n",
              " (('International', 'Ltd'), 'O'),\n",
              " (('Ltd', 'and'), 'O'),\n",
              " (('and', 'Ors'), 'O'),\n",
              " (('and', 'Commissioner'), 'O'),\n",
              " (('Commissioner', 'of'), 'O'),\n",
              " (('of', 'Central'), 'O'),\n",
              " (('Central', 'Excise'), 'O'),\n",
              " (('Excise', 'Pondicherry'), 'O'),\n",
              " (('Pondicherry', 'v'), 'O'),\n",
              " (('v', 'Acer'), 'O'),\n",
              " (('Acer', 'India'), 'O'),\n",
              " (('India', 'Ltd'), 'O'),\n",
              " (('Ltd', '2'), 'O'),\n",
              " (('a', 'two'), 'O'),\n",
              " (('two', 'judge'), 'O'),\n",
              " (('judge', 'Bench'), 'O'),\n",
              " (('Bench', 'of'), 'O'),\n",
              " (('of', 'this'), 'O'),\n",
              " (('this', 'Court'), 'O'),\n",
              " (('Court', 'by'), 'O'),\n",
              " (('by', 'order'), 'O'),\n",
              " (('order', 'dated'), 'O'),\n",
              " (('dated', '30'), 'O'),\n",
              " (('30', 'th'), 'O'),\n",
              " (('th', 'July'), 'O'),\n",
              " (('July', '20093'), 'O'),\n",
              " (('20093', 'referred'), 'O'),\n",
              " (('the', 'following'), 'O'),\n",
              " (('following', 'questions'), 'O'),\n",
              " (('questions', 'for'), 'O'),\n",
              " (('for', 'an'), 'O'),\n",
              " (('an', 'answer'), 'O'),\n",
              " (('answer', 'by'), 'O'),\n",
              " (('by', 'a'), 'O'),\n",
              " (('a', 'larger'), 'O'),\n",
              " (('larger', 'bench'), 'O'),\n",
              " (('1', '1984'), 'O'),\n",
              " (('1984', '1'), 'O'),\n",
              " (('1', 'SCC'), 'O'),\n",
              " (('SCC', '467'), 'O'),\n",
              " (('2', '2004'), 'O'),\n",
              " (('2004', '8'), 'O'),\n",
              " (('8', 'SCC'), 'O'),\n",
              " (('SCC', '173'), 'O'),\n",
              " (('3', '2009'), 'O'),\n",
              " (('2009', '14'), 'O'),\n",
              " (('14', 'SCC'), 'O'),\n",
              " (('SCC', '596'), 'O'),\n",
              " (('Whether', 'Section'), 'O'),\n",
              " (('Section', '4'), 'O'),\n",
              " (('4', 'of'), 'O'),\n",
              " (('of', 'the'), 'O'),\n",
              " (('the', 'Central'), 'O'),\n",
              " (('Central', 'Excise'), 'O'),\n",
              " (('Excise', 'Act'), 'O'),\n",
              " (('Act', '1944'), 'O'),\n",
              " (('as', 'substituted'), 'O'),\n",
              " (('substituted', 'with'), 'O'),\n",
              " (('with', 'effect'), 'O'),\n",
              " (('effect', 'from'), 'O'),\n",
              " (('from', '1'), 'O'),\n",
              " (('1', '7'), 'O'),\n",
              " (('7', '2000'), 'O'),\n",
              " (('2000', 'and'), 'O'),\n",
              " (('and', 'the'), 'O'),\n",
              " (('definition', 'of'), 'O'),\n",
              " (('of', 'transaction'), 'O'),\n",
              " (('transaction', 'value'), 'O'),\n",
              " (('value', 'in'), 'O'),\n",
              " (('in', 'clause'), 'O'),\n",
              " (('clause', 'd'), 'O'),\n",
              " (('d', 'of'), 'O'),\n",
              " (('of', 'sub'), 'O'),\n",
              " (('section', '3'), 'O'),\n",
              " (('3', 'of'), 'O'),\n",
              " (('of', 'Section'), 'O'),\n",
              " (('Section', '4'), 'O'),\n",
              " (('4', 'are'), 'O'),\n",
              " (('are', 'subject'), 'O'),\n",
              " (('subject', 'to'), 'O'),\n",
              " (('to', 'Section'), 'O'),\n",
              " (('Section', '3'), 'O'),\n",
              " (('3', 'of'), 'O'),\n",
              " (('of', 'the'), 'O'),\n",
              " (('Whether', 'Sections'), 'O'),\n",
              " (('Sections', '3'), 'O'),\n",
              " (('3', 'and'), 'O'),\n",
              " (('and', '4'), 'O'),\n",
              " (('4', 'of'), 'O'),\n",
              " (('of', 'the'), 'O'),\n",
              " (('the', 'Central'), 'O'),\n",
              " (('Central', 'Excise'), 'O'),\n",
              " (('Act', 'despite'), 'O'),\n",
              " (('despite', 'being'), 'O'),\n",
              " (('being', 'interlinked'), 'O'),\n",
              " (('interlinked', 'operate'), 'O'),\n",
              " (('operate', 'in'), 'O'),\n",
              " (('in', 'different'), 'O'),\n",
              " (('different', 'fields'), 'O'),\n",
              " (('and', 'what'), 'O'),\n",
              " (('what', 'is'), 'O'),\n",
              " (('is', 'their'), 'O'),\n",
              " (('their', 'real'), 'O'),\n",
              " (('real', 'scope'), 'O'),\n",
              " (('scope', 'and'), 'O'),\n",
              " (('and', 'ambit'), 'O'),\n",
              " (('Whether', 'the'), 'O'),\n",
              " (('the', 'concept'), 'O'),\n",
              " (('concept', 'of'), 'O'),\n",
              " (('of', 'transaction'), 'O'),\n",
              " (('transaction', 'value'), 'O'),\n",
              " (('value', 'makes'), 'O'),\n",
              " (('any', 'material'), 'O'),\n",
              " (('material', 'departure'), 'O'),\n",
              " (('departure', 'from'), 'O'),\n",
              " (('from', 'the'), 'O'),\n",
              " (('the', 'deemed'), 'O'),\n",
              " (('deemed', 'normal'), 'O'),\n",
              " (('normal', 'price'), 'O'),\n",
              " (('concept', 'of'), 'O'),\n",
              " (('of', 'the'), 'O'),\n",
              " (('the', 'erstwhile'), 'O'),\n",
              " (('erstwhile', 'Section'), 'O'),\n",
              " (('Section', '4'), 'O'),\n",
              " (('4', '1'), 'O'),\n",
              " (('1', 'a'), 'O'),\n",
              " (('a', 'of'), 'O'),\n",
              " (('of', 'the'), 'O'),\n",
              " (('the', 'Act'), 'O'),\n",
              " (('As', 'the'), 'O'),\n",
              " (('the', 'decisions'), 'O'),\n",
              " (('decisions', 'in'), 'O'),\n",
              " (('in', 'Bombay'), 'O'),\n",
              " (('Bombay', 'Tyre'), 'O'),\n",
              " (('Tyre', 'International'), 'O'),\n",
              " (('International', 'Ltd'), 'O'),\n",
              " (('Ltd', 'supra'), 'O'),\n",
              " (('supra', 'and'), 'O'),\n",
              " (('Acer', 'India'), 'O'),\n",
              " (('India', 'Ltd'), 'O'),\n",
              " (('Ltd', 'supra'), 'O'),\n",
              " (('supra', 'were'), 'O'),\n",
              " (('were', 'rendered'), 'O'),\n",
              " (('rendered', 'by'), 'O'),\n",
              " (('by', 'Benches'), 'O'),\n",
              " (('Benches', 'of'), 'O'),\n",
              " (('of', 'Three'), 'O'),\n",
              " (('Three', 'Hon'), 'O'),\n",
              " (('Hon', 'ble'), 'O'),\n",
              " (('Judges', 'of'), 'O'),\n",
              " (('of', 'this'), 'O'),\n",
              " (('this', 'Court'), 'O'),\n",
              " (('Court', 'the'), 'O'),\n",
              " (('the', 'above'), 'O'),\n",
              " (('above', 'questions'), 'O'),\n",
              " (('questions', 'were'), 'O'),\n",
              " (('were', 'referred'), 'O'),\n",
              " (('referred', 'by'), 'O'),\n",
              " (('by', 'order'), 'O'),\n",
              " (('order', 'dated'), 'O'),\n",
              " (('30th', 'March'), 'O'),\n",
              " (('March', '20164'), 'O'),\n",
              " (('20164', 'to'), 'O'),\n",
              " (('to', 'an'), 'O'),\n",
              " (('an', 'even'), 'O'),\n",
              " (('even', 'larger'), 'O'),\n",
              " (('larger', 'Bench'), 'O'),\n",
              " (('This', 'is'), 'O'),\n",
              " (('is', 'how'), 'O'),\n",
              " (('how', 'we'), 'O'),\n",
              " (('we', 'are'), 'O'),\n",
              " (('are', 'in'), 'O'),\n",
              " (('in', 'seisin'), 'O'),\n",
              " (('seisin', 'of'), 'O'),\n",
              " (('the', 'matter'), 'O'),\n",
              " (('What', 'is'), 'O'),\n",
              " (('is', 'excise'), 'O'),\n",
              " (('excise', 'duty'), 'O'),\n",
              " (('duty', 'and'), 'O'),\n",
              " (('and', 'what'), 'O'),\n",
              " (('what', 'is'), 'O'),\n",
              " (('is', 'the'), 'O'),\n",
              " (('the', 'relationship'), 'O'),\n",
              " (('relationship', 'between'), 'O'),\n",
              " (('between', 'the'), 'O'),\n",
              " (('the', 'nature'), 'O'),\n",
              " (('of', 'the'), 'O'),\n",
              " (('the', 'duty'), 'O'),\n",
              " (('duty', 'and'), 'O'),\n",
              " (('and', 'the'), 'O'),\n",
              " (('the', 'measure'), 'O'),\n",
              " (('measure', 'of'), 'O'),\n",
              " (('of', 'the'), 'O'),\n",
              " (('the', 'levy'), 'O'),\n",
              " (('levy', 'are'), 'O'),\n",
              " (('are', 'the'), 'O'),\n",
              " (('the', 'two'), 'O'),\n",
              " (('two', 'precise'), 'O'),\n",
              " (('precise', 'questions'), 'O'),\n",
              " (('questions', 'that'), 'O'),\n",
              " (('would', 'arise'), 'O'),\n",
              " (('arise', 'for'), 'O'),\n",
              " (('for', 'determination'), 'O'),\n",
              " (('determination', 'in'), 'O'),\n",
              " (('in', 'the'), 'O'),\n",
              " (('the', 'present'), 'O'),\n",
              " (('present', 'reference'), 'O'),\n",
              " (('On', 'first'), 'O'),\n",
              " (('first', 'principles'), 'O'),\n",
              " (('principles', 'there'), 'O'),\n",
              " (('there', 'can'), 'O'),\n",
              " (('can', 'be'), 'O'),\n",
              " (('be', 'no'), 'O'),\n",
              " (('no', 'dispute'), 'O'),\n",
              " (('Excise', 'is'), 'O'),\n",
              " (('is', 'a'), 'O'),\n",
              " (('a', 'levy'), 'O'),\n",
              " (('levy', 'on'), 'O'),\n",
              " (('manufacture', 'and'), 'O'),\n",
              " (('and', 'upon'), 'O'),\n",
              " (('upon', 'the'), 'O'),\n",
              " (('the', 'manufacturer'), 'O'),\n",
              " (('manufacturer', 'who'), 'O'),\n",
              " (('who', 'is'), 'O'),\n",
              " (('is', 'entitled'), 'O'),\n",
              " (('entitled', 'under'), 'O'),\n",
              " (('under', 'law'), 'O'),\n",
              " (('law', 'to'), 'O'),\n",
              " (('to', 'pass'), 'O'),\n",
              " (('on', 'the'), 'O'),\n",
              " (('the', 'burden'), 'O'),\n",
              " (('burden', 'to'), 'O'),\n",
              " (('to', 'the'), 'O'),\n",
              " (('the', 'first'), 'O'),\n",
              " (('first', 'purchaser'), 'O'),\n",
              " (('purchaser', 'of'), 'O'),\n",
              " (('of', 'the'), 'O'),\n",
              " (('the', 'manufactured'), 'O'),\n",
              " (('manufactured', 'goods'), 'O'),\n",
              " (('The', 'levy'), 'O'),\n",
              " (('of', 'excise'), 'O'),\n",
              " (('excise', 'flows'), 'O'),\n",
              " (('flows', 'from'), 'O'),\n",
              " (('from', 'a'), 'O'),\n",
              " (('a', 'constitutional'), 'O'),\n",
              " (('constitutional', 'authorisation'), 'O'),\n",
              " (('authorisation', 'under'), 'O'),\n",
              " (('under', 'Entry'), 'O'),\n",
              " (('Entry', '84'), 'O'),\n",
              " (('84', 'of'), 'O'),\n",
              " (('of', 'List'), 'O'),\n",
              " (('List', 'I'), 'O'),\n",
              " (('4', '2016'), 'O'),\n",
              " (('2016', '6'), 'O'),\n",
              " (('6', 'SCC'), 'O'),\n",
              " (('SCC', '391'), 'O'),\n",
              " (('of', 'the'), 'O'),\n",
              " (('the', 'Seventh'), 'O'),\n",
              " (('Seventh', 'Schedule'), 'O'),\n",
              " (('Schedule', 'to'), 'O'),\n",
              " (('to', 'the'), 'O'),\n",
              " (('the', 'Constitution'), 'O'),\n",
              " (('Constitution', 'of'), 'O'),\n",
              " (('of', 'India'), 'O'),\n",
              " (('The', 'stage'), 'O'),\n",
              " (('stage', 'of'), 'O'),\n",
              " (('collection', 'of'), 'O'),\n",
              " (('of', 'the'), 'O'),\n",
              " (('the', 'levy'), 'O'),\n",
              " (('levy', 'and'), 'O'),\n",
              " (('and', 'the'), 'O'),\n",
              " (('the', 'measure'), 'O'),\n",
              " (('measure', 'thereof'), 'O'),\n",
              " (('thereof', 'is'), 'O'),\n",
              " (('is', 'however'), 'O'),\n",
              " (('however', 'a'), 'O'),\n",
              " (('a', 'statutory'), 'O'),\n",
              " (('So', 'long'), 'O'),\n",
              " (('long', 'the'), 'O'),\n",
              " (('the', 'statutory'), 'O'),\n",
              " (('statutory', 'exercise'), 'O'),\n",
              " (('exercise', 'in'), 'O'),\n",
              " (('in', 'this'), 'O'),\n",
              " (('this', 'regard'), 'O'),\n",
              " (('regard', 'is'), 'O'),\n",
              " (('is', 'a'), 'O'),\n",
              " (('a', 'competent'), 'O'),\n",
              " (('exercise', 'of'), 'O'),\n",
              " (('of', 'legislative'), 'O'),\n",
              " (('legislative', 'power'), 'O'),\n",
              " (('power', 'the'), 'O'),\n",
              " (('the', 'legislative'), 'O'),\n",
              " (('legislative', 'wisdom'), 'O'),\n",
              " (('wisdom', 'both'), 'O'),\n",
              " (('both', 'with'), 'O'),\n",
              " (('with', 'regard'), 'O'),\n",
              " (('regard', 'to'), 'O'),\n",
              " (('the', 'stage'), 'O'),\n",
              " (('stage', 'of'), 'O'),\n",
              " (('of', 'collection'), 'O'),\n",
              " (('collection', 'and'), 'O'),\n",
              " (('and', 'the'), 'O'),\n",
              " (('the', 'measure'), 'O'),\n",
              " (('measure', 'of'), 'O'),\n",
              " (('of', 'the'), 'O'),\n",
              " (('the', 'levy'), 'O'),\n",
              " (('levy', 'must'), 'O'),\n",
              " (('must', 'be'), 'O'),\n",
              " (('be', 'allowed'), 'O'),\n",
              " (('allowed', 'to'), 'O'),\n",
              " (('The', 'measure'), 'O'),\n",
              " (('measure', 'of'), 'O'),\n",
              " (('of', 'the'), 'O'),\n",
              " (('the', 'levy'), 'O'),\n",
              " (('levy', 'must'), 'O'),\n",
              " (('must', 'not'), 'O'),\n",
              " (('not', 'be'), 'O'),\n",
              " (('be', 'confused'), 'O'),\n",
              " (('confused', 'with'), 'O'),\n",
              " (('with', 'the'), 'O'),\n",
              " (('the', 'nature'), 'O'),\n",
              " (('thereof', 'though'), 'O'),\n",
              " (('though', 'there'), 'O'),\n",
              " (('there', 'must'), 'O'),\n",
              " (('must', 'be'), 'O'),\n",
              " (('be', 'some'), 'O'),\n",
              " (('some', 'nexus'), 'O'),\n",
              " (('nexus', 'between'), 'O'),\n",
              " (('between', 'the'), 'O'),\n",
              " (('the', 'two'), 'O'),\n",
              " (('But', 'the'), 'O'),\n",
              " (('measure', 'cannot'), 'O'),\n",
              " (('cannot', 'be'), 'O'),\n",
              " (('be', 'controlled'), 'O'),\n",
              " (('controlled', 'by'), 'O'),\n",
              " (('by', 'the'), 'O'),\n",
              " (('the', 'rigors'), 'O'),\n",
              " (('rigors', 'of'), 'O'),\n",
              " (('of', 'the'), 'O'),\n",
              " (('the', 'nature'), 'O'),\n",
              " (('These', 'are'), 'O'),\n",
              " (('some', 'of'), 'O'),\n",
              " (('of', 'the'), 'O'),\n",
              " (('the', 'settled'), 'O'),\n",
              " (('settled', 'principles'), 'O'),\n",
              " (('principles', 'of'), 'O'),\n",
              " (('of', 'laws'), 'O'),\n",
              " (('laws', 'emanating'), 'O'),\n",
              " (('emanating', 'from'), 'O'),\n",
              " (('from', 'a'), 'O'),\n",
              " (('a', 'long'), 'O'),\n",
              " (('long', 'line'), 'O'),\n",
              " (('line', 'of'), 'O'),\n",
              " (('decisions', 'of'), 'O'),\n",
              " (('of', 'this'), 'O'),\n",
              " (('this', 'Court'), 'O'),\n",
              " (('Court', 'which'), 'O'),\n",
              " (('which', 'we'), 'O'),\n",
              " (('we', 'will'), 'O'),\n",
              " (('will', 'take'), 'O'),\n",
              " (('take', 'note'), 'O'),\n",
              " (('note', 'of'), 'O'),\n",
              " (('of', 'shortly'), 'O'),\n",
              " (('Do', 'these'), 'O'),\n",
              " (('principles', 'that'), 'O'),\n",
              " (('that', 'have'), 'O'),\n",
              " (('have', 'withstood'), 'O'),\n",
              " (('withstood', 'the'), 'O'),\n",
              " (('the', 'test'), 'O'),\n",
              " (('test', 'of'), 'O'),\n",
              " (('of', 'time'), 'O'),\n",
              " (('time', 'require'), 'O'),\n",
              " (('require', 'a'), 'O'),\n",
              " (('a', 'rethink'), 'O'),\n",
              " (('rethink', 'is'), 'O'),\n",
              " (('is', 'the'), 'O'),\n",
              " (('question', 'that'), 'O'),\n",
              " (('that', 'poses'), 'O'),\n",
              " (('poses', 'for'), 'O'),\n",
              " (('for', 'an'), 'O'),\n",
              " (('an', 'answer'), 'O'),\n",
              " (('answer', 'in'), 'O'),\n",
              " (('in', 'the'), 'O'),\n",
              " (('the', 'present'), 'O'),\n",
              " (('present', 'reference'), 'O'),\n",
              " (('At', 'this'), 'O'),\n",
              " (('this', 'stage'), 'O'),\n",
              " (('stage', 'it'), 'O'),\n",
              " (('it', 'may'), 'O'),\n",
              " (('may', 'be'), 'O'),\n",
              " (('be', 'necessary'), 'O'),\n",
              " (('necessary', 'to'), 'O'),\n",
              " (('to', 'specifically'), 'O'),\n",
              " (('specifically', 'take'), 'O'),\n",
              " (('take', 'note'), 'O'),\n",
              " (('note', 'of'), 'O'),\n",
              " (('of', 'the'), 'O'),\n",
              " (('provisions', 'of'), 'O'),\n",
              " (('of', 'Sections'), 'O'),\n",
              " (('Sections', '3'), 'O'),\n",
              " (('3', 'and'), 'O'),\n",
              " (('and', '4'), 'O'),\n",
              " (('4', 'as'), 'O'),\n",
              " (('as', 'originally'), 'O'),\n",
              " (('originally', 'enacted'), 'O'),\n",
              " (('enacted', 'and'), 'O'),\n",
              " (('and', 'as'), 'O'),\n",
              " (('as', 'amended'), 'O'),\n",
              " (('amended', 'from'), 'O'),\n",
              " (('time', 'to'), 'O'),\n",
              " (('to', 'time'), 'O'),\n",
              " (('Section', '3'), 'O'),\n",
              " (('Section', '3'), 'O'),\n",
              " (('3', 'of'), 'O'),\n",
              " (('of', 'the'), 'O'),\n",
              " (('the', 'Act'), 'O'),\n",
              " (('Act', 'in'), 'O'),\n",
              " (('in', 'force'), 'O'),\n",
              " (('force', 'Relevant'), 'O'),\n",
              " (('Relevant', 'portion'), 'O'),\n",
              " (('portion', 'of'), 'O'),\n",
              " (('of', 'Section'), 'O'),\n",
              " (('Section', '3'), 'O'),\n",
              " (('3', 'as'), 'O'),\n",
              " (('prior', 'to'), 'O'),\n",
              " (('to', 'amendment'), 'O'),\n",
              " (('amendment', 'by'), 'O'),\n",
              " (('by', 'Finance'), 'O'),\n",
              " (('Finance', 'substituted'), 'O'),\n",
              " (('substituted', 'amended'), 'O'),\n",
              " (('amended', 'with'), 'O'),\n",
              " (('with', 'effect'), 'O'),\n",
              " (('Act', '2000'), 'O'),\n",
              " (('2000', 'Act'), 'O'),\n",
              " (('Act', '10'), 'O'),\n",
              " (('10', 'of'), 'O'),\n",
              " (('of', '2000'), 'O'),\n",
              " (('2000', 'from'), 'O'),\n",
              " (('from', '12th'), 'O'),\n",
              " (('12th', 'May'), 'O'),\n",
              " (('May', '2000'), 'O'),\n",
              " (('2000', 'by'), 'O'),\n",
              " (('by', 'Section'), 'O'),\n",
              " (('92', 'of'), 'O'),\n",
              " (('of', 'the'), 'O'),\n",
              " (('the', 'Finance'), 'O'),\n",
              " (('Finance', 'Act'), 'O'),\n",
              " (('Act', '2000'), 'O'),\n",
              " (('No', '10'), 'O'),\n",
              " (('10', 'of'), 'O'),\n",
              " (('of', '2000'), 'O'),\n",
              " (('Duties', 'specified'), 'O'),\n",
              " (('specified', 'in'), 'O'),\n",
              " (('in', 'the'), 'O'),\n",
              " (('the', 'First'), 'O'),\n",
              " (('First', '3'), 'O'),\n",
              " (('Duties', 'specified'), 'O'),\n",
              " (('specified', 'in'), 'O'),\n",
              " (('in', 'the'), 'O'),\n",
              " (('the', 'First'), 'O'),\n",
              " (('Schedule', 'to'), 'O'),\n",
              " (('to', 'be'), 'O'),\n",
              " (('be', 'levied'), 'O'),\n",
              " (('Schedule', 'and'), 'O'),\n",
              " (('and', 'the'), 'O'),\n",
              " (('the', 'Second'), 'O'),\n",
              " (('Schedule', 'to'), 'O'),\n",
              " (('to', 'the'), 'O'),\n",
              " (('the', 'Central'), 'O'),\n",
              " (('Central', 'Excise'), 'O'),\n",
              " (('1', 'There'), 'O'),\n",
              " (('There', 'shall'), 'O'),\n",
              " (('shall', 'be'), 'O'),\n",
              " (('be', 'levied'), 'O'),\n",
              " (('levied', 'and'), 'O'),\n",
              " (('and', 'Tariff'), 'O'),\n",
              " (('Tariff', 'Act'), 'O'),\n",
              " (('Act', '1985'), 'O'),\n",
              " (('1985', 'to'), 'O'),\n",
              " (('to', 'be'), 'O'),\n",
              " (('be', 'levied'), 'O'),\n",
              " (('collected', 'in'), 'O'),\n",
              " (('in', 'such'), 'O'),\n",
              " (('such', 'manner'), 'O'),\n",
              " (('manner', 'as'), 'O'),\n",
              " (('as', 'may'), 'O'),\n",
              " (('be', 'prescribed'), 'O'),\n",
              " (('prescribed', 'There'), 'O'),\n",
              " (('There', 'shall'), 'O'),\n",
              " (('shall', 'be'), 'O'),\n",
              " (('be', 'levied'), 'O'),\n",
              " (('levied', 'and'), 'O'),\n",
              " (('and', 'collected'), 'O'),\n",
              " (('in', 'such'), 'O'),\n",
              " (('such', 'manner'), 'O'),\n",
              " (('manner', 'as'), 'O'),\n",
              " (('as', 'may'), 'O'),\n",
              " (('may', 'be'), 'O'),\n",
              " (('a', 'a'), 'O'),\n",
              " (('a', 'duty'), 'O'),\n",
              " (('duty', 'of'), 'O'),\n",
              " (('of', 'excise'), 'O'),\n",
              " (('excise', 'on'), 'O'),\n",
              " (('on', 'all'), 'O'),\n",
              " (('all', 'prescribed'), 'O'),\n",
              " (('excisable', 'goods'), 'O'),\n",
              " (('goods', 'which'), 'O'),\n",
              " (('which', 'are'), 'O'),\n",
              " (('produced', 'or'), 'O'),\n",
              " (('or', 'manufactured'), 'O'),\n",
              " (('manufactured', 'in'), 'O'),\n",
              " (('in', 'a'), 'O'),\n",
              " (('a', 'a'), 'O'),\n",
              " (('a', 'duty'), 'O'),\n",
              " (('duty', 'of'), 'O'),\n",
              " (('of', 'excise'), 'O'),\n",
              " (('excise', 'to'), 'O'),\n",
              " (('to', 'be'), 'O'),\n",
              " (('be', 'called'), 'O'),\n",
              " (('India', 'as'), 'O'),\n",
              " (('as', 'and'), 'O'),\n",
              " (('and', 'at'), 'O'),\n",
              " (('at', 'the'), 'O'),\n",
              " (('the', 'rates'), 'O'),\n",
              " (('rates', 'set'), 'O'),\n",
              " (('set', 'the'), 'O'),\n",
              " (('the', 'Central'), 'O'),\n",
              " (('Central', 'Value'), 'O'),\n",
              " (('Value', 'Added'), 'O'),\n",
              " (('Added', 'Tax'), 'O'),\n",
              " (('forth', 'in'), 'O'),\n",
              " (('in', 'the'), 'O'),\n",
              " (('the', 'First'), 'O'),\n",
              " (('First', 'Schedule'), 'O'),\n",
              " (('Schedule', 'to'), 'O'),\n",
              " (('to', 'the'), 'O'),\n",
              " (('the', 'CENVAT'), 'O'),\n",
              " (('CENVAT', 'on'), 'O'),\n",
              " (('on', 'all'), 'O'),\n",
              " (('all', 'excisable'), 'O'),\n",
              " (('excisable', 'goods'), 'O'),\n",
              " (('Central', 'Excise'), 'O'),\n",
              " (('Excise', 'Tariff'), 'O'),\n",
              " (('Tariff', 'Act'), 'O'),\n",
              " (('Act', '1985'), 'O'),\n",
              " (('1985', 'which'), 'O'),\n",
              " (('which', 'are'), 'O'),\n",
              " (('are', 'produced'), 'O'),\n",
              " (('produced', 'or'), 'O'),\n",
              " (('manufactured', 'in'), 'O'),\n",
              " (('in', 'India'), 'O'),\n",
              " (('India', 'as'), 'O'),\n",
              " (('as', 'and'), 'O'),\n",
              " (('and', 'at'), 'O'),\n",
              " (('the', 'rates'), 'O'),\n",
              " (('rates', 'set'), 'O'),\n",
              " (('set', 'forth'), 'O'),\n",
              " (('forth', 'in'), 'O'),\n",
              " (('in', 'the'), 'O'),\n",
              " (('the', 'First'), 'O'),\n",
              " (('Schedule', 'to'), 'O'),\n",
              " (('to', 'the'), 'O'),\n",
              " (('the', 'Central'), 'O'),\n",
              " (('Central', 'Excise'), 'O'),\n",
              " (('Tariff', 'Act'), 'O'),\n",
              " (('Act', '1985'), 'O'),\n",
              " (('1985', '5'), 'O'),\n",
              " (('5', 'of'), 'O'),\n",
              " (('of', '1986'), 'O'),\n",
              " (('b', 'b'), 'O'),\n",
              " (('Section', '4'), 'O'),\n",
              " (('Section', '4'), 'O'),\n",
              " (('4', 'as'), 'O'),\n",
              " (('as', 'originally'), 'O'),\n",
              " (('originally', 'Section'), 'O'),\n",
              " (('Section', '4'), 'O'),\n",
              " (('4', 'as'), 'O'),\n",
              " (('as', 'amended'), 'O'),\n",
              " (('amended', 'Section'), 'O'),\n",
              " (('Section', '4'), 'O'),\n",
              " (('4', 'as'), 'O'),\n",
              " (('enacted', 'in'), 'O'),\n",
              " (('in', 'the'), 'O'),\n",
              " (('the', 'by'), 'O'),\n",
              " (('by', 'Amendment'), 'O'),\n",
              " (('Amendment', 'Act'), 'O'),\n",
              " (('Act', 'amended'), 'O'),\n",
              " (('amended', 'by'), 'O'),\n",
              " (('by', 'Finance'), 'O'),\n",
              " (('Central', 'Excise'), 'O'),\n",
              " (('Excise', 'and'), 'O'),\n",
              " (('and', 'No'), 'O'),\n",
              " (('No', '22'), 'O'),\n",
              " (('22', 'of'), 'O'),\n",
              " (('of', '1973'), 'O'),\n",
              " (('1973', 'Act'), 'O'),\n",
              " (('Act', '2000'), 'O'),\n",
              " (('2000', 'with'), 'O'),\n",
              " (('with', 'effect'), 'O'),\n",
              " (('Salt', 'Act'), 'O'),\n",
              " (('Act', '1944'), 'O'),\n",
              " (('1944', 'from'), 'O'),\n",
              " (('from', '1'), 'O'),\n",
              " (('1', '7'), 'O'),\n",
              " (('7', '2000'), 'O'),\n",
              " (('Determination', 'of'), 'O'),\n",
              " (('of', 'Valuation'), 'O'),\n",
              " (('Valuation', 'of'), 'O'),\n",
              " (('of', 'Valuation'), 'O'),\n",
              " (('Valuation', 'of'), 'O'),\n",
              " (('value', 'for'), 'O'),\n",
              " (('for', 'the'), 'O'),\n",
              " (('the', 'excisable'), 'O'),\n",
              " (('excisable', 'goods'), 'O'),\n",
              " (('goods', 'for'), 'O'),\n",
              " (('for', 'excisable'), 'O'),\n",
              " (('excisable', 'goods'), 'O'),\n",
              " (('goods', 'for'), 'O'),\n",
              " (('purposes', 'of'), 'O'),\n",
              " (('of', 'duty'), 'O'),\n",
              " (('duty', 'purposes'), 'O'),\n",
              " (('purposes', 'of'), 'O'),\n",
              " (('of', 'charging'), 'O'),\n",
              " (('charging', 'purposes'), 'O'),\n",
              " (('purposes', 'of'), 'O'),\n",
              " (('Where', 'under'), 'O'),\n",
              " (('under', 'this'), 'O'),\n",
              " (('this', 'Act'), 'O'),\n",
              " (('Act', 'of'), 'O'),\n",
              " (('of', 'duty'), 'O'),\n",
              " (('duty', 'of'), 'O'),\n",
              " (('of', 'excise'), 'O'),\n",
              " (('excise', '1'), 'O'),\n",
              " (('1', 'charging'), 'O'),\n",
              " (('charging', 'of'), 'O'),\n",
              " (('of', 'duty'), 'O'),\n",
              " (('duty', 'of'), 'O'),\n",
              " (('any', 'article'), 'O'),\n",
              " (('article', 'is'), 'O'),\n",
              " (('is', 'Where'), 'O'),\n",
              " (('Where', 'under'), 'O'),\n",
              " (('under', 'this'), 'O'),\n",
              " (('this', 'Act'), 'O'),\n",
              " (('1', 'Where'), 'O'),\n",
              " (('chargeable', 'with'), 'O'),\n",
              " (('with', 'duty'), 'O'),\n",
              " (('duty', 'the'), 'O'),\n",
              " (('the', 'duty'), 'O'),\n",
              " (('duty', 'of'), 'O'),\n",
              " (('of', 'excise'), 'O'),\n",
              " (('excise', 'is'), 'O'),\n",
              " (('under', 'this'), 'O'),\n",
              " (('this', 'Act'), 'O'),\n",
              " (('Act', 'the'), 'O'),\n",
              " (('at', 'a'), 'O'),\n",
              " (('a', 'rate'), 'O'),\n",
              " (('rate', 'dependent'), 'O'),\n",
              " (('dependent', 'chargeable'), 'O'),\n",
              " (('chargeable', 'on'), 'O'),\n",
              " (('on', 'any'), 'O'),\n",
              " (('on', 'the'), 'O'),\n",
              " (('the', 'value'), 'O'),\n",
              " (('value', 'of'), 'O'),\n",
              " (('of', 'the'), 'O'),\n",
              " (('the', 'excisable'), 'O'),\n",
              " (('excisable', 'goods'), 'O'),\n",
              " (('goods', 'with'), 'O'),\n",
              " (('with', 'duty'), 'O'),\n",
              " (('duty', 'of'), 'O'),\n",
              " (('of', 'excise'), 'O'),\n",
              " (('excise', 'is'), 'O'),\n",
              " (('article', 'such'), 'O'),\n",
              " (('such', 'value'), 'O'),\n",
              " (('value', 'reference'), 'O'),\n",
              " (('reference', 'to'), 'O'),\n",
              " (('to', 'value'), 'O'),\n",
              " (('value', 'chargeable'), 'O'),\n",
              " (('chargeable', 'on'), 'O'),\n",
              " (('on', 'any'), 'O'),\n",
              " (('shall', 'be'), 'O'),\n",
              " (('be', 'deemed'), 'O'),\n",
              " (('deemed', 'to'), 'O'),\n",
              " (('to', 'such'), 'O'),\n",
              " (('such', 'value'), 'O'),\n",
              " (('value', 'shall'), 'O'),\n",
              " (('shall', 'excisable'), 'O'),\n",
              " (('excisable', 'goods'), 'O'),\n",
              " (('goods', 'with'), 'O'),\n",
              " (('be', 'the'), 'O'),\n",
              " (('the', 'wholesale'), 'O'),\n",
              " (('wholesale', 'subject'), 'O'),\n",
              " (('subject', 'to'), 'O'),\n",
              " (('to', 'the'), 'O'),\n",
              " (('the', 'other'), 'O'),\n",
              " (('other', 'reference'), 'O'),\n",
              " (('reference', 'to'), 'O'),\n",
              " (('to', 'their'), 'O'),\n",
              " (('cash', 'price'), 'O'),\n",
              " (('price', 'for'), 'O'),\n",
              " (('for', 'which'), 'O'),\n",
              " (('which', 'provisions'), 'O'),\n",
              " (('provisions', 'of'), 'O'),\n",
              " (('of', 'this'), 'O'),\n",
              " (('this', 'value'), 'O'),\n",
              " (('value', 'then'), 'O'),\n",
              " (('then', 'on'), 'O'),\n",
              " (('on', 'each'), 'O'),\n",
              " (('an', 'article'), 'O'),\n",
              " (('article', 'of'), 'O'),\n",
              " (('of', 'the'), 'O'),\n",
              " (('the', 'like'), 'O'),\n",
              " (('like', 'section'), 'O'),\n",
              " (('section', 'be'), 'O'),\n",
              " (('be', 'deemed'), 'O'),\n",
              " (('deemed', 'to'), 'O'),\n",
              " (('to', 'removal'), 'O'),\n",
              " (('removal', 'of'), 'O'),\n",
              " (('of', 'the'), 'O'),\n",
              " (('the', 'goods'), 'O'),\n",
              " (('kind', 'and'), 'O'),\n",
              " (('and', 'quality'), 'O'),\n",
              " (('quality', 'is'), 'O'),\n",
              " (('is', 'be'), 'O'),\n",
              " (('be', 'such'), 'O'),\n",
              " (('such', 'value'), 'O'),\n",
              " (('value', 'shall'), 'O'),\n",
              " (('sold', 'or'), 'O'),\n",
              " (('or', 'is'), 'O'),\n",
              " (('is', 'capable'), 'O'),\n",
              " (('capable', 'of'), 'O'),\n",
              " (('being', 'sold'), 'O'),\n",
              " (('sold', 'for'), 'O'),\n",
              " ...]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nSNOAac_JskH"
      },
      "source": [
        "#### Data engg stage 2 where we will storing final dictionary of both train test in csv"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uZGnCQB3Ghci"
      },
      "source": [
        "def final_csv(final_dictionary,data=None):\n",
        "  if data==\"train\":\n",
        "    data=pd.DataFrame(columns=[\"File_Name\",\"word\",\"token\"])\n",
        "    for file_key , values in final_dictionary.items():\n",
        "      file_list=[]\n",
        "      word_token=[]\n",
        "      token_value=[]\n",
        "      for value in values:\n",
        "        if value[1]==\"Person\":\n",
        "          file_list.append(file_key)\n",
        "          word_token.append(value[0])\n",
        "          token_value.append(value[1])\n",
        "        else:\n",
        "          bigram_data=[' '.join(w) for w in value]\n",
        "          file_list.append(file_key)\n",
        "          word_token.append(bigram_data[0])\n",
        "          token_value.append(bigram_data[1])\n",
        "      minor_frame=pd.DataFrame(columns=[\"File_Name\",\"word\",\"token\"])\n",
        "      minor_frame[\"File_Name\"]=file_list\n",
        "      minor_frame[\"word\"]=word_token\n",
        "      minor_frame[\"token\"]=token_value\n",
        "      data=data.append(minor_frame)\n",
        "  else:\n",
        "    data=pd.DataFrame(columns=[\"File_Name\",\"word\"])\n",
        "    for file_key , values in final_dictionary.items():\n",
        "      file_list=[]\n",
        "      word_token=[]\n",
        "      for value in values:\n",
        "        bigram_data=[''.join(w) for w in value]\n",
        "        file_list.append(file_key)\n",
        "        word_token.append(bigram_data[0])\n",
        "      minor_frame=pd.DataFrame(columns=[\"File_Name\",\"word\"])\n",
        "      minor_frame[\"File_Name\"]=file_list\n",
        "      minor_frame[\"word\"]=word_token\n",
        "      data=data.append(minor_frame)\n",
        "  return data\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8H3EUhMhv0eM"
      },
      "source": [
        "#### As we can see the lenght of sentence is really large more than 512 so we will scrape of sentences 20 units before \"person\" tag and after \"person\" tag to preserve the sentence (actually this error I got more block down but showing up here because I will be making function here to clean the data more)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nrySzHYsN3kB"
      },
      "source": [
        "final_train_set=final_csv(final_dictionary_train,\"train\")\n",
        "final_test_set=final_csv(final_dictionary_test,\"test\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZvyHVV04boSI",
        "outputId": "7400a87f-6c03-41c3-ce5a-0f92ef331968",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import gc\n",
        "gc.collect()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T7zGdDjG1tNE"
      },
      "source": [
        "final_train_set.to_csv(\"train.csv\")\n",
        "final_test_set.to_csv(\"test.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ERR6ErY6kD3"
      },
      "source": [
        "## find files where person got tagged get their index values and scrape the rows from orginal set of train data "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZlelNP2q-fkH"
      },
      "source": [
        "final_train_set=final_train_set.reset_index()\n",
        "final_test_set=final_test_set.reset_index()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9qWLPzku_vz6"
      },
      "source": [
        "final_test_set=final_test_set.drop(\"index\",axis=1)\n",
        "final_train_set=final_train_set.drop(\"index\",axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ergINGG8Eo3",
        "outputId": "584c998c-76c9-4d35-fe45-1aa1d52a30a4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(final_train_set.index)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3315428"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1VRkVCeS53x8"
      },
      "source": [
        "files_person=final_train_set[final_train_set[\"token\"]==\"Person\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sf0q3mAP9eN5",
        "outputId": "2776eef0-943a-49d1-baea-56e84ca11ca6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "files_person.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>File_Name</th>\n",
              "      <th>word</th>\n",
              "      <th>token</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>32222</th>\n",
              "      <td>/content/NLP Assignment/train_data/10000_2008_...</td>\n",
              "      <td>Arun Mishra</td>\n",
              "      <td>Person</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33590</th>\n",
              "      <td>/content/NLP Assignment/train_data/10022_2008_...</td>\n",
              "      <td>Abhay Manohar Sapre</td>\n",
              "      <td>Person</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36226</th>\n",
              "      <td>/content/NLP Assignment/train_data/10034_2016_...</td>\n",
              "      <td>A.M. Khanwilkar</td>\n",
              "      <td>Person</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37720</th>\n",
              "      <td>/content/NLP Assignment/train_data/10034_2016_...</td>\n",
              "      <td>A.M. Khanwilkar</td>\n",
              "      <td>Person</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45730</th>\n",
              "      <td>/content/NLP Assignment/train_data/10091_2003_...</td>\n",
              "      <td>R.K. Agrawal</td>\n",
              "      <td>Person</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               File_Name  ...   token\n",
              "32222  /content/NLP Assignment/train_data/10000_2008_...  ...  Person\n",
              "33590  /content/NLP Assignment/train_data/10022_2008_...  ...  Person\n",
              "36226  /content/NLP Assignment/train_data/10034_2016_...  ...  Person\n",
              "37720  /content/NLP Assignment/train_data/10034_2016_...  ...  Person\n",
              "45730  /content/NLP Assignment/train_data/10091_2003_...  ...  Person\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TdfTjyZ46YES"
      },
      "source": [
        "person_index=files_person.index"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5mJcZEu169d5"
      },
      "source": [
        "new_train=pd.DataFrame(columns=[\"File_Name\",\"word\",\"token\"])\n",
        "for i in person_index:\n",
        "  new_data=final_train_set.iloc[i - 25 : i + 20]\n",
        "  new_train=new_train.append(new_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x_8g-LGezIm-",
        "outputId": "2da29959-aefd-46ee-ce5e-5b4d2c09a2b2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "new_train.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(15333, 3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MU1mM-pGliLg"
      },
      "source": [
        "# So as far we have prepared the data for training now we are moving with data preprocessing and training of our data "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_xnhFesUmn4R"
      },
      "source": [
        "# step1 .grouping of documents and their words with it's tag"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A-f7Dg5HWt54"
      },
      "source": [
        "class SentenceGetter(object):\n",
        "    \n",
        "    def __init__(self, data):\n",
        "        self.n_sent = 1\n",
        "        self.data = data\n",
        "        self.empty = False\n",
        "        agg_func = lambda s: [(w,t) for w, t in zip(s[\"word\"].values.tolist(),s[\"token\"].values.tolist())]\n",
        "        self.grouped = self.data.groupby(\"File_Name\").apply(agg_func)\n",
        "        self.sentences = [s for s in self.grouped]\n",
        "    \n",
        "    def get_next(self):\n",
        "        try:\n",
        "            s = self.grouped[\"Sentence: {}\".format(self.n_sent)]\n",
        "            self.n_sent += 1\n",
        "            return s\n",
        "        except:\n",
        "            return None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O-6dCrvZm0lp"
      },
      "source": [
        "# Get full document data structure\n",
        "getter = SentenceGetter(new_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ToxNTLEEL5a",
        "outputId": "ba5c4e71-c985-44f5-c908-fc12531fa7ea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        }
      },
      "source": [
        "sentences = [\" \".join([s[0] for s in sent]) for sent in getter.sentences]\n",
        "sentences[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'and sentence sentence as as recorded recorded against against the the appellants appellants are are acquitted acquitted of of the the charge charge of of Section Section 498A 498A IPC IPC leveled leveled against The appellants appellants are are on on bail Their bail bail bonds bonds stand stand discharged Arun Mishra Uday Umesh Umesh Lalit New Delhi May 9 9 2018 Non Reportable IN THE THE SUPREME SUPREME COURT COURT OF OF INDIA CRIMINAL APPELLATE APPELLATE JURISDICTION CRIMINAL APPEAL APPEAL NO NO 1340 1340 OF OF 2013 Manoharan Anr'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R0hVl7qFESfA",
        "outputId": "9b99345d-56a9-4d8e-e42e-ff2ea41f6501",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "labels = [[s[1] for s in sent] for sent in getter.sentences]\n",
        "print(labels[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'Person', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jpBK-IP6EZbx"
      },
      "source": [
        "tags_vals = list(set(new_train[\"token\"].values))\n",
        "tag2idx = {t: i for i, t in enumerate(tags_vals)}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2kEK3yFPgac1"
      },
      "source": [
        "tag2name={tag2idx[key] : key for key in tag2idx.keys()}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qz1TQqgoq2C4"
      },
      "source": [
        "Make training data\n",
        "Make raw data into trainable data for BERT, including:\n",
        "Set gpu environment\n",
        "Load tokenizer and tokenize\n",
        "Set 3 embedding, token embedding, mask word embedding, segmentation embedding\n",
        "Split data set into train and validate, then send them to dataloader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rq27Heqtqury"
      },
      "source": [
        "#setup environment\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "n_gpu = torch.cuda.device_count()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g1sezb8zq_oF",
        "outputId": "796907b3-c551-43c2-8ac9-0b80e5db488b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "n_gpu"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9H82SKZzrWUz"
      },
      "source": [
        "### Load tokenizer\n",
        "### download the tokenizer file into local folder first "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WQqs5SpbrI7c",
        "outputId": "8c696a21-9f2b-458d-fb9e-11067804bd56",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "!wget https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-vocab.txt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-10-07 15:49:47--  https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-vocab.txt\n",
            "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.216.131.101\n",
            "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.216.131.101|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 213450 (208K) [text/plain]\n",
            "Saving to: ‘bert-base-cased-vocab.txt’\n",
            "\n",
            "bert-base-cased-voc 100%[===================>] 208.45K  1016KB/s    in 0.2s    \n",
            "\n",
            "2020-10-07 15:49:48 (1016 KB/s) - ‘bert-base-cased-vocab.txt’ saved [213450/213450]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5gh8D98ws_3t"
      },
      "source": [
        "vocabulary = \"/content/bert-base-cased-vocab.txt\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fxfAnCjltTbp"
      },
      "source": [
        "max_len  = 176"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zPDUrX4YBloW"
      },
      "source": [
        "tokenizer=BertTokenizer(vocab_file=vocabulary,do_lower_case=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x0wUgOT8BxeZ"
      },
      "source": [
        "Tokenizer text\n",
        "\n",
        "In hunggieface for bert, when come across OOV, will word piece the word\n",
        "We need to adjust the labels base on the tokenize result, “##abc” need to set label \"X\"\n",
        "Need to set \"[CLS]\" at front and \"[SEP]\" at the end, as what the paper do, BERT indexer should add [CLS] and [SEP] tokens"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sFjcnfFZCQRO",
        "outputId": "65eccceb-1405-4a7b-9995-810fdafffda6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 231508/231508 [00:00<00:00, 767662.03B/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2zhPRD_nD69O",
        "outputId": "940149e5-c9a0-4b2c-d255-5f13b9036d84",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "tokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]\n",
        "print(len(tokenized_texts[0]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "115\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "blHco_6ZD-el"
      },
      "source": [
        "input_ids = pad_sequences([tokenizer.convert_tokens_to_ids(txt) for txt in tokenized_texts],\n",
        "                          maxlen=176, dtype=\"long\", truncating=\"post\", padding=\"post\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VmRyc9CREo-4"
      },
      "source": [
        "tags = pad_sequences([[tag2idx.get(l) for l in lab] for lab in labels],\n",
        "                     maxlen=176, value=tag2idx[\"O\"], padding=\"post\",\n",
        "                     dtype=\"long\", truncating=\"post\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q6eHns3EFeir"
      },
      "source": [
        "attention_masks = [[float(i>0) for i in ii] for ii in input_ids]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-s9esjRSGCrg"
      },
      "source": [
        "# Since only one sentence, all the segment set to 0\n",
        "segment_ids = [[0] * len(input_id) for input_id in input_ids]\n",
        "segment_ids[0];"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BgIE7dbRGf8s"
      },
      "source": [
        "Split data into train and validate\n",
        "70% for training, 30% for validation\n",
        "\n",
        "Split all data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U9Xbkzq-Fnym"
      },
      "source": [
        "tr_inputs, val_inputs, tr_tags, val_tags,tr_masks, val_masks,tr_segs, val_segs = train_test_split(input_ids, tags,attention_masks,segment_ids, \n",
        "                                                            random_state=4, test_size=0.3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2DjeYtt0Gi-T"
      },
      "source": [
        "set data to tensor"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gzQEyCgeFs4e",
        "outputId": "ab465787-c22d-448f-ac00-8c749ef6fa0f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(tr_inputs),len(val_inputs),len(tr_segs),len(val_segs)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(231, 99, 231, 99)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VfaFBkdVGaSX",
        "outputId": "77826a55-9178-4285-a888-1f42a29b8786",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 309
        }
      },
      "source": [
        "tr_inputs = torch.tensor(tr_inputs)\n",
        "val_inputs = torch.tensor(val_inputs)\n",
        "tr_tags = torch.tensor(tr_tags)\n",
        "val_tags = torch.tensor(val_tags)\n",
        "tr_masks = torch.tensor(tr_masks)\n",
        "val_masks = torch.tensor(val_masks)\n",
        "tr_segs = torch.tensor(tr_segs)\n",
        "val_segs = torch.tensor(val_segs)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  \n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:4: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  after removing the cwd from sys.path.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  \"\"\"\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  \n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  import sys\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3giw1pxeG8Z6"
      },
      "source": [
        "Put data into data loader\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BfGOnwICHAHl"
      },
      "source": [
        "batch_num = 32"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fM17PN3iHCzf"
      },
      "source": [
        "# Only set token embedding, attention embedding, no segment embedding for seqeunce tagging task segment-id is not needed\n",
        "train_data = TensorDataset(tr_inputs, tr_masks, tr_tags)\n",
        "train_sampler = RandomSampler(train_data)\n",
        "# Drop last can make batch training better for the last one\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_num,drop_last=True)\n",
        "\n",
        "valid_data = TensorDataset(val_inputs, val_masks, val_tags)\n",
        "valid_sampler = SequentialSampler(valid_data)\n",
        "valid_dataloader = DataLoader(valid_data, sampler=valid_sampler, batch_size=batch_num)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uIPCWnd3HXxQ"
      },
      "source": [
        "Train model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZAyblZbVHSb7",
        "outputId": "46dc28db-5fc6-4b76-f976-967eea936b53",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 220,
          "referenced_widgets": [
            "a594ddd4bbd04fe5bfaebf075601de43",
            "5c043bc7cc8943528e38584ce4952ab7",
            "54da51376714422aa0e4272b50524eed",
            "8a5845bbafee4c8c8bb06d69d2d060ab",
            "1bd60691c2ab4aedaaabe203440eecdb",
            "9a546f8bca4b497ca25ffb9b15b83fa2",
            "08567e53754f47eebc7e3a2ead1c2207",
            "0bbaeb54bf5b42a8866803576cd3c974",
            "c5f9258b9e22459fa1098fdf59486b78",
            "717ad637862a4e43ad7a9206a56be19d",
            "5a0cb095c4ff407a84f2d82ee48fac91",
            "583ed522c3454a42bdd6bd9e0e902e96",
            "197f61c79711455bb1f2938a2ed88140",
            "7fdc998ecc3846eeacc649f4b508148e",
            "1c26eae779494c92a4db415f883c6903",
            "12f4a98ede3648d99b1a0db59b94e7b8"
          ]
        }
      },
      "source": [
        "model = BertForTokenClassification.from_pretrained(\"bert-base-uncased\", num_labels=len(tag2idx))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a594ddd4bbd04fe5bfaebf075601de43",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=433.0, style=ProgressStyle(description_…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c5f9258b9e22459fa1098fdf59486b78",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=440473133.0, style=ProgressStyle(descri…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
            "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sQ_pVGUnHk0G"
      },
      "source": [
        "# Set model to GPU,if you are using GPU machine\n",
        "model.cuda();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "725238GzHwRY"
      },
      "source": [
        "# Set epoch and grad max num\n",
        "epochs = 50\n",
        "max_grad_norm = 1.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nAxNjaQgICbp"
      },
      "source": [
        "import math\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NqLpwm5fH7OE"
      },
      "source": [
        "# Cacluate train optimiazaion num\n",
        "num_train_optimization_steps = int( math.ceil(len(tr_inputs) / batch_num) / 1) * epochs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TOFaLVVQH-TX"
      },
      "source": [
        "# True: fine tuning all the layers \n",
        "# False: only fine tuning the classifier layers\n",
        "FULL_FINETUNING = True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j_IgVDgEILjJ"
      },
      "source": [
        "if FULL_FINETUNING:\n",
        "    # Fine tune model all layer parameters\n",
        "    param_optimizer = list(model.named_parameters())\n",
        "    no_decay = ['bias', 'gamma', 'beta']\n",
        "    optimizer_grouped_parameters = [\n",
        "        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
        "         'weight_decay_rate': 0.01},\n",
        "        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
        "         'weight_decay_rate': 0.0}\n",
        "    ]\n",
        "else:\n",
        "    # Only fine tune classifier parameters\n",
        "    param_optimizer = list(model.classifier.named_parameters()) \n",
        "    optimizer_grouped_parameters = [{\"params\": [p for n, p in param_optimizer]}]\n",
        "optimizer = AdamW(optimizer_grouped_parameters, lr=3e-5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W7-Ee501IZnF"
      },
      "source": [
        "Fine-tuning model\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1p5CLlCAIWK7",
        "outputId": "e0ca512c-d0cd-481a-fa7a-149473d051f8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model.train()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertForTokenClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ej8FUDWCIeS4"
      },
      "source": [
        "from seqeval.metrics import f1_score\n",
        "\n",
        "def flat_accuracy(preds, labels):\n",
        "    pred_flat = np.argmax(preds, axis=2).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3G-vqKjNI2Wd",
        "outputId": "e15d30c8-baca-487e-b503-3d7d5dc47d9b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 952
        }
      },
      "source": [
        "print(\"***** Running training *****\")\n",
        "print(\"  Num examples = %d\"%(len(tr_inputs)))\n",
        "print(\"  Batch size = %d\"%(batch_num))\n",
        "print(\"  Num steps = %d\"%(num_train_optimization_steps))\n",
        "for _ in trange(epochs,desc=\"Epoch\"):\n",
        "    tr_loss = 0\n",
        "    nb_tr_examples, nb_tr_steps = 0, 0\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "        # add batch to gpu\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        b_input_ids, b_input_mask, b_labels = batch\n",
        "        \n",
        "        # forward pass\n",
        "        outputs = model(b_input_ids, token_type_ids=None,\n",
        "        attention_mask=b_input_mask, labels=b_labels)\n",
        "        loss, scores = outputs[:2]\n",
        "        if n_gpu>1:\n",
        "            # When multi gpu, average it\n",
        "            loss = loss.mean()\n",
        "        \n",
        "        # backward pass\n",
        "        loss.backward()\n",
        "        \n",
        "        # track train loss\n",
        "        tr_loss += loss.item()\n",
        "        nb_tr_examples += b_input_ids.size(0)\n",
        "        nb_tr_steps += 1\n",
        "        \n",
        "        # gradient clipping\n",
        "        torch.nn.utils.clip_grad_norm_(parameters=model.parameters(), max_norm=max_grad_norm)\n",
        "        \n",
        "        # update parameters\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "    # print train loss per epoch\n",
        "    print(\"Train loss: {}\".format(tr_loss/nb_tr_steps))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:   0%|          | 0/50 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "***** Running training *****\n",
            "  Num examples = 231\n",
            "  Batch size = 32\n",
            "  Num steps = 400\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:   2%|▏         | 1/50 [00:06<05:12,  6.37s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss: 0.22720687304224288\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:   4%|▍         | 2/50 [00:12<05:00,  6.27s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss: 0.0557943619787693\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:   6%|▌         | 3/50 [00:18<04:51,  6.21s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss: 0.05558104280914579\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:   8%|▊         | 4/50 [00:24<04:44,  6.18s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss: 0.0545095902468477\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  10%|█         | 5/50 [00:30<04:38,  6.20s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss: 0.051807391324213574\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  12%|█▏        | 6/50 [00:37<04:33,  6.22s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss: 0.04830970082964216\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  14%|█▍        | 7/50 [00:43<04:28,  6.24s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss: 0.045134635376078744\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  16%|█▌        | 8/50 [00:49<04:23,  6.28s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss: 0.041599590863500326\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  18%|█▊        | 9/50 [00:56<04:20,  6.35s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss: 0.03822438418865204\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  20%|██        | 10/50 [01:02<04:17,  6.43s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss: 0.03559547157159874\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  22%|██▏       | 11/50 [01:09<04:13,  6.49s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss: 0.03363165844764028\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  24%|██▍       | 12/50 [01:16<04:09,  6.57s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss: 0.03286077667559896\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  26%|██▌       | 13/50 [01:23<04:06,  6.67s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss: 0.03102945350110531\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  28%|██▊       | 14/50 [01:30<04:04,  6.79s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss: 0.0291364174336195\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  30%|███       | 15/50 [01:37<04:01,  6.90s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss: 0.027540374813335284\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  32%|███▏      | 16/50 [01:44<03:57,  6.99s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss: 0.023806771263480186\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  34%|███▍      | 17/50 [01:51<03:51,  7.00s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss: 0.02168367989361286\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  36%|███▌      | 18/50 [01:58<03:43,  6.98s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss: 0.02099959472460406\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  38%|███▊      | 19/50 [02:05<03:35,  6.94s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss: 0.018550609770630087\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  40%|████      | 20/50 [02:12<03:26,  6.89s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss: 0.01610066555440426\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  42%|████▏     | 21/50 [02:18<03:18,  6.85s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss: 0.014447838201054506\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  44%|████▍     | 22/50 [02:25<03:11,  6.82s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss: 0.01338035480252334\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  46%|████▌     | 23/50 [02:32<03:03,  6.81s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss: 0.012581351213157177\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  48%|████▊     | 24/50 [02:39<02:57,  6.81s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss: 0.011145827959158592\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  50%|█████     | 25/50 [02:46<02:50,  6.84s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss: 0.010138042138091155\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  52%|█████▏    | 26/50 [02:53<02:44,  6.87s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss: 0.011540904840720552\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  54%|█████▍    | 27/50 [03:00<02:38,  6.89s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss: 0.01007980021781155\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  56%|█████▌    | 28/50 [03:07<02:32,  6.91s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss: 0.008886672024215971\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  58%|█████▊    | 29/50 [03:13<02:25,  6.92s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss: 0.006917680879788739\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  60%|██████    | 30/50 [03:20<02:18,  6.92s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss: 0.005343044675620539\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  62%|██████▏   | 31/50 [03:27<02:11,  6.91s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss: 0.005475451870422278\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  64%|██████▍   | 32/50 [03:34<02:04,  6.90s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss: 0.005420549706156764\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  66%|██████▌   | 33/50 [03:41<01:57,  6.89s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss: 0.006430773064494133\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  68%|██████▊   | 34/50 [03:48<01:49,  6.87s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss: 0.005079062688829643\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  70%|███████   | 35/50 [03:55<01:43,  6.87s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss: 0.0041837748140096664\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  72%|███████▏  | 36/50 [04:02<01:36,  6.87s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss: 0.005035504698753357\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  74%|███████▍  | 37/50 [04:08<01:29,  6.87s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss: 0.005277774828885283\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  76%|███████▌  | 38/50 [04:15<01:22,  6.88s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss: 0.004021791940821069\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  78%|███████▊  | 39/50 [04:22<01:15,  6.88s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss: 0.0040234554658776945\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  80%|████████  | 40/50 [04:29<01:08,  6.90s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss: 0.0037088838372645633\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  82%|████████▏ | 41/50 [04:36<01:02,  6.90s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss: 0.0024501843228270964\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  84%|████████▍ | 42/50 [04:43<00:55,  6.91s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss: 0.0027655030717141926\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  86%|████████▌ | 43/50 [04:50<00:48,  6.91s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss: 0.003960005440083998\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  88%|████████▊ | 44/50 [04:57<00:41,  6.91s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss: 0.002777108274001096\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  90%|█████████ | 45/50 [05:04<00:34,  6.91s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss: 0.003069659395675574\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  92%|█████████▏| 46/50 [05:11<00:27,  6.91s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss: 0.00175447718772505\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  94%|█████████▍| 47/50 [05:18<00:20,  6.91s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss: 0.001456912118425992\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  96%|█████████▌| 48/50 [05:24<00:13,  6.90s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss: 0.0016342324504096592\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  98%|█████████▊| 49/50 [05:31<00:06,  6.90s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss: 0.0018988024343603424\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 100%|██████████| 50/50 [05:38<00:00,  6.77s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss: 0.0018893460801336914\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U4eIlTqQ8GfU"
      },
      "source": [
        "tag2name={tag2idx[key] : key for key in tag2idx.keys()}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FMpVj88ifQ1_"
      },
      "source": [
        "Save model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BmMxmcmYI64m",
        "outputId": "c1aff739-cd6a-4933-c6e9-2add82d1eb21",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "bert_out_address = 'models/bert_out_model/judge20'\n",
        "# Make dir if not exits\n",
        "if not os.path.exists(bert_out_address):\n",
        "        os.makedirs(bert_out_address)\n",
        "# Save a trained model, configuration and tokenizer\n",
        "model_to_save = model.module if hasattr(model, 'module') else model  # Only save the model it-self\n",
        "# If we save using the predefined names, we can load using `from_pretrained`\n",
        "output_model_file = os.path.join(bert_out_address, \"pytorch_model.bin\")\n",
        "output_config_file = os.path.join(bert_out_address, \"config.json\")\n",
        "# Save model into file\n",
        "torch.save(model_to_save.state_dict(), output_model_file)\n",
        "model_to_save.config.to_json_file(output_config_file)\n",
        "tokenizer.save_vocabulary(bert_out_address)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'models/bert_out_model/judge20/vocab.txt'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c0-tiCDBf5gX"
      },
      "source": [
        "# Load model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ekY8Vf7vfv9_"
      },
      "source": [
        "model = BertForTokenClassification.from_pretrained(bert_out_address,num_labels=len(tag2idx))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Lw0iLesf0KR"
      },
      "source": [
        "# Set model to GPU\n",
        "model.cuda();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "axuD6CWmf--N"
      },
      "source": [
        "# Eval model\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "16SfoRLff710",
        "outputId": "f5bc8195-df7c-4730-f266-c899ca387c5a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model.eval()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertForTokenClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RH7woZklgCnG",
        "outputId": "b65d0f99-4a36-428b-e0f0-13ffa0280508",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        }
      },
      "source": [
        "eval_loss, eval_accuracy = 0, 0\n",
        "nb_eval_steps, nb_eval_examples = 0, 0\n",
        "y_true = []\n",
        "y_pred = []\n",
        "\n",
        "print(\"***** Running evaluation *****\")\n",
        "print(\"  Num examples ={}\".format(len(val_inputs)))\n",
        "print(\"  Batch size = {}\".format(batch_num))\n",
        "for step, batch in enumerate(valid_dataloader):\n",
        "    batch = tuple(t.to(device) for t in batch)\n",
        "    input_ids, input_mask, label_ids = batch\n",
        "    \n",
        "#     if step > 2:\n",
        "#         break\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        outputs = model(input_ids, token_type_ids=None,\n",
        "        attention_mask=input_mask,)\n",
        "        # For eval mode, the first result of outputs is logits\n",
        "        logits = outputs[0] \n",
        "    \n",
        "    # Get NER predict result\n",
        "    logits = torch.argmax(F.log_softmax(logits,dim=2),dim=2)\n",
        "    logits = logits.detach().cpu().numpy()\n",
        "    \n",
        "    \n",
        "    # Get NER true result\n",
        "    label_ids = label_ids.to('cpu').numpy()\n",
        "    \n",
        "    \n",
        "    # Only predict the real word, mark=0, will not calculate\n",
        "    input_mask = input_mask.to('cpu').numpy()\n",
        "    \n",
        "    # Compare the valuable predict result\n",
        "    for i,mask in enumerate(input_mask):\n",
        "        # Real one\n",
        "        temp_1 = []\n",
        "        # Predict one\n",
        "        temp_2 = []\n",
        "        \n",
        "        for j, m in enumerate(mask):\n",
        "            # Mark=0, meaning its a pad word, dont compare\n",
        "            if m:\n",
        "                if tag2name[label_ids[i][j]] != \"X\" and tag2name[label_ids[i][j]] != \"[CLS]\" and tag2name[label_ids[i][j]] != \"[SEP]\" : # Exclude the X label\n",
        "                    temp_1.append(tag2name[label_ids[i][j]])\n",
        "                    temp_2.append(tag2name[logits[i][j]])\n",
        "            else:\n",
        "                break\n",
        "        \n",
        "            \n",
        "        y_true.append(temp_1)\n",
        "        y_pred.append(temp_2)\n",
        "\n",
        "        \n",
        "\n",
        "print(\"f1 socre: %f\"%(f1_score(y_true, y_pred)))\n",
        "print(\"Accuracy score: %f\"%(accuracy_score(y_true, y_pred)))\n",
        "\n",
        "# Get acc , recall, F1 result report\n",
        "report = classification_report(y_true, y_pred,digits=4)\n",
        "\n",
        "# Save the report into file\n",
        "output_eval_file = os.path.join(bert_out_address, \"eval_results.txt\")\n",
        "with open(output_eval_file, \"w\") as writer:\n",
        "    print(\"***** Eval results *****\")\n",
        "    print(\"\\n%s\"%(report))\n",
        "    print(\"f1 socre: %f\"%(f1_score(y_true, y_pred)))\n",
        "    print(\"Accuracy score: %f\"%(accuracy_score(y_true, y_pred)))\n",
        "    \n",
        "    writer.write(\"f1 socre:\\n\")\n",
        "    writer.write(str(f1_score(y_true, y_pred)))\n",
        "    writer.write(\"\\n\\nAccuracy score:\\n\")\n",
        "    writer.write(str(accuracy_score(y_true, y_pred)))\n",
        "    writer.write(\"\\n\\n\")  \n",
        "    writer.write(report)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "***** Running evaluation *****\n",
            "  Num examples =99\n",
            "  Batch size = 32\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/seqeval/metrics/sequence_labeling.py:42: UserWarning: Person seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "f1 socre: 0.483721\n",
            "Accuracy score: 0.989924\n",
            "***** Eval results *****\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       erson     0.5049    0.4643    0.4837       112\n",
            "\n",
            "   micro avg     0.5049    0.4643    0.4837       112\n",
            "   macro avg     0.5049    0.4643    0.4837       112\n",
            "weighted avg     0.5049    0.4643    0.4837       112\n",
            "\n",
            "f1 socre: 0.483721\n",
            "Accuracy score: 0.989924\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PpVJAwIRg39E"
      },
      "source": [
        "# Submission task"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RKRJ9pa_gHou",
        "outputId": "c8514473-ee2a-4a0b-b623-868e14a2809a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "final_test_set.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>File_Name</th>\n",
              "      <th>word</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>/content/NLP Assignment/test_data/MMD20052009W...</td>\n",
              "      <td>ORISSA</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>/content/NLP Assignment/test_data/MMD20052009W...</td>\n",
              "      <td>HIGH</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>/content/NLP Assignment/test_data/MMD20052009W...</td>\n",
              "      <td>COURT</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>/content/NLP Assignment/test_data/MMD20052009W...</td>\n",
              "      <td>W</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>/content/NLP Assignment/test_data/MMD20052009W...</td>\n",
              "      <td>C</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                           File_Name    word\n",
              "0  /content/NLP Assignment/test_data/MMD20052009W...  ORISSA\n",
              "1  /content/NLP Assignment/test_data/MMD20052009W...    HIGH\n",
              "2  /content/NLP Assignment/test_data/MMD20052009W...   COURT\n",
              "3  /content/NLP Assignment/test_data/MMD20052009W...       W\n",
              "4  /content/NLP Assignment/test_data/MMD20052009W...       C"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jcAtsrJPhAAB"
      },
      "source": [
        "Make query into embeddings\n",
        "token id embedding, need to tokenize first\n",
        "mask word embedding\n",
        "segmentation embedding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FnUuqqoeg7Nt"
      },
      "source": [
        "last_index=[]\n",
        "new_test=pd.DataFrame(columns=[\"File_Name\",\"word\"])\n",
        "for i in final_test_set[\"File_Name\"].unique():\n",
        "  new_data=final_test_set[final_test_set[\"File_Name\"]==i]\n",
        "  last_index.append(new_data.index[-1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SZJWV6ZikKg-"
      },
      "source": [
        "for index,file_name in zip(last_index,final_test_set[\"File_Name\"].unique()):\n",
        "  new_data=final_test_set[final_test_set[\"File_Name\"]==file_name]\n",
        "  new_data=new_data.iloc[index - 600 : index]\n",
        "  new_test=new_test.append(new_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dROkPpDOx8WH",
        "outputId": "ea30dd07-ddf1-476b-c820-157f35e9124e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        }
      },
      "source": [
        "new_test"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>File_Name</th>\n",
              "      <th>word</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>5676</th>\n",
              "      <td>/content/NLP Assignment/test_data/MMD20052009W...</td>\n",
              "      <td>than</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5677</th>\n",
              "      <td>/content/NLP Assignment/test_data/MMD20052009W...</td>\n",
              "      <td>what</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5678</th>\n",
              "      <td>/content/NLP Assignment/test_data/MMD20052009W...</td>\n",
              "      <td>has</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5679</th>\n",
              "      <td>/content/NLP Assignment/test_data/MMD20052009W...</td>\n",
              "      <td>awarded</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5680</th>\n",
              "      <td>/content/NLP Assignment/test_data/MMD20052009W...</td>\n",
              "      <td>to</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2673</th>\n",
              "      <td>/content/NLP Assignment/test_data/CrlA2212011.txt</td>\n",
              "      <td>LCR</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2674</th>\n",
              "      <td>/content/NLP Assignment/test_data/CrlA2212011.txt</td>\n",
              "      <td>JUDGE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2675</th>\n",
              "      <td>/content/NLP Assignment/test_data/CrlA2212011.txt</td>\n",
              "      <td>Appeal</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2676</th>\n",
              "      <td>/content/NLP Assignment/test_data/CrlA2212011.txt</td>\n",
              "      <td>N</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2677</th>\n",
              "      <td>/content/NLP Assignment/test_data/CrlA2212011.txt</td>\n",
              "      <td>221</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>127351 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                              File_Name     word\n",
              "5676  /content/NLP Assignment/test_data/MMD20052009W...     than\n",
              "5677  /content/NLP Assignment/test_data/MMD20052009W...     what\n",
              "5678  /content/NLP Assignment/test_data/MMD20052009W...      has\n",
              "5679  /content/NLP Assignment/test_data/MMD20052009W...  awarded\n",
              "5680  /content/NLP Assignment/test_data/MMD20052009W...       to\n",
              "...                                                 ...      ...\n",
              "2673  /content/NLP Assignment/test_data/CrlA2212011.txt      LCR\n",
              "2674  /content/NLP Assignment/test_data/CrlA2212011.txt    JUDGE\n",
              "2675  /content/NLP Assignment/test_data/CrlA2212011.txt   Appeal\n",
              "2676  /content/NLP Assignment/test_data/CrlA2212011.txt        N\n",
              "2677  /content/NLP Assignment/test_data/CrlA2212011.txt      221\n",
              "\n",
              "[127351 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 128
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ATbwDaKO6SAa"
      },
      "source": [
        "from scipy.special import softmax\n",
        "save_model_address = 'models/bert_out_model/judge20'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iB0l6odD9IvK"
      },
      "source": [
        "save_model = BertForTokenClassification.from_pretrained(save_model_address,num_labels=len(tag2idx))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qBeFWqI42SCO"
      },
      "source": [
        "result_files={}\n",
        "for file_name in final_test_set[\"File_Name\"].unique():\n",
        "  test_df=final_test_set[final_test_set[\"File_Name\"]==file_name]\n",
        "  out = ' '.join(test_df[\"word\"])\n",
        "  tokenized_texts = []\n",
        "  temp_token = []\n",
        "  temp_token.append('[CLS]')\n",
        "  token_list = tokenizer.tokenize(out)\n",
        "  for m,token in enumerate(token_list):\n",
        "    temp_token.append(token)\n",
        "  if len(temp_token) > max_len-1:\n",
        "    temp_token= temp_token[:max_len-1]\n",
        "  temp_token.append('[SEP]')\n",
        "  tokenized_texts.append(temp_token)\n",
        "  # Make text token into id\n",
        "  input_ids = pad_sequences([tokenizer.convert_tokens_to_ids(txt) for txt in tokenized_texts],\n",
        "                            maxlen=max_len, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
        "  # For fine tune of predict, with token mask is 1,pad token is 0\n",
        "  attention_masks = [[int(i>0) for i in ii] for ii in input_ids]\n",
        "  attention_masks[0]\n",
        "  segment_ids = [[0] * len(input_id) for input_id in input_ids]\n",
        "  segment_ids[0]\n",
        "  input_ids = torch.tensor(input_ids)\n",
        "  attention_masks = torch.tensor(attention_masks)\n",
        "  segment_ids = torch.tensor(segment_ids)\n",
        "  save_model.eval()\n",
        "  # Get model predict result\n",
        "  with torch.no_grad():\n",
        "    outputs = save_model(input_ids, token_type_ids=None,\n",
        "    attention_mask=None,)\n",
        "    # For eval mode, the first result of outputs is logits\n",
        "    logits = outputs[0]\n",
        "  # Make logits into numpy type predict result\n",
        "  # The predict result contain each token's all tags predict result\n",
        "  predict_results = logits.detach().cpu().numpy()\n",
        "  result_arrays_soft = softmax(predict_results[0])\n",
        "  # Get each token final predict tag index result\n",
        "  result_list = np.argmax(result_arrays_soft,axis=-1)\n",
        "  for i, mark in enumerate(attention_masks[0]):\n",
        "    if mark>0:\n",
        "      if tag2name[result_list[i]] == \"Person\":\n",
        "        result_files[file_name]=temp_token[i]\n",
        "        #print(\"Posibility:%f\"%(result_array[i][result_list[i]]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pWKej1ZM3h7q"
      },
      "source": [
        "submission_df=pd.DataFrame(columns=[\"File_Name\",\"Name\"])\n",
        "files=[]\n",
        "names=[]\n",
        "for key , person_name in result_files.items():\n",
        "  files.append(key)\n",
        "  names.append(person_name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "64YQjXCe-bno"
      },
      "source": [
        "submission_df[\"File_Name\"]=files\n",
        "submission_df[\"Name\"]=names"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ACUX7xVfA6qB"
      },
      "source": [
        "submission_df.to_csv(\"submission.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}